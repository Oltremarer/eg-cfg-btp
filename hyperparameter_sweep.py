import os
import json
import subprocess
import shutil
from pathlib import Path
from datetime import datetime

# ==============================================================================
# 1. åœ¨è¿™é‡Œå®šä¹‰ä½ æƒ³è¦æµ‹è¯•çš„è¶…å‚æ•°ç»„åˆ
# ==============================================================================
# æ¯ä¸€ç»„{}ä»£è¡¨ä¸€æ¬¡ç‹¬ç«‹çš„å®éªŒã€‚ä½ å¯ä»¥éšæ„å¢åˆ æˆ–ä¿®æ”¹ã€‚
# "run_name" ä¼šè¢«ç”¨ä½œè¾“å‡ºç›®å½•åï¼Œæ‰€ä»¥è¯·ä¿æŒå…¶å”¯ä¸€æ€§ã€‚
PARAMETER_GRID = [
    {
        "run_name": "lr_1e-5_r_8_epoch_3",
        "learning_rate": 1e-5,
        "lora_r": 8,
        "lora_alpha": 16,
        "num_train_epochs": 3,
    },
    {
        "run_name": "lr_2e-5_r_8_epoch_3",
        "learning_rate": 2e-5,
        "lora_r": 8,
        "lora_alpha": 16,
        "num_train_epochs": 3,
    },
    {
        "run_name": "lr_1e-5_r_16_epoch_3",
        "learning_rate": 1e-5,
        "lora_r": 16,
        "lora_alpha": 32,
        "num_train_epochs": 3,
    },
    {
        "run_name": "lr_2e-5_r_16_epoch_5_epochs",
        "learning_rate": 2e-5,
        "lora_r": 16,
        "lora_alpha": 32,
        "num_train_epochs": 5, # å°è¯•æ›´å¤šè®­ç»ƒå‘¨æœŸ
    },
]

# ==============================================================================
# 2. åœ¨è¿™é‡Œé…ç½®ä½ çš„é€šç”¨å‚æ•°
# ==============================================================================
# è¿™æ˜¯æ‰€æœ‰å®éªŒå…±äº«çš„å‚æ•°ï¼Œè¯·æ ¹æ®ä½ çš„å®é™…æƒ…å†µä¿®æ”¹ã€‚
BASE_CONFIG = {
    "script_path": "experiments/mbpp/step2_btp_experiment.py",
    "base_model": "deepseek-ai/deepseek-coder-1.3b-instruct",
    # ã€éå¸¸é‡è¦ã€‘è¯·åœ¨è¿™é‡Œå¡«å†™ä½ ç”¨V2ç”Ÿæˆçš„ç¼“å­˜æ•°æ®è·¯å¾„ï¼
    "sample_cache_path": "./results/Deepseek_V2_mbpp_all/sampling_cache/sampling_cache_deepseek_ai_DeepSeek_Coder_V2_Lite_Instruct_max974_beams5.json",
    "sweep_output_base_dir": "./sweep_results", # æ‰€æœ‰å®éªŒç»“æœçš„æ€»ç›®å½•
    # è¯„ä¼°æ—¶ä½¿ç”¨çš„é¢˜ç›®æ•°é‡
    "eval_max_problems": 50,
}

def run_command(command: list):
    """è¾…åŠ©å‡½æ•°ï¼šè¿è¡Œå‘½ä»¤è¡ŒæŒ‡ä»¤å¹¶å®æ—¶æ‰“å°è¾“å‡º"""
    print("\n" + "="*80)
    print(f"ğŸš€ EXECUTING: {' '.join(command)}")
    print("="*80 + "\n")
    try:
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8')
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(output.strip())
        rc = process.poll()
        if rc != 0:
            print(f"âŒ Command failed with return code {rc}")
        return rc
    except Exception as e:
        print(f"âŒ An exception occurred while running command: {e}")
        return -1


def parse_pass_at_1(eval_output_dir: str) -> float:
    """
    è¾…åŠ©å‡½æ•°ï¼šä»è¯„ä¼°ç»“æœçš„JSONæ–‡ä»¶ä¸­è§£æå‡ºpass@1ã€‚
    æ³¨æ„ï¼šè¿™é‡Œçš„å®ç°åŸºäºstep2è„šæœ¬æœ€ç»ˆä¼šç»Ÿè®¡'fully_passed_count'çš„å‡è®¾ã€‚
    """
    try:
        # æŸ¥æ‰¾è¯„ä¼°ç›®å½•ä¸‹æœ€æ–°çš„jsonç»“æœæ–‡ä»¶
        result_files = sorted(Path(eval_output_dir).glob('*.json'), key=os.path.getmtime, reverse=True)
        if not result_files:
            print(f"âš ï¸ No result JSON file found in {eval_output_dir}")
            return -1.0

        latest_result_file = result_files[0]
        with open(latest_result_file, 'r') as f:
            results = json.load(f)

        # ä»ç»“æœä¸­æå–é€šè¿‡ç‡
        total_experiences = results.get('experience_stats', {}).get('total_experiences', 0)
        # è¿™é‡Œçš„ total_experiences å®é™…ä¸Šæ˜¯è¯„ä¼°çš„é—®é¢˜æ•°
        total_problems_evaluated = BASE_CONFIG['eval_max_problems']

        # ç»Ÿè®¡pass@1ï¼Œå³beam searchçš„ä»»ä¸€beamé€šè¿‡å°±ç®—é€šè¿‡
        passed_problems = set()
        for exp in results.get('all_experiences', []):
            if exp.get('pass_rate', 0) > 0:
                passed_problems.add(exp['problem_id'])

        if total_problems_evaluated == 0:
            return 0.0

        pass_at_1 = len(passed_problems) / total_problems_evaluated
        return pass_at_1 * 100 # è¿”å›ç™¾åˆ†æ¯”
    except Exception as e:
        print(f"Error parsing pass@1 from {eval_output_dir}: {e}")
        return -1.0


def main():
    """ä¸»å‡½æ•°ï¼Œç¼–æ’æ‰€æœ‰å®éªŒ"""
    
    # ç¡®ä¿step2è„šæœ¬ä¸­å·²ç»åŠ å…¥äº† save_strategy="epoch"
    print("ğŸ”” Reminder: Please ensure that `save_strategy=\"epoch\"` is set in the TrainingArguments of your step2 script.")

    # åˆ›å»ºæœ¬æ¬¡sweepçš„æ€»ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    sweep_dir = Path(BASE_CONFIG["sweep_output_base_dir"]) / f"sweep_{timestamp}"
    sweep_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“‚ All results for this sweep will be saved in: {sweep_dir}")

    summary_results = []

    for i, params in enumerate(PARAMETER_GRID):
        run_name = params["run_name"]
        print("\n" + "#"*100)
        print(f"##  Starting Run {i+1}/{len(PARAMETER_GRID)}: {run_name}")
        print("#"*100 + "\n")

        run_output_dir = sweep_dir / run_name
        run_output_dir.mkdir(exist_ok=True)

        # --- 1. è®­ç»ƒ ---
        train_command = [
            "python", BASE_CONFIG["script_path"],
            "--model", BASE_CONFIG["base_model"],
            "--mode", "finetune",
            "--sample-cache-path", BASE_CONFIG["sample_cache_path"],
            "--output-dir", str(run_output_dir),
            "--learning-rate", str(params["learning_rate"]),
            "--lora-r", str(params["lora_r"]),
            "--lora-alpha", str(params["lora_alpha"]),
            "--num-train-epochs", str(params["num_train_epochs"]),
            # ä½¿ç”¨ä¸€äº›åˆç†çš„é»˜è®¤å€¼
            "--per-device-batch-size", "4",
            "--grad-accum-steps", "8",
        ]
        
        run_command(train_command)

        # --- 2. è¯„ä¼°æ‰€æœ‰checkpoints ---
        best_pass_rate = -1.0
        best_checkpoint = "None"
        
        checkpoint_dirs = sorted([d for d in run_output_dir.iterdir() if d.is_dir() and d.name.startswith('checkpoint')])

        if not checkpoint_dirs:
            print(f"âš ï¸ No checkpoints found for run {run_name}. Skipping evaluation.")
            continue

        for checkpoint_dir in checkpoint_dirs:
            print(f"\n--- Evaluating checkpoint: {checkpoint_dir.name} ---")
            eval_output_dir = checkpoint_dir / "evaluation"
            eval_output_dir.mkdir(exist_ok=True)

            eval_command = [
                "python", BASE_CONFIG["script_path"],
                "--model", str(checkpoint_dir),
                "--mode", "local",
                "--max-problems", str(BASE_CONFIG["eval_max_problems"]),
                "--output-dir", str(eval_output_dir),
                "--force-resample", # ç¡®ä¿æ¯æ¬¡éƒ½é‡æ–°è¯„ä¼°è€Œä¸æ˜¯ç”¨ç¼“å­˜
            ]

            run_command(eval_command)
            
            pass_rate = parse_pass_at_1(str(eval_output_dir))
            print(f"âœ… Pass@1 for {checkpoint_dir.name}: {pass_rate:.2f}%")

            if pass_rate > best_pass_rate:
                best_pass_rate = pass_rate
                best_checkpoint = checkpoint_dir.name
        
        summary_results.append({
            **params,
            "best_checkpoint": best_checkpoint,
            "best_pass_at_1": f"{best_pass_rate:.2f}%"
        })

    # --- 3. æ‰“å°æœ€ç»ˆæ€»ç»“æŠ¥å‘Š ---
    print("\n\n" + "*"*100)
    print("##  HYPERPARAMETER SWEEP COMPLETE - FINAL REPORT")
    print("*"*100 + "\n")
    
    # æ‰“å°è¡¨å¤´
    headers = list(PARAMETER_GRID[0].keys()) + ["best_checkpoint", "best_pass_at_1"]
    header_line = " | ".join(f"{h:<25}" for h in headers)
    print(header_line)
    print("-" * len(header_line))

    # æ‰“å°æ¯ä¸€è¡Œç»“æœ
    for result in summary_results:
        row_values = [str(result.get(h, 'N/A')) for h in headers]
        row_line = " | ".join(f"{v:<25}" for v in row_values)
        print(row_line)

if __name__ == "__main__":
    main() 