#!/usr/bin/env python3
"""
æ™ºèƒ½Promptæ¨¡æ¿ç³»ç»Ÿæ¼”ç¤ºè„šæœ¬
æ¼”ç¤ºå¦‚ä½•ä¸ºä¸åŒæ¨¡å‹è‡ªåŠ¨ç”Ÿæˆé€‚é…çš„promptæ ¼å¼

è¿è¡Œæ–¹å¼:
python demo_smart_prompts.py
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from experiments.prompt_templates import (
    get_model_prompt, 
    detect_model_info, 
    validate_model_compatibility,
    ModelFamily,
    ModelType
)

def print_separator(title: str):
    """æ‰“å°åˆ†éš”ç¬¦"""
    print("\n" + "="*60)
    print(f" {title} ")
    print("="*60)

def demo_model_detection():
    """æ¼”ç¤ºæ¨¡å‹æ£€æµ‹åŠŸèƒ½"""
    print_separator("ğŸ” æ¨¡å‹æ£€æµ‹æ¼”ç¤º")
    
    test_models = [
        "deepseek-ai/deepseek-coder-1.3b-instruct",
        "meta-llama/CodeLlama-7b-Instruct-hf",
        "meta-llama/CodeLlama-70b-Instruct-hf",  # ç‰¹æ®Šæ ¼å¼
        "bigcode/starcoder-15b",
        "WizardLM/WizardCoder-15B-V1.0",
        "Qwen/CodeQwen1.5-7B-Chat", 
        "gpt-3.5-turbo",
        "claude-3-sonnet",
        "unknown-model-name"
    ]
    
    for model in test_models:
        model_info = detect_model_info(model)
        special = f" (ç‰¹æ®Šæ ¼å¼: {model_info.special_format})" if model_info.special_format else ""
        size = f" - {model_info.size}" if model_info.size else ""
        
        print(f"ğŸ“‹ {model}")
        print(f"   å®¶æ—: {model_info.family.value}")
        print(f"   ç±»å‹: {model_info.type.value}{size}{special}")
        print()

def demo_prompt_generation():
    """æ¼”ç¤ºpromptç”ŸæˆåŠŸèƒ½"""
    print_separator("ğŸ¯ Promptç”Ÿæˆæ¼”ç¤º")
    
    # ç¤ºä¾‹é—®é¢˜
    sample_problem = {
        'text': 'Write a function to remove the first and last occurrence of a given character from the string.',
        'test_list': [
            'assert remove_Occ("hello","l") == "heo"',
            'assert remove_Occ("abcda","a") == "bcd"',
            'assert remove_Occ("PHP","P") == "H"'
        ]
    }
    
    test_cases = [
        {
            "model": "deepseek-ai/deepseek-coder-1.3b-instruct",
            "description": "DeepSeekæŒ‡ä»¤æ¨¡å‹ (æ”¯æŒfew-shot)"
        },
        {
            "model": "meta-llama/CodeLlama-7b-Instruct-hf", 
            "description": "CodeLLaMAæŒ‡ä»¤æ¨¡å‹"
        },
        {
            "model": "meta-llama/CodeLlama-70b-Instruct-hf",
            "description": "CodeLLaMA-70B (ç‰¹æ®ŠSourceæ ¼å¼)"
        },
        {
            "model": "bigcode/starcoder-15b",
            "description": "StarCoderåŸºç¡€æ¨¡å‹"
        },
        {
            "model": "WizardLM/WizardCoder-15B-V1.0", 
            "description": "WizardCoder (Alpacaæ ¼å¼)"
        },
        {
            "model": "gpt-3.5-turbo",
            "description": "OpenAIæ¨¡å‹ (Messagesæ ¼å¼)"
        }
    ]
    
    for case in test_cases:
        print(f"ğŸ¤– {case['description']}")
        print(f"   æ¨¡å‹: {case['model']}")
        
        try:
            prompt = get_model_prompt(
                model_name=case['model'],
                dataset="mbpp",
                problem=sample_problem
            )
            
            if isinstance(prompt, list):
                # OpenAI/Claudeçš„messagesæ ¼å¼
                print("   æ ¼å¼: Messages")
                for msg in prompt:
                    role = msg['role']
                    content = msg['content'][:100] + "..." if len(msg['content']) > 100 else msg['content']
                    print(f"   {role}: {content}")
            else:
                # å­—ç¬¦ä¸²æ ¼å¼
                preview = prompt[:150] + "..." if len(prompt) > 150 else prompt
                print(f"   Prompté¢„è§ˆ: {preview}")
                
        except Exception as e:
            print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        print()

def demo_few_shot_examples():
    """æ¼”ç¤ºfew-shot examplesåŠŸèƒ½"""
    print_separator("ğŸ“š Few-shot Examplesæ¼”ç¤º")
    
    sample_problem = {
        'text': 'Write a function to find the maximum element in a list.',
        'test_list': ['assert max_element([1, 2, 3]) == 3']
    }
    
    # å‡†å¤‡few-shot examples
    examples = [
        {
            "problem": "Write a function to find the similar elements from the given two tuple lists.",
            "test_cases": [
                "assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)",
                "assert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)"
            ],
            "solution": """def similar_elements(test_tup1, test_tup2):
  res = tuple(set(test_tup1) & set(test_tup2))
  return (res)"""
        }
    ]
    
    print("ğŸ§ª DeepSeekæ¨¡å‹ - ä¸ä½¿ç”¨examples:")
    prompt1 = get_model_prompt(
        model_name="deepseek-ai/deepseek-coder-6.7b-instruct",
        dataset="mbpp",
        problem=sample_problem,
        use_examples=False
    )
    print(prompt1[:200] + "...")
    
    print("\nğŸ“ DeepSeekæ¨¡å‹ - ä½¿ç”¨few-shot examples:")
    prompt2 = get_model_prompt(
        model_name="deepseek-ai/deepseek-coder-6.7b-instruct", 
        dataset="mbpp",
        problem=sample_problem,
        use_examples=True,
        examples=examples
    )
    print(prompt2[:400] + "...")

def demo_compatibility_check():
    """æ¼”ç¤ºå…¼å®¹æ€§æ£€æŸ¥"""
    print_separator("âœ… å…¼å®¹æ€§æ£€æŸ¥æ¼”ç¤º")
    
    test_cases = [
        ("deepseek-ai/deepseek-coder-1.3b-instruct", "mbpp"),
        ("bigcode/starcoder-15b", "humaneval"),
        ("bigcode/starcoder-15b", "mbpp"),
        ("unknown-model", "mbpp")
    ]
    
    for model, dataset in test_cases:
        print(f"ğŸ” æ£€æŸ¥: {model} + {dataset}")
        
        compatibility = validate_model_compatibility(model, dataset)
        
        print(f"   çŠ¶æ€: {'âœ… å…¼å®¹' if compatibility['supported'] else 'âŒ ä¸å…¼å®¹'}")
        
        if compatibility['recommendations']:
            print("   ğŸ’¡ å»ºè®®:")
            for rec in compatibility['recommendations']:
                print(f"      - {rec}")
        
        if compatibility['warnings']:
            print("   âš ï¸  è­¦å‘Š:")
            for warning in compatibility['warnings']:
                print(f"      - {warning}")
        
        print()

def demo_dataset_differences():
    """æ¼”ç¤ºä¸åŒæ•°æ®é›†çš„æ ¼å¼å·®å¼‚"""
    print_separator("ğŸ“Š æ•°æ®é›†æ ¼å¼å·®å¼‚æ¼”ç¤º")
    
    datasets = [
        {
            "name": "mbpp",
            "problem": {
                'text': 'Write a function to find the maximum element in a list.',
                'test_list': ['assert max_element([1, 2, 3]) == 3']
            }
        },
        {
            "name": "humaneval", 
            "problem": {
                'prompt': '''def max_element(lst):
    """
    Find the maximum element in a list.
    
    Args:
        lst: A list of numbers
        
    Returns:
        The maximum element
        
    Examples:
    >>> max_element([1, 2, 3])
    3
    >>> max_element([10, 5, 8])
    10
    """'''
            }
        },
        {
            "name": "apps",
            "problem": {
                'question': 'Find the maximum element in a given list of integers.',
                'test_list': [
                    'assert max_element([1, 2, 3]) == 3',
                    'assert max_element([10, 5, 8]) == 10'
                ]
            }
        }
    ]
    
    model_name = "meta-llama/CodeLlama-7b-Instruct-hf"
    
    for dataset_info in datasets:
        dataset_name = dataset_info["name"]
        problem = dataset_info["problem"]
        
        print(f"ğŸ“‹ æ•°æ®é›†: {dataset_name.upper()}")
        
        prompt = get_model_prompt(
            model_name=model_name,
            dataset=dataset_name,
            problem=problem
        )
        
        preview = prompt[:200] + "..." if len(prompt) > 200 else prompt
        print(f"   æ ¼å¼åŒ–ç»“æœ: {preview}")
        print()

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("ğŸš€ æ™ºèƒ½Promptæ¨¡æ¿ç³»ç»Ÿæ¼”ç¤º")
    print("åŸºäºã€Šä»£ç ç”Ÿæˆæ¨¡å‹Promptæ¨¡æ¿æƒå¨æŒ‡å—ã€‹")
    
    try:
        demo_model_detection()
        demo_prompt_generation()
        demo_few_shot_examples()
        demo_compatibility_check()
        demo_dataset_differences()
        
        print_separator("ğŸ‰ æ¼”ç¤ºå®Œæˆ")
        print("ğŸ’¡ æç¤º: æŸ¥çœ‹ experiments/model_prompt_usage_guide.md è·å–è¯¦ç»†ä½¿ç”¨æŒ‡å—")
        print("ğŸ“ æç¤º: ç°æœ‰å®éªŒå¯ä»¥é€æ­¥è¿ç§»åˆ°æ–°ç³»ç»Ÿï¼Œæ— éœ€ä¸€æ¬¡æ€§é‡å†™")
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…ä¾èµ–å¹¶è®¾ç½®Pythonè·¯å¾„")
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 