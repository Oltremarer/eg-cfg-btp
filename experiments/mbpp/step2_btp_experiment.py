#!/usr/bin/env python3
"""
MBPPæ•°æ®é›†çš„BTPå®éªŒ (Beam Search + Testing + Prioritized Experience Replay)

BTPç®—æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š
1. é˜¶æ®µ1: Beam Searché‡‡æ · + æµ‹è¯•éªŒè¯
2. é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾ (PPER) è®­ç»ƒ

æ”¯æŒçš„åŠŸèƒ½ï¼š
- æœ¬åœ°æ¨¡å‹çš„BTPå®éªŒ
- æœ¬åœ°æ¨¡å‹çš„BTPå¾®è°ƒå®éªŒ  
- OpenAI APIçš„BTPå®éªŒ
- DeepSeek APIçš„BTPå®éªŒ

ä½¿ç”¨ç¤ºä¾‹ï¼š
1. æœ¬åœ°æ¨¡å‹BTPå®éªŒï¼š
   python experiments/mbpp/step2_btp_experiment.py --model deepseek-ai/deepseek-coder-1.3b-instruct --mode local

2. æœ¬åœ°æ¨¡å‹å¾®è°ƒï¼š
   python experiments/mbpp/step2_btp_experiment.py --model deepseek-ai/deepseek-coder-1.3b-instruct --target-model deepseek-ai/deepseek-coder-1.3b-instruct --mode finetune

3. OpenAIå®éªŒï¼š
   python experiments/mbpp/step2_btp_experiment.py --model gpt-4 --mode openai --api-key YOUR_KEY
"""

import os
import sys
import json
import argparse
import numpy as np
import random
import torch
import math
import logging
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict, deque
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

# å¯¼å…¥å…±äº«åŸºç¡€ç±»
from experiments.shared.base_experiment import Step2BTPExperiment  
from experiments.shared.dataset_configs import MBPP_CONFIG
from experiments.shared.common_utils import safe_execute_code, load_mbpp_problems

# æ¡ä»¶å¯¼å…¥
try:
    from transformers import (
        AutoModelForCausalLM, 
        AutoTokenizer, 
        Trainer, 
        TrainingArguments,
        DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    HF_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘HuggingFaceä¾èµ–: {e}")
    HF_AVAILABLE = False

try:
    from eg_cfg.openai_utils import OpenAIClient, OpenAIInferenceError
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# é¡¹ç›®ç›¸å…³å¯¼å…¥
from eg_cfg.mbpp_utils import run_tests
if HF_AVAILABLE:
    from eg_cfg.model_utils import setup_device, load_model, load_tokenizer

from experiments.prompt_templates import get_model_prompt, detect_model_info, validate_model_compatibility
from experiments.shared.model_configs import get_model_config, get_optimal_generation_params


class ModelAdapter:
    """ç»Ÿä¸€æ¨¡å‹é€‚é…å™¨ - æ”¯æŒæœ¬åœ°å’ŒAPIæ¨¡å‹"""
    
    def __init__(self, model_name: str, model_type: str = "local", 
                 api_key: str = None, api_base: str = None, **kwargs):
        self.model_name = model_name
        self.model_type = model_type
        self.api_key = api_key
        self.api_base = api_base
        self.kwargs = kwargs
        
        self.model = None
        self.tokenizer = None
        self.device = None
        self._setup_model()
    
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        if self.model_type == "local" or self.model_type == "finetune":
            self._setup_local_model()
        elif self.model_type == "openai":
            self._setup_openai_model()
        elif self.model_type in ["deepseek", "api"]:
            self._setup_api_model()
    
    def _setup_local_model(self):
        """è®¾ç½®æœ¬åœ°æ¨¡å‹"""
        if not HF_AVAILABLE:
            raise ImportError("æœ¬åœ°æ¨¡å‹éœ€è¦å®‰è£…transformersåº“")
        
        print(f"ğŸ”§ åŠ è½½æœ¬åœ°æ¨¡å‹: {self.model_name}")
        self.device = setup_device()
        self.model, self.tokenizer = load_model(self.model_name, self.device)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def _setup_openai_model(self):
        """è®¾ç½®OpenAIæ¨¡å‹"""
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAIæ¨¡å‹éœ€è¦å®‰è£…openaiç›¸å…³ä¾èµ–")
        
        print(f"ğŸ”§ é…ç½®OpenAIæ¨¡å‹: {self.model_name}")
        self.client = OpenAIClient(api_key=self.api_key, model=self.model_name)
    
    def _setup_api_model(self):
        """è®¾ç½®APIæ¨¡å‹"""
        if not REQUESTS_AVAILABLE:
            raise ImportError("APIæ¨¡å‹éœ€è¦å®‰è£…requestsåº“")
        
        print(f"ğŸ”§ é…ç½®APIæ¨¡å‹: {self.model_name}")
        self.api_headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def generate(self, prompt: str, **generation_kwargs) -> List[Dict]:
        """ç»Ÿä¸€ç”Ÿæˆæ¥å£"""
        if self.model_type == "local" or self.model_type == "finetune":
            return self._generate_local(prompt, **generation_kwargs)
        elif self.model_type == "openai":
            return self._generate_openai(prompt, **generation_kwargs)
        elif self.model_type in ["deepseek", "api"]:
            return self._generate_api(prompt, **generation_kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _generate_local(self, prompt, num_beams: int = 5, 
                       temperature: float = 0.8, max_tokens: int = 512,
                       **kwargs) -> list:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆ - ç»è¿‡æœ€ç»ˆä¼˜åŒ–çš„ç‰ˆæœ¬"""
        # åˆ¤æ–­promptç±»å‹å¹¶åº”ç”¨æ¨¡æ¿
        if isinstance(prompt, list):
            if not hasattr(self, '_debug_prompt_printed'):
                print("\n" + "="*50)
                print(">>> è¯Šæ–­ä¿¡æ¯: æ£€æŸ¥ Tokenizer åº”ç”¨æ¨¡æ¿åçš„çœŸå® Prompt <<<")
            # å°† token IDs è§£ç å›å­—ç¬¦ä¸²ï¼Œçœ‹çœ‹ç‰¹æ®Šæ ‡è®°æ˜¯å¦çœŸçš„å­˜åœ¨
                temp_prompt_str = self.tokenizer.decode(
                    self.tokenizer.apply_chat_template(
                        prompt, add_generation_prompt=True, return_tensors="pt"
                    )[0]
                )
                print(temp_prompt_str)
                print("="*50 + "\n")
                self._debug_prompt_printed = True
                
            input_ids = self.tokenizer.apply_chat_template(
                prompt,
                add_generation_prompt=True,
                return_tensors="pt"
            ).to(self.device)
        else:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            input_ids = inputs.input_ids.to(self.device)

        input_ids_len = input_ids.shape[1]

        with torch.no_grad():
            outputs = self.model.generate(
                input_ids=input_ids,  # ä½¿ç”¨å…³é”®å­—å‚æ•°ï¼Œæ›´è§„èŒƒ
                num_beams=num_beams,
                num_return_sequences=num_beams,
                max_new_tokens=max_tokens,
                do_sample=temperature > 0,
                temperature=temperature if temperature > 0 else 1.0,
                return_dict_in_generate=True,
                output_scores=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )

        results = []
        sequences = outputs.sequences
        scores = getattr(outputs, 'sequences_scores', None)
        
        for i, sequence in enumerate(sequences):
            output_ids = sequence[input_ids_len:]
            # æ›´ç¨³å¥çš„ä»£ç åå¤„ç†ï¼Œå»é™¤markdownä»£ç å—
            decoded_code = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
            if decoded_code.startswith("```python"):
                decoded_code = decoded_code[len("```python"):].strip()
            if decoded_code.endswith("```"):
                decoded_code = decoded_code[:-3].strip()
            code = decoded_code

            if scores is not None:
                log_prob = scores[i].item()
                output_len = len(output_ids) if len(output_ids) > 0 else 1
                possibility = min(math.exp(log_prob / output_len), 1.0)
            else:
                log_prob = -10.0
                possibility = 0.5
            
            results.append({
                'code': code,
                'possibility': possibility,
                'log_prob': log_prob,
                'beam_rank': i,
                'sequence_length': len(output_ids)
            })
        
        return results
    
    def _generate_openai(self, prompt: str, num_beams: int = 5, 
                        temperature: float = 0.8, **kwargs) -> List[Dict]:
        """OpenAIæ¨¡å‹ç”Ÿæˆ - æ”¯æŒçœŸå®æ¦‚ç‡çš„ä¿®å¤ç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨æ–°çš„æ¦‚ç‡æ„ŸçŸ¥æ–¹æ³•ï¼Œä¸€æ¬¡æ€§ç”Ÿæˆå¤šä¸ªå€™é€‰
            results = self.client.generate_code_with_probs(
                prompt=prompt,
                temperature=temperature,
                max_tokens=512,
                n=num_beams,  # ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªå€™é€‰ï¼Œæ•ˆç‡æ›´é«˜
                logprobs=True,  # è·å–çœŸå®æ¦‚ç‡
                top_logprobs=5
            )
            
            print(f"âœ… OpenAIç”ŸæˆæˆåŠŸ: {len(results)}ä¸ªå€™é€‰ï¼Œæ¦‚ç‡èŒƒå›´: {min(r['possibility'] for r in results):.4f} - {max(r['possibility'] for r in results):.4f}")
            return results
            
        except Exception as e:
            print(f"âŒ OpenAIç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯å‡æ¦‚ç‡
            return [{
                'code': '',
                'possibility': 0.0,
                'log_prob': -100.0,
                'beam_rank': i,
                'sequence_length': 0,
                'error': str(e)
            } for i in range(num_beams)]
    
    def _generate_api(self, prompt: str, num_beams: int = 5, 
                     temperature: float = 0.8, **kwargs) -> List[Dict]:
        """APIæ¨¡å‹ç”Ÿæˆ"""
        results = []
        
        for i in range(num_beams):
            try:
                payload = {
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": 512
                }
                
                response = requests.post(
                    self.api_base or "https://api.deepseek.com/v1/chat/completions",
                    headers=self.api_headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    content = data['choices'][0]['message']['content']
                    
                    results.append({
                        'code': content,
                        'possibility': 0.7,
                        'log_prob': -8.0,
                        'beam_rank': i,
                        'sequence_length': len(content)
                    })
                else:
                    raise Exception(f"APIé”™è¯¯: {response.status_code}")
                    
            except Exception as e:
                print(f"APIç”Ÿæˆå¤±è´¥ (beam {i}): {e}")
                results.append({
                    'code': '',
                    'possibility': 0.0,
                    'log_prob': -100.0,
                    'beam_rank': i,
                    'sequence_length': 0,
                    'error': str(e)
                })
        
        return results


class P2ValueCalculator:
    """P2Valueè®¡ç®—å™¨ - ç»“åˆå¯èƒ½æ€§å’Œé€šè¿‡ç‡"""
    
    def __init__(self, alpha: float = 0.5):
        self.alpha = alpha
    
    def calculate_p2value(self, possibility: float, pass_rate: float) -> float:
        """è®¡ç®—P2Value = Î± * possibility + (1-Î±) * pass_rate"""
        return self.alpha * possibility + (1 - self.alpha) * pass_rate
    
    def calculate_p2value_extended(self, log_prob=None, sequence_length=None, 
                                 possibility=None, passed_tests=0, total_tests=1):
        """æ‰©å±•P2Valueè®¡ç®—ï¼Œè€ƒè™‘æ›´å¤šå› ç´ """
        if possibility is None and log_prob is not None:
            possibility = min(math.exp(log_prob / max(sequence_length, 1)), 1.0)
        
        pass_rate = passed_tests / max(total_tests, 1)
        
        if possibility is None:
            possibility = 0.5
        
        return self.calculate_p2value(possibility, pass_rate)


class PrioritizedSampler:
    """ä¼˜å…ˆé‡‡æ ·å™¨ - åŸºäºP2Valueè¿›è¡Œé‡‡æ ·"""
    
    def __init__(self, sampling_method: str = "power", alpha: float = 1.0):
        self.sampling_method = sampling_method
        self.alpha = alpha
        
        if sampling_method not in ["power", "rank"]:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·æ–¹æ³•: {sampling_method}")
    
    def sample(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """é‡‡æ ·ç»éªŒ"""
        if len(experiences) <= batch_size:
            return experiences
        
        if self.sampling_method == "power":
            return self._power_sampling(experiences, batch_size)
        elif self.sampling_method == "rank":
            return self._rank_sampling(experiences, batch_size)
    
    def _power_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """å¹‚é‡‡æ ·"""
        # è®¡ç®—æƒé‡
        weights = []
        for exp in experiences:
            p2value = exp.get('p2value', 0.0)
            weight = max(p2value ** self.alpha, 1e-8)
            weights.append(weight)
        
        weights = np.array(weights)
        probabilities = weights / weights.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(
            len(experiences), 
            size=batch_size, 
            replace=False, 
            p=probabilities
        )
        
        return [experiences[i] for i in indices]
    
    def _rank_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """æ’åºé‡‡æ ·"""
        # æŒ‰P2Valueæ’åº
        sorted_experiences = sorted(
            experiences, 
            key=lambda x: x.get('p2value', 0.0), 
            reverse=True
        )
        
        # è®¡ç®—æ’åºæƒé‡
        weights = [1.0 / (rank + 1) ** self.alpha for rank in range(len(sorted_experiences))]
        weights = np.array(weights)
        probabilities = weights / weights.sum()
        
        # é‡‡æ ·
        indices = np.random.choice(
            len(sorted_experiences), 
            size=batch_size, 
            replace=False, 
            p=probabilities
        )
        
        return [sorted_experiences[i] for i in indices]


class ExperienceReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self.experiences = deque(maxlen=max_size)
        self.p2calculator = P2ValueCalculator()
    
    def add_experience(self, experience: Dict):
        """æ·»åŠ ç»éªŒ"""
        # è®¡ç®—P2Value
        experience['p2value'] = self.p2calculator.calculate_p2value_extended(
            possibility=experience.get('possibility', 0.5),
            passed_tests=experience.get('passed_tests', 0),
            total_tests=experience.get('total_tests', 1)
        )
        
        self.experiences.append(experience)
    
    def get_all_experiences(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ç»éªŒ"""
        return list(self.experiences)
    
    def get_stats(self, include_samples: bool = False, max_samples: int = 10) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.experiences:
            return {}
        
        p2values = [exp.get('p2value', 0.0) for exp in self.experiences]
        pass_rates = [exp.get('pass_rate', 0.0) for exp in self.experiences]
        
        stats = {
            'total_experiences': len(self.experiences),
            'avg_p2value': np.mean(p2values),
            'max_p2value': np.max(p2values), 
            'min_p2value': np.min(p2values),
            'avg_pass_rate': np.mean(pass_rates),
            'fully_passed_count': sum(1 for pr in pass_rates if pr >= 1.0),
            'zero_passed_count': sum(1 for pr in pass_rates if pr == 0.0)
        }
        
        # å¦‚æœéœ€è¦åŒ…å«æ ·æœ¬æ•°æ®ï¼Œæ·»åŠ ä¸€äº›ä»£è¡¨æ€§æ ·æœ¬
        if include_samples:
            samples = []
            
            # è·å–é€šè¿‡ç‡æœ€é«˜çš„æ ·æœ¬
            best_experiences = sorted(self.experiences, key=lambda x: x.get('pass_rate', 0), reverse=True)[:max_samples//2]
            samples.extend(best_experiences)
            
            # æ·»åŠ ä¸€äº›éšæœºæ ·æœ¬
            import random
            remaining_samples = max_samples - len(samples)
            if remaining_samples > 0 and len(self.experiences) > len(samples):
                random_experiences = random.sample(
                    [exp for exp in self.experiences if exp not in samples], 
                    min(remaining_samples, len(self.experiences) - len(samples))
                )
                samples.extend(random_experiences)
            
            stats['sample_experiences'] = samples
        
        return stats


class MBTPFineTuningManager:
    """MBPP BTPå¾®è°ƒç®¡ç†å™¨"""
    
    def __init__(self, model_adapter: ModelAdapter, use_lora: bool = True, 
                 lora_config: Optional[Dict] = None, output_dir: str = "./mbpp_btp_checkpoints"):
        self.model_adapter = model_adapter
        self.use_lora = use_lora
        self.output_dir = output_dir
        self.lora_config = lora_config or {
            'r': 16,
            'lora_alpha': 32,
            'lora_dropout': 0.1
        }
        
        if self.use_lora and HF_AVAILABLE:
            self._setup_lora()
    
    def _setup_lora(self):
        """è®¾ç½®LoRAå¾®è°ƒ"""
        if self.model_adapter.model_type not in ["local", "finetune"]:
            print("âš ï¸  LoRAå¾®è°ƒä»…æ”¯æŒæœ¬åœ°æ¨¡å‹")
            return
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=self.lora_config['r'],
            lora_alpha=self.lora_config['lora_alpha'],
            lora_dropout=self.lora_config['lora_dropout'],
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
        )
        
        self.model_adapter.model = get_peft_model(self.model_adapter.model, lora_config)
        print("âœ… LoRAé…ç½®å®Œæˆ")
    
    def finetune_on_experiences(self, experiences: List[Dict], 
                               training_args: Optional[TrainingArguments] = None) -> None:
        """åŸºäºç»éªŒè¿›è¡Œå¾®è°ƒ"""
        if self.model_adapter.model_type not in ["local", "finetune"]:
            print("âš ï¸  å¾®è°ƒä»…æ”¯æŒæœ¬åœ°æ¨¡å‹")
            return
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        train_dataset = self._prepare_training_dataset(experiences)
        
        if training_args is None:
            training_args = TrainingArguments(
                output_dir=self.output_dir,  # ä½¿ç”¨è‡ªå®šä¹‰è¾“å‡ºç›®å½•
                num_train_epochs=1,
                per_device_train_batch_size=2,
                gradient_accumulation_steps=4,
                warmup_steps=10,
                learning_rate=1e-6,  # å¤§å¹…å‡å°‘å­¦ä¹ ç‡ï¼Œä»1e-4æ”¹ä¸º1e-6
                fp16=True,
                logging_steps=5,
                save_steps=100,
                remove_unused_columns=False,
            )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.model_adapter.tokenizer,
            mlm=False,
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model_adapter.model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator,
        )
        
        print(f"ğŸš€ å¼€å§‹å¾®è°ƒ... æ¨¡å‹å°†ä¿å­˜åˆ°: {self.output_dir}")
        trainer.train()
        trainer.save_model()
        print("âœ… å¾®è°ƒå®Œæˆ")
    
    def _prepare_training_dataset(self, experiences: List[Dict]) -> Dataset:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›† - ä¸æ¨ç†æ—¶æ ¼å¼ç»Ÿä¸€ï¼ŒæŒ‡ä»¤æ¨¡å‹ç”¨chatæ¨¡æ¿
        """
        processed_texts = []
        for exp in experiences:
            problem_dict = {'text': exp['problem_text'], 'test_list': list(exp.get('test_results', {}).keys())}
            model_name = self.model_adapter.model_name.lower()
            is_instruct = any(x in model_name for x in ["instruct", "chat", "deepseek-coder-v2-lite", "deepseek-coder-instruct", "qwen", "chatglm"])
            if is_instruct:
                system_prompt = (
                    "You are an expert Python programmer. Your task is to write a "
                    "Python function that solves a programming problem. "
                    "Please implement the function in a single, clean block of code. "
                    "Do not generate any explanatory text, comments, or markdown formatting like ```python."
                )
                user_instruction = f"Problem: {problem_dict['text']}\n\n"
                if problem_dict.get('test_list'):
                    test_cases_str = "\n".join(problem_dict['test_list'])
                    user_instruction += (
                        "The function should pass the following tests:\n"
                        f"{test_cases_str}\n\n"
                    )
                user_instruction += "Provide the complete Python code for the function now."
                response_code = exp['code']
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_instruction},
                    {"role": "assistant", "content": response_code}
                ]
                formatted_text = self.model_adapter.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=False
                ) + self.model_adapter.tokenizer.eos_token
            else:
                prompt = f'"""\n{problem_dict["text"]}\n"""\n'
                formatted_text = prompt + exp['code'] + self.model_adapter.tokenizer.eos_token
            processed_texts.append(formatted_text)
        def tokenize_function(examples):
            tokenized = self.model_adapter.tokenizer(
                examples['text'],
                truncation=True,
                padding=True,
                max_length=1024,
                return_tensors="pt"
            )
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
        dataset = Dataset.from_dict({'text': processed_texts})
        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])
        return tokenized_dataset


class MBBPBTPExperiment(Step2BTPExperiment):
    """MBPPæ•°æ®é›†çš„BTPå®éªŒ - ä½¿ç”¨æ™ºèƒ½Prompté€‚é…ç³»ç»Ÿ"""
    
    def __init__(self, model_name: str = None, model_type: str = "local", 
                 api_key: str = None, api_base: str = None,
                 sampling_method: str = "power", sampling_alpha: float = 1.0, 
                 p2value_alpha: float = 0.5, output_dir: str = "./mbpp_btp_checkpoints",
                 fixed_sample_path: str = None):
        
        # è®¾ç½®åŸºæœ¬æ¨¡å‹ä¿¡æ¯
        self.model_name = model_name or "deepseek-ai/deepseek-coder-1.3b-instruct"
        self.model_type = model_type
        self.api_key = api_key
        self.api_base = api_base
        self.output_dir = output_dir
        self.fixed_sample_path = fixed_sample_path
        
        # BTPç‰¹å®šå‚æ•°  
        self.sampling_method = sampling_method
        self.sampling_alpha = sampling_alpha
        self.p2value_alpha = p2value_alpha
        
        # é‡‡æ ·æ•°æ®æŒä¹…åŒ–ç›¸å…³
        self.sampling_cache_dir = os.path.join(output_dir, "sampling_cache")
        os.makedirs(self.sampling_cache_dir, exist_ok=True)
        
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(dataset_name="mbpp", model_name=self.model_name)
        
        # åˆå§‹åŒ–æ™ºèƒ½é…ç½®ï¼ˆåœ¨çˆ¶ç±»æ„é€ å‡½æ•°ä¹‹åï¼‰
        self.model_info = detect_model_info(self.model_name)
        self.model_config = get_model_config(self.model_name)
        self.optimal_params = get_optimal_generation_params(self.model_name, "mbpp")
        
        # éªŒè¯æ¨¡å‹å…¼å®¹æ€§
        compatibility = validate_model_compatibility(self.model_name, "mbpp")
        if compatibility["warnings"]:
            print("âš ï¸  æ¨¡å‹å…¼å®¹æ€§è­¦å‘Š:")
            for warning in compatibility["warnings"]:
                print(f"   - {warning}")
        
        if compatibility["recommendations"]:
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for rec in compatibility["recommendations"]:
                print(f"   - {rec}")
        
        # è®¾ç½®adapter
        self.adapter = ModelAdapter(
            model_name=self.model_name,
            model_type=self.model_type,
            api_key=self.api_key or "",  # ç¡®ä¿ä¸æ˜¯None
            api_base=self.api_base or "",  # ç¡®ä¿ä¸æ˜¯None
            **self.optimal_params  # ä½¿ç”¨ä¼˜åŒ–å‚æ•°
        )
        
        # åˆå§‹åŒ–BTPç»„ä»¶
        self.experience_buffer = ExperienceReplayBuffer()
        self.sampler = PrioritizedSampler(sampling_method, sampling_alpha)
        self.p2calculator = P2ValueCalculator(p2value_alpha)
        
        # å¾®è°ƒç®¡ç†å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if model_type == "finetune":
            self.finetuning_manager = MBTPFineTuningManager(
                self.adapter, 
                use_lora=True, 
                output_dir=self.output_dir
            )
        else:
            self.finetuning_manager = None
        
        print(f"ğŸš€ åˆå§‹åŒ–å®Œæˆ:")
        print(f"   æ¨¡å‹: {self.model_name}")
        print(f"   å®¶æ—: {self.model_info.family.value}")
        print(f"   ç±»å‹: {self.model_info.type.value}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   é‡‡æ ·ç¼“å­˜ç›®å½•: {self.sampling_cache_dir}")
        print(f"   ä¼˜åŒ–å‚æ•°: {self.optimal_params}")
    
    def _get_sampling_cache_filename(self, max_problems: int, num_beams: int) -> str:
        """ç”Ÿæˆé‡‡æ ·ç¼“å­˜æ–‡ä»¶å"""
        model_name_safe = self.model_name.replace("/", "_").replace("-", "_")
        return f"sampling_cache_{model_name_safe}_max{max_problems}_beams{num_beams}.json"
    
    def _get_sampling_cache_path(self, max_problems: int, num_beams: int) -> str:
        """è·å–é‡‡æ ·ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        filename = self._get_sampling_cache_filename(max_problems, num_beams)
        return os.path.join(self.sampling_cache_dir, filename)
    
    def save_sampling_results(self, max_problems: int, num_beams: int):
        """ä¿å­˜é‡‡æ ·ç»“æœåˆ°ç¼“å­˜æ–‡ä»¶"""
        cache_path = self._get_sampling_cache_path(max_problems, num_beams)
        
        # è·å–æ‰€æœ‰ç»éªŒæ•°æ®
        all_experiences = self.experience_buffer.get_all_experiences()
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        cache_data = {
            'model_name': self.model_name,
            'max_problems': max_problems,
            'num_beams': num_beams,
            'sampling_method': self.sampling_method,
            'sampling_alpha': self.sampling_alpha,
            'p2value_alpha': self.p2value_alpha,
            'total_experiences': len(all_experiences),
            'experiences': all_experiences,
            'timestamp': datetime.now().isoformat(),
            'cache_version': '1.0'
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ é‡‡æ ·ç»“æœå·²ä¿å­˜åˆ°: {cache_path}")
        print(f"   å…±ä¿å­˜ {len(all_experiences)} ä¸ªç»éªŒæ ·æœ¬")
        
        return cache_path
    
    def load_sampling_results(self, max_problems: int, num_beams: int) -> bool:
        """ä»ç¼“å­˜æ–‡ä»¶åŠ è½½é‡‡æ ·ç»“æœ"""
        cache_path = self._get_sampling_cache_path(max_problems, num_beams)
        
        if not os.path.exists(cache_path):
            print(f"âš ï¸  ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
            return False
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # éªŒè¯ç¼“å­˜æ•°æ®
            if cache_data.get('model_name') != self.model_name:
                print(f"âš ï¸  ç¼“å­˜æ¨¡å‹ä¸åŒ¹é…: ç¼“å­˜={cache_data.get('model_name')}, å½“å‰={self.model_name}")
                return False
            
            # åŠ è½½ç»éªŒæ•°æ®åˆ°ç¼“å†²åŒº
            experiences = cache_data.get('experiences', [])
            for exp in experiences:
                self.experience_buffer.add_experience(exp)
            
            print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½é‡‡æ ·ç»“æœ: {cache_path}")
            print(f"   å…±åŠ è½½ {len(experiences)} ä¸ªç»éªŒæ ·æœ¬")
            print(f"   ç¼“å­˜æ—¶é—´: {cache_data.get('timestamp', 'N/A')}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def check_sampling_cache_exists(self, max_problems: int, num_beams: int) -> bool:
        """æ£€æŸ¥é‡‡æ ·ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        cache_path = self._get_sampling_cache_path(max_problems, num_beams)
        return os.path.exists(cache_path)
    
    def load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½MBPPé…ç½®"""
        return MBPP_CONFIG
    
    def load_dataset(self) -> Dict[str, Any]:
        """åŠ è½½MBPPæ•°æ®é›†"""
        return load_mbpp_problems()
    
    def format_prompt(self, problem: dict) -> object:
        """
        æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨ç”Ÿæˆåˆé€‚çš„promptæ ¼å¼ã€‚
        - DeepSeek/ChatGLM3/Qwenç­‰æŒ‡ä»¤æ¨¡å‹ï¼šè¿”å›æ¶ˆæ¯åˆ—è¡¨
        - Llama/GPTç­‰åŸºç¡€æ¨¡å‹ï¼šè¿”å›å­—ç¬¦ä¸²
        """
        model_name = self.model_name.lower()
        is_instruct = any(x in model_name for x in [
            "instruct", "chat", "deepseek-coder-v2-lite", "deepseek-coder-instruct", "qwen", "chatglm"
        ])
        user_instruction = f"Problem: {problem['text']}\n\n"
        if problem.get('test_list'):
            test_cases_str = "\n".join(problem['test_list'])
            user_instruction += (
                "The function should pass the following tests:\n"
                f"{test_cases_str}\n\n"
            )
        user_instruction += "Provide the complete Python code for the function now."
        if is_instruct:
            system_prompt = (
                "You are an expert Python programmer. Your task is to write a "
                "Python function that solves a programming problem. "
                "Please implement the function in a single, clean block of code. "
                "Do not generate any explanatory text, comments, or markdown formatting like ```python."
            )
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_instruction}
            ]
            return messages
        else:
            prompt = f'"""\n{problem["text"]}\n"""\n'
            return prompt
    
    def _get_few_shot_examples(self) -> List[Dict[str, Any]]:
        """è·å–few-shotç¤ºä¾‹ï¼ˆç‰¹åˆ«é’ˆå¯¹DeepSeekç­‰æ¨¡å‹ï¼‰"""
        
        # MBPPçš„ç»å…¸ç¤ºä¾‹ï¼Œå·²ç»éªŒè¯è¿‡æ•ˆæœ
        examples = [
            {
                "problem": "Write a function to find the similar elements from the given two tuple lists.",
                "test_cases": [
                    "assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)",
                    "assert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)",
                    "assert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)"
                ],
                "solution": """def similar_elements(test_tup1, test_tup2):
  res = tuple(set(test_tup1) & set(test_tup2))
  return (res)"""
            },
            {
                "problem": "Write a python function to identify non-prime numbers.",
                "test_cases": [
                    "assert is_not_prime(2) == False",
                    "assert is_not_prime(10) == True", 
                    "assert is_not_prime(35) == True"
                ],
                "solution": """import math
def is_not_prime(n):
    result = False
    for i in range(2,int(math.sqrt(n)) + 1):
        if n % i == 0:
            result = True
    return result"""
            },
            {
                "problem": "Write a function to find the largest integers from a given list of numbers using heap queue algorithm.",
                "test_cases": [
                    "assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]",
                    "assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75]",
                    "assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]"
                ],
                "solution": """import heapq as hq
def heap_queue_largest(nums,n):
  largest_nums = hq.nlargest(n, nums)
  return largest_nums"""
            }
        ]
        
        # æ ¹æ®æ¨¡å‹é…ç½®é€‰æ‹©ç¤ºä¾‹æ•°é‡
        max_examples = self.model_config.preferred_examples_count
        return examples[:max_examples]
    
    def phase1_beam_search_sampling(self, problems_list: List[tuple], num_beams: int):
        """é˜¶æ®µ1: Beam Searché‡‡æ ·"""
        print("ğŸ” é˜¶æ®µ1: Beam Searché‡‡æ ·")
        
        # è¿›åº¦ä¿å­˜ç›¸å…³
        progress_file = os.path.join(self.output_dir, "sampling_progress.json")
        processed_problems = set()
        
        # åŠ è½½å·²æœ‰è¿›åº¦
        if os.path.exists(progress_file):
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    processed_problems = set(progress_data.get('processed_problems', []))
                print(f"ğŸ“‚ åŠ è½½å·²æœ‰è¿›åº¦: å·²å¤„ç† {len(processed_problems)} ä¸ªé—®é¢˜")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è¿›åº¦å¤±è´¥: {e}")
        
        # è¿‡æ»¤å·²å¤„ç†çš„é—®é¢˜
        remaining_problems = [(task_id, problem) for task_id, problem in problems_list 
                            if str(task_id) not in processed_problems]
        
        if len(remaining_problems) == 0:
            print("âœ… æ‰€æœ‰é—®é¢˜å·²å¤„ç†å®Œæˆ")
            return
        
        print(f"ğŸ“Š å‰©ä½™å¾…å¤„ç†é—®é¢˜: {len(remaining_problems)}")
        
        for task_id, problem in tqdm(remaining_problems, desc="Beam Searché‡‡æ ·"):
            prompt = self.format_prompt(problem)
            
            try:
                # ç”Ÿæˆå€™é€‰è§£
                candidates = self.adapter.generate(
                    prompt, 
                    num_beams=num_beams,
                    temperature=0.2,  # æ›´ä½æ¸©åº¦
                    max_tokens=512
                )
                
                # æµ‹è¯•æ¯ä¸ªå€™é€‰è§£
                for candidate in candidates:
                    code = candidate['code']
                    if not code.strip():
                        continue
                    
                    try:
                        # è¿è¡Œæµ‹è¯•
                        test_results = run_tests(code, problem['test_list'])
                        passed_tests = sum(1 for r in test_results.values() if r.get('result', False))
                        total_tests = len(test_results)
                        pass_rate = passed_tests / total_tests if total_tests > 0 else 0.0
                        
                        experience = {
                            'problem_id': str(task_id),
                            'problem_text': problem['text'],
                            'code': code,
                            'possibility': candidate['possibility'],
                            'pass_rate': pass_rate,
                            'passed_tests': passed_tests,
                            'total_tests': total_tests,
                            'test_results': test_results,
                            'beam_rank': candidate['beam_rank']
                        }
                        
                        self.experience_buffer.add_experience(experience)
                        
                    except Exception as e:
                        # æµ‹è¯•å¤±è´¥ä¹Ÿè¦è®°å½•
                        experience = {
                            'problem_id': str(task_id),
                            'problem_text': problem['text'],
                            'code': code,
                            'possibility': candidate['possibility'],
                            'pass_rate': 0.0,
                            'passed_tests': 0,
                            'total_tests': len(problem.get('test_list', [])),
                            'error': str(e),
                            'beam_rank': candidate['beam_rank']
                        }
                        self.experience_buffer.add_experience(experience)
                        
            except Exception as e:
                print(f"âš ï¸  é—®é¢˜ {task_id} ç”Ÿæˆå¤±è´¥: {e}")
                continue
            
            # æ›´æ–°è¿›åº¦
            processed_problems.add(str(task_id))
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if len(processed_problems) % getattr(self, 'save_interval', 50) == 0:
                self._save_progress(progress_file, processed_problems)
        
        # æœ€ç»ˆä¿å­˜è¿›åº¦
        self._save_progress(progress_file, processed_problems)
        print(f"âœ… é˜¶æ®µ1å®Œæˆï¼Œå…±å¤„ç† {len(processed_problems)} ä¸ªé—®é¢˜")
    
    def _save_progress(self, progress_file: str, processed_problems: set):
        """ä¿å­˜è¿›åº¦"""
        try:
            progress_data = {
                'processed_problems': list(processed_problems),
                'timestamp': datetime.now().isoformat(),
                'total_experiences': len(self.experience_buffer.get_all_experiences())
            }
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(progress_file), exist_ok=True)
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {len(processed_problems)} ä¸ªé—®é¢˜")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def phase2_pper_training(self, n_iterations: int, batch_size: int):
        """é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾è®­ç»ƒï¼Œæ”¯æŒå›ºå®šæ ·æœ¬é›†"""
        print(f"ğŸ¯ é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾è®­ç»ƒ ({n_iterations} è½®è¿­ä»£)")
        
        if self.finetuning_manager is None:
            print("âš ï¸  è·³è¿‡å¾®è°ƒé˜¶æ®µï¼ˆå½“å‰æ¨¡å¼ä¸æ”¯æŒå¾®è°ƒï¼‰")
            return
        
        # åˆå§‹åŒ–ç”¨äºè®­ç»ƒçš„ç»éªŒåˆ—è¡¨
        training_experiences = None
        
        # å¦‚æœæŒ‡å®šäº†å›ºå®šæ ·æœ¬è·¯å¾„ï¼Œåˆ™æ‰§è¡Œ"é‡‡æ ·ä¸€æ¬¡æˆ–åŠ è½½"é€»è¾‘
        if self.fixed_sample_path:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(self.fixed_sample_path):
                print(f"ğŸ”„ ä»å›ºå®šæ ·æœ¬æ–‡ä»¶åŠ è½½ç»éªŒ: {self.fixed_sample_path}")
                try:
                    with open(self.fixed_sample_path, 'r', encoding='utf-8') as f:
                        training_experiences = json.load(f)
                    print(f"   æˆåŠŸåŠ è½½ {len(training_experiences)} ä¸ªå›ºå®šæ ·æœ¬")
                except Exception as e:
                    print(f"âŒ åŠ è½½å›ºå®šæ ·æœ¬å¤±è´¥: {e}")
                    return
            else:
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™æ‰§è¡Œä¸€æ¬¡é‡‡æ ·å¹¶ä¿å­˜
                print("ğŸ”„ é¦–æ¬¡è¿è¡Œï¼Œæ‰§è¡Œä¸€æ¬¡æ€§é‡‡æ ·å¹¶ä¿å­˜å›ºå®šæ ·æœ¬...")
                all_experiences = self.experience_buffer.get_all_experiences()
                if not all_experiences:
                    print("âš ï¸  ç»éªŒæ± ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé‡‡æ ·å’Œè®­ç»ƒã€‚")
                    return
                
                training_experiences = self.sampler.sample(all_experiences, batch_size)
                
                print(f"ğŸ’¾ å°† {len(training_experiences)} ä¸ªé‡‡æ ·ç»éªŒä¿å­˜åˆ°: {self.fixed_sample_path}")
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(self.fixed_sample_path), exist_ok=True)
                with open(self.fixed_sample_path, 'w', encoding='utf-8') as f:
                    json.dump(training_experiences, f, indent=2, ensure_ascii=False)
        
        # --- ä¸»è®­ç»ƒå¾ªç¯ ---
        for iteration in range(n_iterations):
            print(f"\nğŸ“ˆ è¿­ä»£ {iteration + 1}/{n_iterations}")
            
            # å¦‚æœæ²¡æœ‰ä½¿ç”¨å›ºå®šæ ·æœ¬æ¨¡å¼ï¼Œåˆ™æ¯æ¬¡éƒ½é‡æ–°é‡‡æ ·ï¼ˆåŸå§‹é€»è¾‘ï¼‰
            if not self.fixed_sample_path:
                all_experiences = self.experience_buffer.get_all_experiences()
                if not all_experiences:
                    print("âš ï¸  æ²¡æœ‰å¯ç”¨ç»éªŒï¼Œè·³è¿‡æ­¤è½®è¿­ä»£")
                    continue
                training_experiences = self.sampler.sample(all_experiences, batch_size)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨äºè®­ç»ƒçš„ç»éªŒ
            if not training_experiences:
                print("âš ï¸  æ²¡æœ‰å¯ç”¨äºè®­ç»ƒçš„ç»éªŒï¼Œè·³è¿‡æ­¤è½®è¿­ä»£ã€‚")
                continue
            
            print(f"ğŸ“Š ä½¿ç”¨ {len(training_experiences)} ä¸ªç»éªŒè¿›è¡Œæœ¬è½®è®­ç»ƒ")
            
            # æ‰§è¡Œå¾®è°ƒ
            try:
                self.finetuning_manager.finetune_on_experiences(training_experiences)
                print(f"âœ… è¿­ä»£ {iteration + 1} å¾®è°ƒå®Œæˆ")
            except Exception as e:
                print(f"âŒ è¿­ä»£ {iteration + 1} å¾®è°ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    def get_experiment_results(self) -> Dict[str, Any]:
        """è·å–å®éªŒç»“æœ"""
        stats = self.experience_buffer.get_stats(include_samples=True, max_samples=20)
        all_experiences = self.experience_buffer.get_all_experiences()
        
        results = {
            'experiment_type': 'MBPP_BTP',
            'model_name': self.model_name,
            'mode': self.model_type,
            'target_model': self.model_name, # å› ä¸ºå¾®è°ƒæ¨¡å¼ä¸‹ç›®æ ‡æ¨¡å‹å°±æ˜¯å½“å‰æ¨¡å‹
            'sampling_method': self.sampling_method,
            'sampling_alpha': self.sampling_alpha,
            'p2value_alpha': self.p2value_alpha,
            'experience_stats': stats,
            'all_experiences': all_experiences,  # ä¿å­˜æ‰€æœ‰ç”Ÿæˆçš„ä»£ç å’Œç»“æœ
            'config': self.get_experiment_config()
        }
        
        # æ‰“å°ä¸€äº›ç”Ÿæˆçš„ä»£ç æ ·æœ¬ç”¨äºè°ƒè¯•
        print("\n" + "="*80)
        print("ğŸ” ç”Ÿæˆä»£ç æ ·æœ¬åˆ†æ (ç”¨äºè°ƒè¯•0%é€šè¿‡ç‡é—®é¢˜)")
        print("="*80)
        
        if 'sample_experiences' in stats and stats['sample_experiences']:
            samples = stats['sample_experiences'][:5]  # åªçœ‹å‰5ä¸ª
            for i, exp in enumerate(samples):
                print(f"\nğŸ“ æ ·æœ¬ {i+1}:")
                print(f"   é—®é¢˜ID: {exp.get('problem_id', 'N/A')}")
                print(f"   é€šè¿‡ç‡: {exp.get('pass_rate', 0):.2f}")
                print(f"   ç”Ÿæˆæ¦‚ç‡: {exp.get('possibility', 0):.4f}")
                print(f"   ç”Ÿæˆä»£ç :")
                print("   " + "-"*60)
                code_lines = str(exp.get('code', '')).split('\n')
                for line in code_lines[:10]:  # åªæ˜¾ç¤ºå‰10è¡Œ
                    print(f"   {line}")
                if len(code_lines) > 10:
                    print(f"   ... (è¿˜æœ‰ {len(code_lines)-10} è¡Œ)")
                print("   " + "-"*60)
                
                # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
                if 'test_results' in exp and exp['test_results']:
                    test_results = exp['test_results']
                    passed = sum(1 for r in test_results.values() if r.get('result', False))
                    total = len(test_results)
                    print(f"   æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
                    
                    # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•ï¼ˆå¦‚æœæœ‰ï¼‰
                    failed_tests = [k for k, v in test_results.items() if not v.get('result', False)]
                    if failed_tests:
                        print(f"   å¤±è´¥æµ‹è¯•: {failed_tests[:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥æµ‹è¯•
        
        print("="*80)
        
        return results

    def run_experiment(self, max_problems: int = 100, num_beams: int = 5,
                      n_iterations: int = 3, batch_size: int = 100,
                      use_cached_sampling: bool = True, force_resample: bool = False) -> Dict[str, Any]:
        """è¿è¡ŒBTPå®éªŒï¼ˆæ”¯æŒé‡‡æ ·ç¼“å­˜å’Œå›ºå®šæ ·æœ¬ï¼‰"""
        problems_list = self.run_on_problem_subset(max_problems)
        
        print(f"å¼€å§‹è¿è¡ŒBTPå®éªŒï¼Œå…± {len(problems_list)} ä¸ªé—®é¢˜")
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç¼“å­˜çš„é‡‡æ ·ç»“æœ
        if use_cached_sampling and not force_resample:
            if self.check_sampling_cache_exists(max_problems, num_beams):
                print("ğŸ” å‘ç°ç°æœ‰é‡‡æ ·ç¼“å­˜ï¼Œå°è¯•åŠ è½½...")
                if self.load_sampling_results(max_problems, num_beams):
                    print("âœ… æˆåŠŸåŠ è½½ç¼“å­˜çš„é‡‡æ ·ç»“æœï¼Œè·³è¿‡é‡‡æ ·é˜¶æ®µ")
                    # ç›´æ¥è¿›å…¥é˜¶æ®µ2
                    self.phase2_pper_training(n_iterations, batch_size)
                    return self.get_experiment_results()
                else:
                    print("âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°é‡‡æ ·")
        
        # é˜¶æ®µ1: Beam Searché‡‡æ ·
        print("ğŸ” å¼€å§‹é˜¶æ®µ1: Beam Searché‡‡æ ·")
        self.phase1_beam_search_sampling(problems_list, num_beams)
        
        # ä¿å­˜é‡‡æ ·ç»“æœï¼ˆå¦‚æœå¯ç”¨äº†ç¼“å­˜åŠŸèƒ½ï¼‰
        if use_cached_sampling:
            self.save_sampling_results(max_problems, num_beams)
        
        # å¤„ç†å›ºå®šæ ·æœ¬åŠŸèƒ½ï¼ˆåœ¨localæ¨¡å¼ä¸‹ä¹Ÿæ”¯æŒï¼‰
        if self.fixed_sample_path and self.model_type == "local":
            print("ğŸ”„ æœ¬åœ°æ¨¡å¼ï¼šå¤„ç†å›ºå®šæ ·æœ¬åŠŸèƒ½...")
            all_experiences = self.experience_buffer.get_all_experiences()
            if all_experiences:
                # æ‰§è¡Œä¸€æ¬¡é‡‡æ ·å¹¶ä¿å­˜
                sampled_experiences = self.sampler.sample(all_experiences, batch_size)
                print(f"ğŸ’¾ å°† {len(sampled_experiences)} ä¸ªé‡‡æ ·ç»éªŒä¿å­˜åˆ°: {self.fixed_sample_path}")
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(self.fixed_sample_path), exist_ok=True)
                with open(self.fixed_sample_path, 'w', encoding='utf-8') as f:
                    json.dump(sampled_experiences, f, indent=2, ensure_ascii=False)
                print("âœ… å›ºå®šæ ·æœ¬ä¿å­˜å®Œæˆ")
            else:
                print("âš ï¸  ç»éªŒæ± ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜å›ºå®šæ ·æœ¬")
        
        # é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾è®­ç»ƒ
        self.phase2_pper_training(n_iterations, batch_size)
        
        return self.get_experiment_results()


def main():
    parser = argparse.ArgumentParser(
        description='MBPPæ•°æ®é›†çš„BTPå®éªŒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

=== å·¥ä½œæµAï¼šè‡ªåŠ¨åŒ–ç»éªŒæ± ç¼“å­˜ï¼ˆæ¨èï¼‰ ===

1. é¦–æ¬¡è¿è¡Œ - ç”Ÿæˆç¼“å­˜ï¼ˆä½¿ç”¨finetuneæ¨¡å¼ï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 --num-beams 5

2. åç»­è¿è¡Œ - ä½¿ç”¨ç¼“å­˜ï¼ˆä¿®æ”¹è¶…å‚æ•°åå¿«é€Ÿå®éªŒï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 --num-beams 5

=== å·¥ä½œæµBï¼šå›ºå®šè®­ç»ƒæ ·æœ¬ï¼ˆç¡®ä¿å®éªŒä¸€è‡´æ€§ï¼‰ ===

3. ç”Ÿæˆå›ºå®šæ ·æœ¬ï¼ˆlocalæ¨¡å¼ï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode local --max-problems 100 \\
     --fixed-sample-path ./fixed_samples.json

4. ä½¿ç”¨å›ºå®šæ ·æœ¬è¿›è¡Œå¾®è°ƒ:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 \\
     --fixed-sample-path ./fixed_samples.json

=== å…¶ä»–æ¨¡å¼ ===

5. OpenAIå®éªŒ:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model gpt-4 --mode openai \\
     --api-key YOUR_KEY --max-problems 30

6. DeepSeek APIå®éªŒ:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-chat --mode deepseek \\
     --api-key YOUR_KEY --max-problems 30

=== é«˜çº§é€‰é¡¹ ===

7. å¼ºåˆ¶é‡æ–°é‡‡æ ·ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 \\
     --force-resample

8. ç¦ç”¨ç¼“å­˜åŠŸèƒ½:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 \\
     --use-cached-sampling false
        """)
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--mode', type=str, default='local',
                       choices=['local', 'finetune', 'openai', 'deepseek'],
                       help='å®éªŒæ¨¡å¼')
    parser.add_argument('--target-model', type=str, default=None,
                       help='ç›®æ ‡å¾®è°ƒæ¨¡å‹ï¼ˆä»…å¾®è°ƒæ¨¡å¼éœ€è¦ï¼‰')
    
    # APIå‚æ•°
    parser.add_argument('--api-key', type=str,
                       help='APIå¯†é’¥')
    
    # å®éªŒå‚æ•°
    parser.add_argument('--max-problems', type=int, default=50,
                       help='æœ€å¤§é—®é¢˜æ•°é‡')
    parser.add_argument('--num-beams', type=int, default=5,
                       help='Beam Searchæ•°é‡')
    parser.add_argument('--n-iterations', type=int, default=2,
                       help='PPERè®­ç»ƒè¿­ä»£æ¬¡æ•°')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='è®­ç»ƒæ‰¹å¤§å°')
    parser.add_argument('--output-dir', type=str, default='./mbpp_btp_checkpoints',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆä»…å¾®è°ƒæ¨¡å¼ï¼‰')
    
    # BTPç®—æ³•å‚æ•°
    parser.add_argument('--sampling-method', type=str, default='power',
                       choices=['power', 'rank'],
                       help='é‡‡æ ·æ–¹æ³•')
    parser.add_argument('--sampling-alpha', type=float, default=1.0,
                       help='é‡‡æ ·Î±å‚æ•°')
    parser.add_argument('--p2value-alpha', type=float, default=0.5,
                       help='P2Valueæƒé‡Î±')
    
    # é‡‡æ ·ç¼“å­˜å‚æ•°
    parser.add_argument('--use-cached-sampling', action='store_true', default=True,
                       help='ä½¿ç”¨ç¼“å­˜çš„é‡‡æ ·ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰')
    parser.add_argument('--force-resample', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°é‡‡æ ·ï¼Œå¿½ç•¥ç¼“å­˜')
    
    # å›ºå®šæ ·æœ¬å‚æ•°
    parser.add_argument('--fixed-sample-path', type=str, default=None,
                       help='æŒ‡å®šä¸€ä¸ªJSONæ–‡ä»¶ï¼Œä»ä¸­åŠ è½½å›ºå®šæ ·æœ¬')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ—¥å¿—')
    parser.add_argument('--gpu-id', type=int, default=None,
                       help='æŒ‡å®šGPUè®¾å¤‡ID')
    parser.add_argument('--save-interval', type=int, default=50,
                       help='æ¯å¤„ç†å¤šå°‘ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡è¿›åº¦')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    # è®¾ç½®GPUè®¾å¤‡
    if args.gpu_id is not None and torch.cuda.is_available():
        if args.gpu_id < torch.cuda.device_count():
            torch.cuda.set_device(args.gpu_id)
            print(f"ğŸ¯ ä½¿ç”¨GPU {args.gpu_id}: {torch.cuda.get_device_name(args.gpu_id)}")
        else:
            print(f"âš ï¸  GPU {args.gpu_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤GPU")
    
    # æ‰“å°é…ç½®
    print("ğŸš€ MBPP BTPå®éªŒé…ç½®:")
    print(f"  æ¨¡å‹: {args.model}")
    print(f"  æ¨¡å¼: {args.mode}")
    print(f"  æœ€å¤§é—®é¢˜æ•°: {args.max_problems}")
    print(f"  é‡‡æ ·æ–¹æ³•: {args.sampling_method}")
    print(f"  é‡‡æ ·Alpha: {args.sampling_alpha}")
    print(f"  P2Value Alpha: {args.p2value_alpha}")
    if args.mode == "finetune":
        print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    if args.fixed_sample_path:
        print(f"  å›ºå®šæ ·æœ¬è·¯å¾„: {args.fixed_sample_path}")
    if torch.cuda.is_available():
        print(f"  å½“å‰GPU: {torch.cuda.current_device()} - {torch.cuda.get_device_name()}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(torch.cuda.current_device()).total_memory / 1024**3:.1f} GB")
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = MBBPBTPExperiment(
        model_name=args.model,
        model_type=args.mode,  # ä¿®å¤å‚æ•°åï¼šmode -> model_type
        api_key=args.api_key,
        api_base=None,  # API base å‚æ•°åœ¨ ModelAdapter ä¸­å¤„ç†
        sampling_method=args.sampling_method,
        sampling_alpha=args.sampling_alpha,
        p2value_alpha=args.p2value_alpha,
        output_dir=args.output_dir, # ä¼ é€’output_dirå‚æ•°
        fixed_sample_path=args.fixed_sample_path # ä¼ é€’fixed_sample_pathå‚æ•°
    )
    
    # è®¾ç½®ä¿å­˜é—´éš”
    experiment.save_interval = args.save_interval
    
    # è¿è¡Œå®éªŒ
    try:
        results = experiment.run_experiment(
            max_problems=args.max_problems,
            num_beams=args.num_beams,
            n_iterations=args.n_iterations,
            batch_size=args.batch_size
        )
        
        # ä¿å­˜ç»“æœ
        result_file = experiment.save_results(results, "btp_experiment")
        
        print("\nâœ… å®éªŒå®Œæˆ!")
        print(f"ğŸ“Š å®éªŒç»Ÿè®¡:")
        for key, value in results['experience_stats'].items():
            print(f"  {key}: {value}")
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        return 0
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main()) 