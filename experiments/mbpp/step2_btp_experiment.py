#!/usr/bin/env python3
"""
MBPPæ•°æ®é›†çš„BTPå®éªŒ (Beam Search + Testing + Prioritized Experience Replay)

BTPç®—æ³•åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼š
1. é˜¶æ®µ1: Beam Searché‡‡æ · + æµ‹è¯•éªŒè¯
2. é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾ (PPER) è®­ç»ƒ

æ”¯æŒçš„åŠŸèƒ½ï¼š
- æœ¬åœ°æ¨¡å‹çš„BTPå®éªŒ
- æœ¬åœ°æ¨¡å‹çš„BTPå¾®è°ƒå®éªŒ  
- OpenAI APIçš„BTPå®éªŒ
- DeepSeek APIçš„BTPå®éªŒ

ä½¿ç”¨ç¤ºä¾‹ï¼š
1. æœ¬åœ°æ¨¡å‹BTPå®éªŒï¼š
   python experiments/mbpp/step2_btp_experiment.py --model deepseek-ai/deepseek-coder-1.3b-instruct --mode local

2. æœ¬åœ°æ¨¡å‹å¾®è°ƒï¼š
   python experiments/mbpp/step2_btp_experiment.py --model deepseek-ai/deepseek-coder-1.3b-instruct --target-model deepseek-ai/deepseek-coder-1.3b-instruct --mode finetune

3. OpenAIå®éªŒï¼š
   python experiments/mbpp/step2_btp_experiment.py --model gpt-4 --mode openai --api-key YOUR_KEY
"""

import os
import sys
import json
import argparse
import numpy as np
import random
import torch
import math
import logging
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict, deque
from datetime import datetime
from typing import List, Dict, Any, Optional
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

# å¯¼å…¥å…±äº«åŸºç¡€ç±»
from experiments.shared.base_experiment import Step2BTPExperiment  
from experiments.shared.dataset_configs import MBPP_CONFIG
from experiments.shared.common_utils import safe_execute_code, load_mbpp_problems

# æ¡ä»¶å¯¼å…¥
try:
    from transformers import (
        AutoModelForCausalLM, 
        AutoTokenizer, 
        Trainer, 
        TrainingArguments,
        DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    HF_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘HuggingFaceä¾èµ–: {e}")
    HF_AVAILABLE = False

try:
    from eg_cfg.openai_utils import OpenAIClient, OpenAIInferenceError
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# é¡¹ç›®ç›¸å…³å¯¼å…¥
from eg_cfg.mbpp_utils import run_tests
if HF_AVAILABLE:
    from eg_cfg.model_utils import setup_device, load_model, load_tokenizer

from experiments.prompt_templates import get_model_prompt, detect_model_info, validate_model_compatibility
from experiments.shared.model_configs import get_model_config, get_optimal_generation_params


class ModelAdapter:
    """ç»Ÿä¸€æ¨¡å‹é€‚é…å™¨ - æ”¯æŒæœ¬åœ°å’ŒAPIæ¨¡å‹"""
    
    def __init__(self, model_name: str, model_type: str = "local", 
                 api_key: str = None, api_base: str = None, **kwargs):
        self.model_name = model_name
        self.model_type = model_type
        self.api_key = api_key
        self.api_base = api_base
        self.kwargs = kwargs
        
        self.model = None
        self.tokenizer = None
        self.device = None
        self._setup_model()
    
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        if self.model_type == "local" or self.model_type == "finetune":
            self._setup_local_model()
        elif self.model_type == "openai":
            self._setup_openai_model()
        elif self.model_type in ["deepseek", "api"]:
            self._setup_api_model()
    
    def _setup_local_model(self):
        """è®¾ç½®æœ¬åœ°æ¨¡å‹"""
        if not HF_AVAILABLE:
            raise ImportError("æœ¬åœ°æ¨¡å‹éœ€è¦å®‰è£…transformersåº“")
        
        print(f"ğŸ”§ åŠ è½½æœ¬åœ°æ¨¡å‹: {self.model_name}")
        self.device = setup_device()
        self.model, self.tokenizer = load_model(self.model_name, self.device)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def _setup_openai_model(self):
        """è®¾ç½®OpenAIæ¨¡å‹"""
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAIæ¨¡å‹éœ€è¦å®‰è£…openaiç›¸å…³ä¾èµ–")
        
        print(f"ğŸ”§ é…ç½®OpenAIæ¨¡å‹: {self.model_name}")
        self.client = OpenAIClient(api_key=self.api_key, model=self.model_name)
    
    def _setup_api_model(self):
        """è®¾ç½®APIæ¨¡å‹"""
        if not REQUESTS_AVAILABLE:
            raise ImportError("APIæ¨¡å‹éœ€è¦å®‰è£…requestsåº“")
        
        print(f"ğŸ”§ é…ç½®APIæ¨¡å‹: {self.model_name}")
        self.api_headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def generate(self, prompt: str, **generation_kwargs) -> List[Dict]:
        """ç»Ÿä¸€ç”Ÿæˆæ¥å£"""
        if self.model_type == "local" or self.model_type == "finetune":
            return self._generate_local(prompt, **generation_kwargs)
        elif self.model_type == "openai":
            return self._generate_openai(prompt, **generation_kwargs)
        elif self.model_type in ["deepseek", "api"]:
            return self._generate_api(prompt, **generation_kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _generate_local(self, prompt, num_beams: int = 5,
                        temperature: float = 0.8, max_tokens: int = 512,
                        **kwargs) -> list:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆ - æœ€ç»ˆç‰ˆï¼ŒåŒ…å«å¥å£®çš„ä»£ç æå–é€»è¾‘"""
        
        if isinstance(prompt, list):
            input_ids = self.tokenizer.apply_chat_template(
                prompt, add_generation_prompt=True, return_tensors="pt"
            ).to(self.device)
            attention_mask = torch.ones_like(input_ids).to(self.device)
            inputs_for_generate = {'input_ids': input_ids, 'attention_mask': attention_mask}
        else:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
            inputs_for_generate = {k: v.to(self.device) for k, v in inputs.items()}

        input_ids_len = inputs_for_generate['input_ids'].shape[1]

        with torch.no_grad():
            outputs = self.model.generate(**inputs_for_generate, 
                num_beams=num_beams,
                num_return_sequences=num_beams,
                max_new_tokens=max_tokens,
                do_sample=temperature > 0,
                temperature=temperature if temperature > 0 else 1.0,
                return_dict_in_generate=True,
                output_scores=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                use_cache=True
            )

        results = []
        sequences = outputs.sequences
        scores = getattr(outputs, 'sequences_scores', None)
        
        for i, sequence in enumerate(sequences):
            output_ids = sequence[input_ids_len:]
            full_output = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()
            
            # ### å…³é”®ä¿®æ­£ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ä»£ç  ###
            code = full_output
            # å°è¯•åŒ¹é… markdown ä»£ç å—
            match = re.search(r"```(?:python\n)?(.*?)```", full_output, re.DOTALL)
            if match:
                # å¦‚æœæ‰¾åˆ°ï¼Œæå–ä»£ç å—å†…å®¹
                code = match.group(1).strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ° markdown å—ï¼Œå°±å‡å®šæ•´ä¸ªè¾“å‡ºéƒ½æ˜¯ä»£ç 
                # å¹¶å°è¯•ç§»é™¤å¯èƒ½çš„è§£é‡Šæ€§æ–‡å­—ï¼ˆè¿™æ˜¯ä¸€ä¸ªå¤‡ç”¨ç­–ç•¥ï¼‰
                if "Explanation:" in code:
                    code = code.split("Explanation:")[0].strip()
            # ### ä¿®æ­£ç»“æŸ ###

            if scores is not None:
                log_prob = scores[i].item()
                output_len = len(output_ids) if len(output_ids) > 0 else 1
                possibility = min(math.exp(log_prob / output_len), 1.0)
            else:
                log_prob = -10.0
                possibility = 0.5
            
            results.append({
                'code': code, 'possibility': possibility, 'log_prob': log_prob,
                'beam_rank': i, 'sequence_length': len(output_ids)
            })
        
        return results
    
    def _generate_openai(self, prompt: str, num_beams: int = 5, 
                        temperature: float = 0.8, **kwargs) -> List[Dict]:
        """OpenAIæ¨¡å‹ç”Ÿæˆ - æ”¯æŒçœŸå®æ¦‚ç‡çš„ä¿®å¤ç‰ˆæœ¬"""
        try:
            # ä½¿ç”¨æ–°çš„æ¦‚ç‡æ„ŸçŸ¥æ–¹æ³•ï¼Œä¸€æ¬¡æ€§ç”Ÿæˆå¤šä¸ªå€™é€‰
            results = self.client.generate_code_with_probs(
                prompt=prompt,
                temperature=temperature,
                max_tokens=512,
                n=num_beams,  # ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªå€™é€‰ï¼Œæ•ˆç‡æ›´é«˜
                logprobs=True,  # è·å–çœŸå®æ¦‚ç‡
                top_logprobs=5
            )
            
            print(f"âœ… OpenAIç”ŸæˆæˆåŠŸ: {len(results)}ä¸ªå€™é€‰ï¼Œæ¦‚ç‡èŒƒå›´: {min(r['possibility'] for r in results):.4f} - {max(r['possibility'] for r in results):.4f}")
            return results
            
        except Exception as e:
            print(f"âŒ OpenAIç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›ç©ºç»“æœè€Œä¸æ˜¯å‡æ¦‚ç‡
            return [{
                'code': '',
                'possibility': 0.0,
                'log_prob': -100.0,
                'beam_rank': i,
                'sequence_length': 0,
                'error': str(e)
            } for i in range(num_beams)]
    
    def _generate_api(self, prompt: str, num_beams: int = 5, 
                     temperature: float = 0.8, **kwargs) -> List[Dict]:
        """APIæ¨¡å‹ç”Ÿæˆ"""
        results = []
        
        for i in range(num_beams):
            try:
                payload = {
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": 512
                }
                
                response = requests.post(
                    self.api_base or "https://api.deepseek.com/v1/chat/completions",
                    headers=self.api_headers,
                    json=payload,
                    timeout=30
                )
                
                if response.status_code == 200:
                    data = response.json()
                    content = data['choices'][0]['message']['content']
                    
                    results.append({
                        'code': content,
                        'possibility': 0.7,
                        'log_prob': -8.0,
                        'beam_rank': i,
                        'sequence_length': len(content)
                    })
                else:
                    raise Exception(f"APIé”™è¯¯: {response.status_code}")
                    
            except Exception as e:
                print(f"APIç”Ÿæˆå¤±è´¥ (beam {i}): {e}")
                results.append({
                    'code': '',
                    'possibility': 0.0,
                    'log_prob': -100.0,
                    'beam_rank': i,
                    'sequence_length': 0,
                    'error': str(e)
                })
        
        return results


class P2ValueCalculator:
    """P2Valueè®¡ç®—å™¨ - ç»“åˆå¯èƒ½æ€§å’Œé€šè¿‡ç‡"""
    
    def __init__(self, alpha: float = 0.5):
        self.alpha = alpha
    
    def calculate_p2value(self, possibility: float, pass_rate: float) -> float:
        """è®¡ç®—P2Value = Î± * possibility + (1-Î±) * pass_rate"""
        return self.alpha * possibility + (1 - self.alpha) * pass_rate
    
    def calculate_p2value_extended(self, log_prob=None, sequence_length=None, 
                                 possibility=None, passed_tests=0, total_tests=1):
        """æ‰©å±•P2Valueè®¡ç®—ï¼Œè€ƒè™‘æ›´å¤šå› ç´ """
        if possibility is None and log_prob is not None:
            possibility = min(math.exp(log_prob / max(sequence_length, 1)), 1.0)
        
        pass_rate = passed_tests / max(total_tests, 1)
        
        if possibility is None:
            possibility = 0.5
        
        return self.calculate_p2value(possibility, pass_rate)


class PrioritizedSampler:
    """ä¼˜å…ˆé‡‡æ ·å™¨ - åŸºäºP2Valueè¿›è¡Œé‡‡æ ·"""
    
    def __init__(self, sampling_method: str = "power", alpha: float = 1.0):
        self.sampling_method = sampling_method
        self.alpha = alpha
        
        if sampling_method not in ["power", "rank"]:
            raise ValueError(f"ä¸æ”¯æŒçš„é‡‡æ ·æ–¹æ³•: {sampling_method}")
    
    def sample(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """é‡‡æ ·ç»éªŒ"""
        if len(experiences) <= batch_size:
            return experiences
        
        if self.sampling_method == "power":
            return self._power_sampling(experiences, batch_size)
        elif self.sampling_method == "rank":
            return self._rank_sampling(experiences, batch_size)
    
    def _power_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """å¹‚é‡‡æ ·"""
        # è®¡ç®—æƒé‡
        weights = []
        for exp in experiences:
            p2value = exp.get('p2value', 0.0)
            weight = max(p2value ** self.alpha, 1e-8)
            weights.append(weight)
        
        weights = np.array(weights)
        probabilities = weights / weights.sum()
        
        # é‡‡æ ·ç´¢å¼•
        indices = np.random.choice(
            len(experiences), 
            size=batch_size, 
            replace=False, 
            p=probabilities
        )
        
        return [experiences[i] for i in indices]
    
    def _rank_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """æ’åºé‡‡æ ·"""
        # æŒ‰P2Valueæ’åº
        sorted_experiences = sorted(
            experiences, 
            key=lambda x: x.get('p2value', 0.0), 
            reverse=True
        )
        
        # è®¡ç®—æ’åºæƒé‡
        weights = [1.0 / (rank + 1) ** self.alpha for rank in range(len(sorted_experiences))]
        weights = np.array(weights)
        probabilities = weights / weights.sum()
        
        # é‡‡æ ·
        indices = np.random.choice(
            len(sorted_experiences), 
            size=batch_size, 
            replace=False, 
            p=probabilities
        )
        
        return [sorted_experiences[i] for i in indices]


class ExperienceReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self.experiences = deque(maxlen=max_size)
        self.p2calculator = P2ValueCalculator()
    
    def add_experience(self, experience: Dict):
        """æ·»åŠ ç»éªŒ"""
        # è®¡ç®—P2Value
        experience['p2value'] = self.p2calculator.calculate_p2value_extended(
            possibility=experience.get('possibility', 0.5),
            passed_tests=experience.get('passed_tests', 0),
            total_tests=experience.get('total_tests', 1)
        )
        
        self.experiences.append(experience)
    
    def get_all_experiences(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ç»éªŒ"""
        return list(self.experiences)
    
    def get_stats(self, include_samples: bool = False, max_samples: int = 10) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.experiences:
            return {}
        
        p2values = [exp.get('p2value', 0.0) for exp in self.experiences]
        pass_rates = [exp.get('pass_rate', 0.0) for exp in self.experiences]
        
        stats = {
            'total_experiences': len(self.experiences),
            'avg_p2value': np.mean(p2values),
            'max_p2value': np.max(p2values), 
            'min_p2value': np.min(p2values),
            'avg_pass_rate': np.mean(pass_rates),
            'fully_passed_count': sum(1 for pr in pass_rates if pr >= 1.0),
            'zero_passed_count': sum(1 for pr in pass_rates if pr == 0.0)
        }
        
        # å¦‚æœéœ€è¦åŒ…å«æ ·æœ¬æ•°æ®ï¼Œæ·»åŠ ä¸€äº›ä»£è¡¨æ€§æ ·æœ¬
        if include_samples:
            samples = []
            
            # è·å–é€šè¿‡ç‡æœ€é«˜çš„æ ·æœ¬
            best_experiences = sorted(self.experiences, key=lambda x: x.get('pass_rate', 0), reverse=True)[:max_samples//2]
            samples.extend(best_experiences)
            
            # æ·»åŠ ä¸€äº›éšæœºæ ·æœ¬
            import random
            remaining_samples = max_samples - len(samples)
            if remaining_samples > 0 and len(self.experiences) > len(samples):
                random_experiences = random.sample(
                    [exp for exp in self.experiences if exp not in samples], 
                    min(remaining_samples, len(self.experiences) - len(samples))
                )
                samples.extend(random_experiences)
            
            stats['sample_experiences'] = samples
        
        return stats


class MBTPFineTuningManager:
    """MBPP BTPå¾®è°ƒç®¡ç†å™¨"""
    
    def __init__(self, model_adapter: ModelAdapter, use_lora: bool = True, 
                 output_dir: str = "./mbpp_btp_checkpoints", **kwargs):
        self.model_adapter = model_adapter
        self.use_lora = use_lora
        self.output_dir = output_dir
        # LoRAå°†åœ¨ finetune_on_experiences è°ƒç”¨æ—¶æ ¹æ®å‚æ•°åŠ¨æ€è®¾ç½®
    
    def _setup_lora(self, r: int, lora_alpha: int, lora_dropout: float):
        """è®¾ç½®LoRAå¾®è°ƒï¼Œä½¿ç”¨ä¼ å…¥çš„å‚æ•°"""
        if not self.use_lora or not HF_AVAILABLE:
            return

        # é¿å…é‡å¤åº”ç”¨ LoRA
        if hasattr(self.model_adapter.model, 'peft_config'):
            print("â„¹ï¸ LoRA é…ç½®å·²å­˜åœ¨ï¼Œè·³è¿‡è®¾ç½®ã€‚")
            return

        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"]
        )
        
        self.model_adapter.model = get_peft_model(self.model_adapter.model, lora_config)
        print(f"âœ… LoRAé…ç½®å®Œæˆ (r={r}, alpha={lora_alpha}, dropout={lora_dropout})")
    
    def finetune_on_experiences(self, experiences: List[Dict], 
                                 learning_rate: float = 1e-6, warmup_steps: int = 10,
                                 per_device_batch_size: int = 2, grad_accum_steps: int = 4,
                                 lora_r: int = 16, lora_alpha: int = 32, lora_dropout: float = 0.1,
                                 num_train_epochs: int = 1,
                                 training_args: Optional[TrainingArguments] = None, **kwargs) -> None:
        """åŸºäºç»éªŒè¿›è¡Œå¾®è°ƒï¼Œä½¿ç”¨ä¼ å…¥çš„å‚æ•°"""
        if self.model_adapter.model_type not in ["local", "finetune"]:
            print("âš ï¸  å¾®è°ƒä»…æ”¯æŒæœ¬åœ°æ¨¡å‹")
            return
        
        # æ ¹æ®ä¼ å…¥å‚æ•°åŠ¨æ€è®¾ç½® LoRA
        self._setup_lora(r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout)

        train_dataset = self._prepare_training_dataset(experiences)
        
        if training_args is None:
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                num_train_epochs=num_train_epochs,
                per_device_train_batch_size=per_device_batch_size,
                gradient_accumulation_steps=grad_accum_steps,
                warmup_steps=warmup_steps,
                learning_rate=learning_rate,
                fp16=True,
                logging_steps=5,
                save_steps=100,
                remove_unused_columns=False,
            )
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.model_adapter.tokenizer,
            mlm=False,
        )
        
        trainer = Trainer(
            model=self.model_adapter.model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator,
        )
        
        print(f"ğŸš€ å¼€å§‹å¾®è°ƒ... æ¨¡å‹å°†ä¿å­˜åˆ°: {self.output_dir}")
        trainer.train()
        trainer.save_model()
        print("âœ… å¾®è°ƒå®Œæˆ")
    
    def _prepare_training_dataset(self, experiences: List[Dict]) -> Dataset:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®é›† - ä¸æ¨ç†æ—¶æ ¼å¼ç»Ÿä¸€ï¼ŒæŒ‡ä»¤æ¨¡å‹ç”¨chatæ¨¡æ¿
        """
        processed_texts = []
        for exp in experiences:
            problem_dict = {'text': exp['problem_text'], 'test_list': list(exp.get('test_results', {}).keys())}
            model_name = self.model_adapter.model_name.lower()
            is_instruct = any(x in model_name for x in ["instruct", "chat", "deepseek-coder-v2-lite", "deepseek-coder-instruct", "qwen", "chatglm"])
            if is_instruct:
                system_prompt = (
                    "You are an expert Python programmer. Your task is to write a "
                    "Python function that solves a programming problem. "
                    "Please implement the function in a single, clean block of code. "
                    "Do not generate any explanatory text, comments, or markdown formatting like ```python."
                )
                user_instruction = f"Problem: {problem_dict['text']}\n\n"
                if problem_dict.get('test_list'):
                    test_cases_str = "\n".join(problem_dict['test_list'])
                    user_instruction += (
                        "The function should pass the following tests:\n"
                        f"{test_cases_str}\n\n"
                    )
                user_instruction += "Provide the complete Python code for the function now."
                response_code = exp['code']
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_instruction},
                    {"role": "assistant", "content": response_code}
                ]
                formatted_text = self.model_adapter.tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=False
                ) + self.model_adapter.tokenizer.eos_token
            else:
                prompt = f'"""\n{problem_dict["text"]}\n"""\n'
                formatted_text = prompt + exp['code'] + self.model_adapter.tokenizer.eos_token
            processed_texts.append(formatted_text)
        def tokenize_function(examples):
            tokenized = self.model_adapter.tokenizer(
                examples['text'],
                truncation=True,
                padding=True,
                max_length=1024,
                return_tensors="pt"
            )
            tokenized["labels"] = tokenized["input_ids"].clone()
            return tokenized
        dataset = Dataset.from_dict({'text': processed_texts})
        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])
        return tokenized_dataset


class MBBPBTPExperiment(Step2BTPExperiment):
    """MBPPæ•°æ®é›†çš„BTPå®éªŒ - ä½¿ç”¨æ™ºèƒ½Prompté€‚é…ç³»ç»Ÿ"""
    
    def __init__(self, model_name: str = None, model_type: str = "local", 
                 api_key: str = None, api_base: str = None,
                 sampling_method: str = "power", sampling_alpha: float = 1.0, 
                 p2value_alpha: float = 0.5, output_dir: str = "./mbpp_btp_checkpoints",
                 fixed_sample_path: str = None):
        
        # è®¾ç½®åŸºæœ¬æ¨¡å‹ä¿¡æ¯
        self.model_name = model_name or "deepseek-ai/deepseek-coder-1.3b-instruct"
        self.model_type = model_type
        self.api_key = api_key
        self.api_base = api_base
        self.output_dir = output_dir
        self.fixed_sample_path = fixed_sample_path
        
        # BTPç‰¹å®šå‚æ•°  
        self.sampling_method = sampling_method
        self.sampling_alpha = sampling_alpha
        self.p2value_alpha = p2value_alpha
        
        # é‡‡æ ·æ•°æ®æŒä¹…åŒ–ç›¸å…³
        self.sampling_cache_dir = os.path.join(output_dir, "sampling_cache")
        os.makedirs(self.sampling_cache_dir, exist_ok=True)
        
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(dataset_name="mbpp", model_name=self.model_name)
        
        # åˆå§‹åŒ–æ™ºèƒ½é…ç½®ï¼ˆåœ¨çˆ¶ç±»æ„é€ å‡½æ•°ä¹‹åï¼‰
        self.model_info = detect_model_info(self.model_name)
        self.model_config = get_model_config(self.model_name)
        self.optimal_params = get_optimal_generation_params(self.model_name, "mbpp")
        
        # éªŒè¯æ¨¡å‹å…¼å®¹æ€§
        compatibility = validate_model_compatibility(self.model_name, "mbpp")
        if compatibility["warnings"]:
            print("âš ï¸  æ¨¡å‹å…¼å®¹æ€§è­¦å‘Š:")
            for warning in compatibility["warnings"]:
                print(f"   - {warning}")
        
        if compatibility["recommendations"]:
            print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for rec in compatibility["recommendations"]:
                print(f"   - {rec}")
        
        # è®¾ç½®adapter
        self.adapter = ModelAdapter(
            model_name=self.model_name,
            model_type=self.model_type,
            api_key=self.api_key or "",  # ç¡®ä¿ä¸æ˜¯None
            api_base=self.api_base or "",  # ç¡®ä¿ä¸æ˜¯None
            **self.optimal_params  # ä½¿ç”¨ä¼˜åŒ–å‚æ•°
        )
        
        # åˆå§‹åŒ–BTPç»„ä»¶
        self.experience_buffer = ExperienceReplayBuffer()
        self.sampler = PrioritizedSampler(sampling_method, sampling_alpha)
        self.p2calculator = P2ValueCalculator(p2value_alpha)
        
        # å¾®è°ƒç®¡ç†å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if model_type == "finetune":
            self.finetuning_manager = MBTPFineTuningManager(
                self.adapter, 
                use_lora=True, 
                output_dir=self.output_dir
            )
        else:
            self.finetuning_manager = None
        
        print(f"ğŸš€ åˆå§‹åŒ–å®Œæˆ:")
        print(f"   æ¨¡å‹: {self.model_name}")
        print(f"   å®¶æ—: {self.model_info.family.value}")
        print(f"   ç±»å‹: {self.model_info.type.value}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   é‡‡æ ·ç¼“å­˜ç›®å½•: {self.sampling_cache_dir}")
        print(f"   ä¼˜åŒ–å‚æ•°: {self.optimal_params}")
    
    def _get_sampling_cache_filename(self, max_problems: int, num_beams: int) -> str:
        """ç”Ÿæˆé‡‡æ ·ç¼“å­˜æ–‡ä»¶åï¼ˆæ™ºèƒ½åˆ¤æ–­ä½¿ç”¨å“ªä¸ªæ¨¡å‹åï¼‰"""
        # å¦‚æœè®¾ç½®äº† teacher_model_for_cache å±æ€§ï¼Œå°±ç”¨å®ƒæ¥æ‰¾ç¼“å­˜æ–‡ä»¶
        # å¦åˆ™ï¼Œå°±ç”¨ self.model_name (å½“å‰è¿è¡Œçš„æ¨¡å‹å)
        model_name_for_cache = getattr(self, 'teacher_model_for_cache', self.model_name)
        
        model_name_safe = model_name_for_cache.replace("/", "_").replace("-", "_")
        return f"sampling_cache_{model_name_safe}_max{max_problems}_beams{num_beams}.json"
    
    def _get_sampling_cache_path(self, max_problems: int, num_beams: int) -> str:
        """è·å–é‡‡æ ·ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        filename = self._get_sampling_cache_filename(max_problems, num_beams)
        return os.path.join(self.sampling_cache_dir, filename)
    
    def save_sampling_results(self, max_problems: int, num_beams: int):
        """ä¿å­˜é‡‡æ ·ç»“æœåˆ°ç¼“å­˜æ–‡ä»¶"""
        cache_path = self._get_sampling_cache_path(max_problems, num_beams)
        
        # è·å–æ‰€æœ‰ç»éªŒæ•°æ®
        all_experiences = self.experience_buffer.get_all_experiences()
        
        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        cache_data = {
            'model_name': self.model_name,
            'max_problems': max_problems,
            'num_beams': num_beams,
            'sampling_method': self.sampling_method,
            'sampling_alpha': self.sampling_alpha,
            'p2value_alpha': self.p2value_alpha,
            'total_experiences': len(all_experiences),
            'experiences': all_experiences,
            'timestamp': datetime.now().isoformat(),
            'cache_version': '1.0'
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ é‡‡æ ·ç»“æœå·²ä¿å­˜åˆ°: {cache_path}")
        print(f"   å…±ä¿å­˜ {len(all_experiences)} ä¸ªç»éªŒæ ·æœ¬")
        
        return cache_path
    
    def load_sampling_results(self, cache_path: str) -> bool:
        """ä»æŒ‡å®šçš„ç¼“å­˜æ–‡ä»¶è·¯å¾„åŠ è½½é‡‡æ ·ç»“æœ"""
        if not os.path.exists(cache_path):
            print(f"âš ï¸  æŒ‡å®šçš„ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
            return False
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # åœ¨å¾®è°ƒæ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸åŠ è½½çš„æ˜¯ä¸€ä¸ªæ ·æœ¬åˆ—è¡¨
            if isinstance(data, list):
                 experiences = data
                 cache_model_name = "N/A (loaded from list)"
                 timestamp = "N/A (loaded from list)"
            # å…¼å®¹æˆ‘ä»¬ä¹‹å‰ä¿å­˜çš„å¸¦å…ƒæ•°æ®çš„ç¼“å­˜æ–‡ä»¶
            elif isinstance(data, dict):
                 experiences = data.get('experiences', [])
                 cache_model_name = data.get('model_name', 'N/A')
                 timestamp = data.get('timestamp', 'N/A')
            else:
                raise ValueError("æ— æ³•è¯†åˆ«çš„ç¼“å­˜æ–‡ä»¶æ ¼å¼")

            for exp in experiences:
                self.experience_buffer.add_experience(exp)
            
            print(f"ğŸ“‚ ä»æŒ‡å®šè·¯å¾„åŠ è½½é‡‡æ ·ç»“æœ: {cache_path}")
            print(f"   å…±åŠ è½½ {len(experiences)} ä¸ªç»éªŒæ ·æœ¬")
            print(f"   ç¼“å­˜ä¸­çš„æ¨¡å‹å: {cache_model_name}")
            print(f"   ç¼“å­˜æ—¶é—´: {timestamp}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def check_sampling_cache_exists(self, max_problems: int, num_beams: int) -> bool:
        """æ£€æŸ¥é‡‡æ ·ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        cache_path = self._get_sampling_cache_path(max_problems, num_beams)
        return os.path.exists(cache_path)
    
    def load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½MBPPé…ç½®"""
        return MBPP_CONFIG
    
    def load_dataset(self) -> Dict[str, Any]:
        """åŠ è½½MBPPæ•°æ®é›†"""
        return load_mbpp_problems()
    
    def format_prompt(self, problem: dict) -> object:
        """
        æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨ç”Ÿæˆåˆé€‚çš„promptæ ¼å¼ã€‚
        - DeepSeek/ChatGLM3/Qwenç­‰æŒ‡ä»¤æ¨¡å‹ï¼šè¿”å›æ¶ˆæ¯åˆ—è¡¨
        - Llama/GPTç­‰åŸºç¡€æ¨¡å‹ï¼šè¿”å›å­—ç¬¦ä¸²
        """
        model_name = self.model_name.lower()
        is_instruct = any(x in model_name for x in [
            "instruct", "chat", "deepseek-coder-v2-lite", "deepseek-coder-instruct", "qwen", "chatglm"
        ])
        user_instruction = f"Problem: {problem['text']}\n\n"
        if problem.get('test_list'):
            test_cases_str = "\n".join(problem['test_list'])
            user_instruction += (
                "The function should pass the following tests:\n"
                f"{test_cases_str}\n\n"
            )
        user_instruction += "Provide the complete Python code for the function now."
        if is_instruct:
            system_prompt = (
                "You are an expert Python programmer. Your task is to write a "
                "Python function that solves a programming problem. "
                "Please implement the function in a single, clean block of code. "
                "Do not generate any explanatory text, comments, or markdown formatting like ```python."
            )
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_instruction}
            ]
            return messages
        else:
            prompt = f'"""\n{problem["text"]}\n"""\n'
            return prompt
    
    def _get_few_shot_examples(self) -> List[Dict[str, Any]]:
        """è·å–few-shotç¤ºä¾‹ï¼ˆç‰¹åˆ«é’ˆå¯¹DeepSeekç­‰æ¨¡å‹ï¼‰"""
        
        # MBPPçš„ç»å…¸ç¤ºä¾‹ï¼Œå·²ç»éªŒè¯è¿‡æ•ˆæœ
        examples = [
            {
                "problem": "Write a function to find the similar elements from the given two tuple lists.",
                "test_cases": [
                    "assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)",
                    "assert similar_elements((1, 2, 3, 4),(5, 4, 3, 7)) == (3, 4)",
                    "assert similar_elements((11, 12, 14, 13),(17, 15, 14, 13)) == (13, 14)"
                ],
                "solution": """def similar_elements(test_tup1, test_tup2):
  res = tuple(set(test_tup1) & set(test_tup2))
  return (res)"""
            },
            {
                "problem": "Write a python function to identify non-prime numbers.",
                "test_cases": [
                    "assert is_not_prime(2) == False",
                    "assert is_not_prime(10) == True", 
                    "assert is_not_prime(35) == True"
                ],
                "solution": """import math
def is_not_prime(n):
    result = False
    for i in range(2,int(math.sqrt(n)) + 1):
        if n % i == 0:
            result = True
    return result"""
            },
            {
                "problem": "Write a function to find the largest integers from a given list of numbers using heap queue algorithm.",
                "test_cases": [
                    "assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],3)==[85, 75, 65]",
                    "assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],2)==[85, 75]",
                    "assert heap_queue_largest( [25, 35, 22, 85, 14, 65, 75, 22, 58],5)==[85, 75, 65, 58, 35]"
                ],
                "solution": """import heapq as hq
def heap_queue_largest(nums,n):
  largest_nums = hq.nlargest(n, nums)
  return largest_nums"""
            }
        ]
        
        # æ ¹æ®æ¨¡å‹é…ç½®é€‰æ‹©ç¤ºä¾‹æ•°é‡
        max_examples = self.model_config.preferred_examples_count
        return examples[:max_examples]
    
    def phase1_beam_search_sampling(self, problems_list: List[tuple], num_beams: int):
        """é˜¶æ®µ1: Beam Searché‡‡æ ·"""
        print("ğŸ” é˜¶æ®µ1: Beam Searché‡‡æ ·")
        
        # è¿›åº¦ä¿å­˜ç›¸å…³
        progress_file = os.path.join(self.output_dir, "sampling_progress.json")
        processed_problems = set()
        
        # åŠ è½½å·²æœ‰è¿›åº¦
        if os.path.exists(progress_file):
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    processed_problems = set(progress_data.get('processed_problems', []))
                print(f"ğŸ“‚ åŠ è½½å·²æœ‰è¿›åº¦: å·²å¤„ç† {len(processed_problems)} ä¸ªé—®é¢˜")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½è¿›åº¦å¤±è´¥: {e}")
        
        # è¿‡æ»¤å·²å¤„ç†çš„é—®é¢˜
        remaining_problems = [(task_id, problem) for task_id, problem in problems_list 
                            if str(task_id) not in processed_problems]
        
        if len(remaining_problems) == 0:
            print("âœ… æ‰€æœ‰é—®é¢˜å·²å¤„ç†å®Œæˆ")
            return
        
        print(f"ğŸ“Š å‰©ä½™å¾…å¤„ç†é—®é¢˜: {len(remaining_problems)}")
        
        for task_id, problem in tqdm(remaining_problems, desc="Beam Searché‡‡æ ·"):
            prompt = self.format_prompt(problem)
            
            try:
                # ç”Ÿæˆå€™é€‰è§£
                candidates = self.adapter.generate(
                    prompt, 
                    num_beams=num_beams,
                    temperature=0.2,  # æ›´ä½æ¸©åº¦
                    max_tokens=512
                )
                
                # æµ‹è¯•æ¯ä¸ªå€™é€‰è§£
                for candidate in candidates:
                    code = candidate['code']
                    if not code.strip():
                        continue
                    
                    try:
                        # è¿è¡Œæµ‹è¯•
                        test_results = run_tests(code, problem['test_list'])
                        passed_tests = sum(1 for r in test_results.values() if r.get('result', False))
                        total_tests = len(test_results)
                        pass_rate = passed_tests / total_tests if total_tests > 0 else 0.0
                        
                        experience = {
                            'problem_id': str(task_id),
                            'problem_text': problem['text'],
                            'code': code,
                            'possibility': candidate['possibility'],
                            'pass_rate': pass_rate,
                            'passed_tests': passed_tests,
                            'total_tests': total_tests,
                            'test_results': test_results,
                            'beam_rank': candidate['beam_rank']
                        }
                        
                        self.experience_buffer.add_experience(experience)
                        
                    except Exception as e:
                        # æµ‹è¯•å¤±è´¥ä¹Ÿè¦è®°å½•
                        experience = {
                            'problem_id': str(task_id),
                            'problem_text': problem['text'],
                            'code': code,
                            'possibility': candidate['possibility'],
                            'pass_rate': 0.0,
                            'passed_tests': 0,
                            'total_tests': len(problem.get('test_list', [])),
                            'error': str(e),
                            'beam_rank': candidate['beam_rank']
                        }
                        self.experience_buffer.add_experience(experience)
                        
            except Exception as e:
                print(f"âš ï¸  é—®é¢˜ {task_id} ç”Ÿæˆå¤±è´¥: {e}")
                continue
            
            # æ›´æ–°è¿›åº¦
            processed_problems.add(str(task_id))
            
            # å®šæœŸä¿å­˜è¿›åº¦
            if len(processed_problems) % getattr(self, 'save_interval', 50) == 0:
                self._save_progress(progress_file, processed_problems)
        
        # æœ€ç»ˆä¿å­˜è¿›åº¦
        self._save_progress(progress_file, processed_problems)
        print(f"âœ… é˜¶æ®µ1å®Œæˆï¼Œå…±å¤„ç† {len(processed_problems)} ä¸ªé—®é¢˜")
    
    def _save_progress(self, progress_file: str, processed_problems: set):
        """ä¿å­˜è¿›åº¦"""
        try:
            progress_data = {
                'processed_problems': list(processed_problems),
                'timestamp': datetime.now().isoformat(),
                'total_experiences': len(self.experience_buffer.get_all_experiences())
            }
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(progress_file), exist_ok=True)
            
            with open(progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜: {len(processed_problems)} ä¸ªé—®é¢˜")
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è¿›åº¦å¤±è´¥: {e}")
    
    def phase2_pper_training(self, n_iterations: int, batch_size: int, min_pass_rate: float = 1.0, num_train_epochs: int = 1, **kwargs):
        """é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾è®­ç»ƒï¼Œå¹¶ä¿å­˜å½“æ¬¡è®­ç»ƒä½¿ç”¨çš„æ ·æœ¬"""
        print(f"ğŸ¯ é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾è®­ç»ƒ ({n_iterations} è½®è¿­ä»£)")
        
        if self.finetuning_manager is None:
            print("âš ï¸  è·³è¿‡å¾®è°ƒé˜¶æ®µï¼ˆå½“å‰æ¨¡å¼ä¸æ”¯æŒå¾®è°ƒï¼‰")
            return
            
        all_experiences = self.experience_buffer.get_all_experiences()
        
        print(f"ç­›é€‰å‰æ€»ç»éªŒæ•°: {len(all_experiences)}")
        filtered_experiences = [exp for exp in all_experiences if exp.get('pass_rate', 0) >= min_pass_rate]
        print(f"ç­›é€‰å (pass_rate >= {min_pass_rate}) ç»éªŒæ•°: {len(filtered_experiences)}")

        if not filtered_experiences:
            print("âš ï¸  æ²¡æœ‰ç¬¦åˆ pass_rate æ¡ä»¶çš„ç»éªŒï¼Œè·³è¿‡å¾®è°ƒã€‚")
            return
            
        for iteration in range(n_iterations):
            print(f"\nğŸ“ˆ è¿­ä»£ {iteration + 1}/{n_iterations}")
            
            training_experiences = self.sampler.sample(filtered_experiences, batch_size)
            
            # ### å…³é”®æ–°å¢ä»£ç ï¼šä¿å­˜åœ¨è¯¥æ¬¡è¿­ä»£ä¸­ç”¨äºè®­ç»ƒçš„æ ·æœ¬ ###
            if training_experiences:
                sampled_data_path = os.path.join(self.output_dir, f"iteration_{iteration+1}_sampled_training_data.json")
                print(f"ğŸ’¾ å°†æœ¬æ¬¡ç”¨äºå¾®è°ƒçš„ {len(training_experiences)} ä¸ªæ ·æœ¬ä¿å­˜åˆ°: {sampled_data_path}")
                with open(sampled_data_path, 'w', encoding='utf-8') as f:
                    json.dump(training_experiences, f, indent=2, ensure_ascii=False)
            # ### æ–°å¢ä»£ç ç»“æŸ ###

            if not training_experiences:
                print("âš ï¸  æ²¡æœ‰é‡‡æ ·åˆ°ç»éªŒï¼Œè·³è¿‡æ­¤è½®è¿­ä»£ã€‚")
                continue
            
            print(f"ğŸ“Š ä½¿ç”¨ {len(training_experiences)} ä¸ªç»éªŒè¿›è¡Œæœ¬è½®è®­ç»ƒ")
            
            try:
                self.finetuning_manager.finetune_on_experiences(training_experiences, num_train_epochs=num_train_epochs, **kwargs)
                print(f"âœ… è¿­ä»£ {iteration + 1} å¾®è°ƒå®Œæˆ")
            except Exception as e:
                print(f"âŒ è¿­ä»£ {iteration + 1} å¾®è°ƒå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue
    
    def get_experiment_results(self) -> Dict[str, Any]:
        """è·å–å®éªŒç»“æœ"""
        stats = self.experience_buffer.get_stats(include_samples=True, max_samples=20)
        all_experiences = self.experience_buffer.get_all_experiences()
        
        results = {
            'experiment_type': 'MBPP_BTP',
            'model_name': self.model_name,
            'mode': self.model_type,
            'target_model': self.model_name, # å› ä¸ºå¾®è°ƒæ¨¡å¼ä¸‹ç›®æ ‡æ¨¡å‹å°±æ˜¯å½“å‰æ¨¡å‹
            'sampling_method': self.sampling_method,
            'sampling_alpha': self.sampling_alpha,
            'p2value_alpha': self.p2value_alpha,
            'experience_stats': stats,
            'all_experiences': all_experiences,  # ä¿å­˜æ‰€æœ‰ç”Ÿæˆçš„ä»£ç å’Œç»“æœ
            'config': self.get_experiment_config()
        }
        
        # æ‰“å°ä¸€äº›ç”Ÿæˆçš„ä»£ç æ ·æœ¬ç”¨äºè°ƒè¯•
        print("\n" + "="*80)
        print("ğŸ” ç”Ÿæˆä»£ç æ ·æœ¬åˆ†æ (ç”¨äºè°ƒè¯•0%é€šè¿‡ç‡é—®é¢˜)")
        print("="*80)
        
        if 'sample_experiences' in stats and stats['sample_experiences']:
            samples = stats['sample_experiences'][:5]  # åªçœ‹å‰5ä¸ª
            for i, exp in enumerate(samples):
                print(f"\nğŸ“ æ ·æœ¬ {i+1}:")
                print(f"   é—®é¢˜ID: {exp.get('problem_id', 'N/A')}")
                print(f"   é€šè¿‡ç‡: {exp.get('pass_rate', 0):.2f}")
                print(f"   ç”Ÿæˆæ¦‚ç‡: {exp.get('possibility', 0):.4f}")
                print(f"   ç”Ÿæˆä»£ç :")
                print("   " + "-"*60)
                code_lines = str(exp.get('code', '')).split('\n')
                for line in code_lines[:10]:  # åªæ˜¾ç¤ºå‰10è¡Œ
                    print(f"   {line}")
                if len(code_lines) > 10:
                    print(f"   ... (è¿˜æœ‰ {len(code_lines)-10} è¡Œ)")
                print("   " + "-"*60)
                
                # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
                if 'test_results' in exp and exp['test_results']:
                    test_results = exp['test_results']
                    passed = sum(1 for r in test_results.values() if r.get('result', False))
                    total = len(test_results)
                    print(f"   æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
                    
                    # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•ï¼ˆå¦‚æœæœ‰ï¼‰
                    failed_tests = [k for k, v in test_results.items() if not v.get('result', False)]
                    if failed_tests:
                        print(f"   å¤±è´¥æµ‹è¯•: {failed_tests[:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥æµ‹è¯•
        
        print("="*80)
        
        return results

    def run_on_problem_subset(self, max_problems: int, offset: int = 0) -> List[tuple]:
        """åŠ è½½å¹¶è¿”å›æŒ‡å®šæ•°é‡å’Œåç§»é‡çš„é—®é¢˜å­é›†"""
        problems = self.load_dataset()
        problems_list = sorted(problems.items())
        
        # åº”ç”¨åç§»é‡å’Œé™åˆ¶
        start_index = min(offset, len(problems_list))
        end_index = min(start_index + max_problems, len(problems_list))
        
        return problems_list[start_index:end_index]

    def run_experiment(self, max_problems: int = 100, problem_offset: int = 0, num_beams: int = 5,
                      n_iterations: int = 3, batch_size: int = 100,
                      use_cached_sampling: bool = True, force_resample: bool = False,
                      sample_cache_path: Optional[str] = None, num_train_epochs: int = 1, **kwargs) -> Dict[str, Any]:
        """è¿è¡ŒBTPå®éªŒï¼ˆæ”¯æŒæ‰‹åŠ¨æŒ‡å®šç¼“å­˜å’Œæ‰€æœ‰å¾®è°ƒå‚æ•°ï¼‰"""
        
        should_sample = True
        
        # ### æ ¸å¿ƒä¿®æ”¹ï¼šä¼˜å…ˆå¤„ç†æ‰‹åŠ¨æŒ‡å®šçš„ç¼“å­˜è·¯å¾„ ###
        if sample_cache_path:
            if self.load_sampling_results(sample_cache_path):
                print("âœ… æˆåŠŸä»æ‰‹åŠ¨æŒ‡å®šçš„è·¯å¾„åŠ è½½ç¼“å­˜ï¼Œè·³è¿‡é‡‡æ ·é˜¶æ®µã€‚")
                should_sample = False
            else:
                print("âš ï¸  æ— æ³•ä»æ‰‹åŠ¨è·¯å¾„åŠ è½½ç¼“å­˜ï¼Œå°†ç»§ç»­æ‰§è¡Œé‡‡æ ·æµç¨‹ã€‚")
        # å¦‚æœæ²¡æœ‰æ‰‹åŠ¨æŒ‡å®šè·¯å¾„ï¼Œåˆ™ä½¿ç”¨è‡ªåŠ¨æŸ¥æ‰¾é€»è¾‘
        elif use_cached_sampling and not force_resample:
            auto_cache_path = self._get_sampling_cache_path(max_problems, num_beams)
            if os.path.exists(auto_cache_path):
                print(f"ğŸ” å‘ç°è‡ªåŠ¨æŸ¥æ‰¾çš„ç¼“å­˜æ–‡ä»¶ï¼Œå°è¯•åŠ è½½: {auto_cache_path}")
                if self.load_sampling_results(auto_cache_path):
                    print("âœ… æˆåŠŸåŠ è½½è‡ªåŠ¨æŸ¥æ‰¾åˆ°çš„ç¼“å­˜ï¼Œè·³è¿‡é‡‡æ ·é˜¶æ®µã€‚")
                    should_sample = False
                else:
                    print("âš ï¸  è‡ªåŠ¨ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†é‡æ–°é‡‡æ ·ã€‚")
            else:
                print("â„¹ï¸  æœªå‘ç°è‡ªåŠ¨æŸ¥æ‰¾çš„ç¼“å­˜æ–‡ä»¶ï¼Œå°†è¿›è¡Œé‡‡æ ·ã€‚")

        # æ ¹æ® should_sample æ ‡å¿—å†³å®šæ˜¯å¦æ‰§è¡Œé˜¶æ®µ1
        if should_sample:
            print("ğŸ” å¼€å§‹é˜¶æ®µ1: Beam Searché‡‡æ ·")
            problems_list = self.run_on_problem_subset(max_problems, offset=problem_offset)
            print(f"å…±å¤„ç† {len(problems_list)} ä¸ªé—®é¢˜")
            self.phase1_beam_search_sampling(problems_list, num_beams)
            
            # ä¿å­˜é‡‡æ ·ç»“æœ
            if use_cached_sampling:
                self.save_sampling_results(max_problems, num_beams)
        
        # é˜¶æ®µ2: ä¼˜å…ˆç»éªŒå›æ”¾è®­ç»ƒ
        self.phase2_pper_training(n_iterations, batch_size, num_train_epochs=num_train_epochs, **kwargs)
        
        return self.get_experiment_results()


def main():
    parser = argparse.ArgumentParser(
        description='MBPPæ•°æ®é›†çš„BTPå®éªŒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

=== å·¥ä½œæµAï¼šè‡ªåŠ¨åŒ–ç»éªŒæ± ç¼“å­˜ï¼ˆæ¨èï¼‰ ===

1. é¦–æ¬¡è¿è¡Œ - ç”Ÿæˆç¼“å­˜ï¼ˆä½¿ç”¨finetuneæ¨¡å¼ï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 --num-beams 5

2. åç»­è¿è¡Œ - ä½¿ç”¨ç¼“å­˜ï¼ˆä¿®æ”¹è¶…å‚æ•°åå¿«é€Ÿå®éªŒï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 --num-beams 5

=== å·¥ä½œæµBï¼šå›ºå®šè®­ç»ƒæ ·æœ¬ï¼ˆç¡®ä¿å®éªŒä¸€è‡´æ€§ï¼‰ ===

3. ç”Ÿæˆå›ºå®šæ ·æœ¬ï¼ˆlocalæ¨¡å¼ï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode local --max-problems 100 \\
     --fixed-sample-path ./fixed_samples.json

4. ä½¿ç”¨å›ºå®šæ ·æœ¬è¿›è¡Œå¾®è°ƒ:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 \\
     --fixed-sample-path ./fixed_samples.json

=== å…¶ä»–æ¨¡å¼ ===

5. OpenAIå®éªŒ:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model gpt-4 --mode openai \\
     --api-key YOUR_KEY --max-problems 30

6. DeepSeek APIå®éªŒ:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-chat --mode deepseek \\
     --api-key YOUR_KEY --max-problems 30

=== é«˜çº§é€‰é¡¹ ===

7. å¼ºåˆ¶é‡æ–°é‡‡æ ·ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 \\
     --force-resample

8. ç¦ç”¨ç¼“å­˜åŠŸèƒ½:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 \\
     --use-cached-sampling false

=== LoRAå¾®è°ƒå‚æ•° ===

9. å¯ç”¨LoRAå¾®è°ƒ:
   python experiments/mbpp/step2_btp_experiment.py \\
     --model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100 \\
     --use-lora true

10. è®¾ç½®LoRA rç»´åº¦:
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --lora-r 16

11. è®¾ç½®LoRA alpha:
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --lora-alpha 32

12. è®¾ç½®LoRA dropout:
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --lora-dropout 0.1

=== è®­ç»ƒå‚æ•° ===

13. è®¾ç½®å¾®è°ƒå­¦ä¹ ç‡:
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --learning-rate 1e-6

14. è®¾ç½®Warmup Steps:
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --warmup-steps 10

15. è®¾ç½®Per-Device Batch Size:
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --per-device-batch-size 2

16. è®¾ç½®Gradient Accumulation Steps:
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --grad-accum-steps 4

=== æ•°æ®ç­›é€‰å‚æ•° ===

17. è®¾ç½®æœ€å°é€šè¿‡ç‡ (ç­›é€‰ç»éªŒ):
    python experiments/mbpp/step2_btp_experiment.py \\
      --model deepseek-ai/deepseek-coder-1.3b-instruct \\
      --mode finetune --max-problems 100 \\
      --min-pass-rate 0.8
        """)
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹åç§°æˆ–è·¯å¾„')
    parser.add_argument('--mode', type=str, default='local',
                       choices=['local', 'finetune', 'openai', 'deepseek'],
                       help='å®éªŒæ¨¡å¼')
    parser.add_argument('--target-model', type=str, default=None,
                       help='ç›®æ ‡å¾®è°ƒæ¨¡å‹ï¼ˆä»…å¾®è°ƒæ¨¡å¼éœ€è¦ï¼‰')
    
    # APIå‚æ•°
    parser.add_argument('--api-key', type=str,
                       help='APIå¯†é’¥')
    
    # åœ¨ä¸‹æ–¹æ·»åŠ è¿™å—ä»£ç 
    parser.add_argument('--teacher-model', type=str, default=None,
                       help='ç”¨äºåŠ è½½é‡‡æ ·ç¼“å­˜çš„Teacheræ¨¡å‹åç§° (ä»…åœ¨å¾®è°ƒæ¨¡å¼ä¸‹ä½¿ç”¨)')
    
    # å®éªŒå‚æ•°
    parser.add_argument('--max-problems', type=int, default=50,
                       help='æœ€å¤§é—®é¢˜æ•°é‡')
    parser.add_argument('--num-beams', type=int, default=5,
                       help='Beam Searchæ•°é‡')
    parser.add_argument('--n-iterations', type=int, default=2,
                       help='PPERè®­ç»ƒè¿­ä»£æ¬¡æ•°')
    
    parser.add_argument('--num-train-epochs', type=int, default=1, help='å¾®è°ƒæ—¶ä½¿ç”¨çš„è®­ç»ƒè½®æ¬¡æ•°')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='è®­ç»ƒæ‰¹å¤§å°')
    parser.add_argument('--output-dir', type=str, default='./mbpp_btp_checkpoints',
                       help='æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆä»…å¾®è°ƒæ¨¡å¼ï¼‰')
    
    # BTPç®—æ³•å‚æ•°
    parser.add_argument('--sampling-method', type=str, default='power',
                       choices=['power', 'rank'],
                       help='é‡‡æ ·æ–¹æ³•')
    parser.add_argument('--sampling-alpha', type=float, default=1.0,
                       help='é‡‡æ ·Î±å‚æ•°')
    parser.add_argument('--p2value-alpha', type=float, default=0.5,
                       help='P2Valueæƒé‡Î±')
    
    # é‡‡æ ·ç¼“å­˜å‚æ•°
    parser.add_argument('--use-cached-sampling', action='store_true', default=True,
                       help='ä½¿ç”¨ç¼“å­˜çš„é‡‡æ ·ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰')
    parser.add_argument('--force-resample', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°é‡‡æ ·ï¼Œå¿½ç•¥ç¼“å­˜')
    
    # å›ºå®šæ ·æœ¬å‚æ•°
    parser.add_argument('--fixed-sample-path', type=str, default=None,
                       help='æŒ‡å®šä¸€ä¸ªJSONæ–‡ä»¶ï¼Œä»ä¸­åŠ è½½å›ºå®šæ ·æœ¬')
    
    parser.add_argument('--sample-cache-path', type=str, default=None,
                       help='ç›´æ¥æŒ‡å®šè¦åŠ è½½çš„é‡‡æ ·ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œå°†è¦†ç›–è‡ªåŠ¨æŸ¥æ‰¾é€»è¾‘')
    
    # LoRAå¾®è°ƒå‚æ•°
    parser.add_argument('--use-lora', action='store_true', default=False,
                       help='å¯ç”¨LoRAå¾®è°ƒ')
    parser.add_argument('--lora-r', type=int, default=16, help='LoRA r dimension')
    parser.add_argument('--lora-alpha', type=int, default=32, help='LoRA alpha')
    parser.add_argument('--lora-dropout', type=float, default=0.1, help='LoRA dropout')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--learning-rate', type=float, default=1e-6, help='å¾®è°ƒæ—¶ä½¿ç”¨çš„å­¦ä¹ ç‡')
    parser.add_argument('--warmup-steps', type=int, default=10, help='Number of warmup steps for learning rate scheduler')
    parser.add_argument('--per-device-batch-size', type=int, default=2, help='Batch size per GPU during training')
    parser.add_argument('--grad-accum-steps', type=int, default=4, help='Gradient accumulation steps')

    # æ•°æ®ç­›é€‰å‚æ•°
    parser.add_argument('--min-pass-rate', type=float, default=1.0, help='Minimum pass_rate for samples to be included in finetuning')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ—¥å¿—')
    parser.add_argument('--gpu-id', type=int, default=None,
                       help='æŒ‡å®šGPUè®¾å¤‡ID')
    parser.add_argument('--save-interval', type=int, default=50,
                       help='æ¯å¤„ç†å¤šå°‘ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡è¿›åº¦')
    
    parser.add_argument('--problem-offset', type=int, default=0,
                       help='ä»ç¬¬å‡ ä¸ªé—®é¢˜å¼€å§‹é€‰æ‹© max-problems')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    # è®¾ç½®GPUè®¾å¤‡
    if args.gpu_id is not None and torch.cuda.is_available():
        if args.gpu_id < torch.cuda.device_count():
            torch.cuda.set_device(args.gpu_id)
            print(f"ğŸ¯ ä½¿ç”¨GPU {args.gpu_id}: {torch.cuda.get_device_name(args.gpu_id)}")
        else:
            print(f"âš ï¸  GPU {args.gpu_id} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤GPU")
    
    # æ‰“å°é…ç½®
    print("ğŸš€ MBPP BTPå®éªŒé…ç½®:")
    print(f"  æ¨¡å‹: {args.model}")
    print(f"  æ¨¡å¼: {args.mode}")
    print(f"  æœ€å¤§é—®é¢˜æ•°: {args.max_problems}")
    print(f"  é‡‡æ ·æ–¹æ³•: {args.sampling_method}")
    print(f"  é‡‡æ ·Alpha: {args.sampling_alpha}")
    print(f"  P2Value Alpha: {args.p2value_alpha}")
    if args.mode == "finetune":
        print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    if args.fixed_sample_path:
        print(f"  å›ºå®šæ ·æœ¬è·¯å¾„: {args.fixed_sample_path}")
    if torch.cuda.is_available():
        print(f"  å½“å‰GPU: {torch.cuda.current_device()} - {torch.cuda.get_device_name()}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(torch.cuda.current_device()).total_memory / 1024**3:.1f} GB")
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = MBBPBTPExperiment(
        model_name=args.model,
        model_type=args.mode,  # ä¿®å¤å‚æ•°åï¼šmode -> model_type
        api_key=args.api_key,
        api_base=None,  # API base å‚æ•°åœ¨ ModelAdapter ä¸­å¤„ç†
        sampling_method=args.sampling_method,
        sampling_alpha=args.sampling_alpha,
        p2value_alpha=args.p2value_alpha,
        output_dir=args.output_dir, # ä¼ é€’output_dirå‚æ•°
        fixed_sample_path=args.fixed_sample_path # ä¼ é€’fixed_sample_pathå‚æ•°
    )
    
    # åœ¨ä¸‹æ–¹æ·»åŠ è¿™å—ä»£ç 
    # å¦‚æœå‘½ä»¤è¡Œä¼ å…¥äº† teacher-model, å°†å…¶è®¾ç½®åˆ° experiment å¯¹è±¡ä¸Š
    if args.teacher_model:
        experiment.teacher_model_for_cache = args.teacher_model
    
    # è®¾ç½®ä¿å­˜é—´éš”
    experiment.save_interval = args.save_interval
    
    # è¿è¡Œå®éªŒ
    try:
        results = experiment.run_experiment(
            max_problems=args.max_problems,
            problem_offset=args.problem_offset, # æ·»åŠ è¿™ä¸€è¡Œ
            num_beams=args.num_beams,
            n_iterations=args.n_iterations,
            batch_size=args.batch_size,
            use_cached_sampling=args.use_cached_sampling, # ç¡®ä¿ä¼ é€’
            force_resample=args.force_resample,         # ç¡®ä¿ä¼ é€’
            sample_cache_path=args.sample_cache_path,    # ### æ·»åŠ è¿™ä¸€è¡Œ ###
            # ä¼ é€’æ‰€æœ‰æ–°çš„å¾®è°ƒå‚æ•°
            learning_rate=args.learning_rate,
            lora_r=args.lora_r,
            lora_alpha=args.lora_alpha,
            lora_dropout=args.lora_dropout,
            warmup_steps=args.warmup_steps,
            per_device_batch_size=args.per_device_batch_size,
            grad_accum_steps=args.grad_accum_steps,
            min_pass_rate=args.min_pass_rate,
            num_train_epochs=args.num_train_epochs
        )
        
        # ä¿å­˜ç»“æœ
        result_file = experiment.save_results(results, "btp_experiment")
        
        print("\nâœ… å®éªŒå®Œæˆ!")
        print(f"ğŸ“Š å®éªŒç»Ÿè®¡:")
        for key, value in results['experience_stats'].items():
            print(f"  {key}: {value}")
        
        print(f"ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        return 0
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main()) 