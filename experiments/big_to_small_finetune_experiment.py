#!/usr/bin/env python3
"""
å¤§æ¨¡å‹é‡‡æ · -> å°æ¨¡å‹å¾®è°ƒçš„BTPå®éªŒ
å®éªŒæ€è·¯ï¼šç”¨DeepSeek-V3-0324é‡‡æ ·ï¼Œå¾®è°ƒdeepseek-coder-1.3b-instruct
"""

import os
import sys
import json
import argparse
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from step2_btp_finetune_experiment import BTPFineTuneExperiment
from transformers import TrainingArguments

def create_experiment_config():
    """åˆ›å»ºå®éªŒé…ç½®"""
    return {
        # æ¨¡å‹é…ç½® - å…³é”®ï¼šå¤§æ¨¡å‹é‡‡æ ·ï¼Œå°æ¨¡å‹å¾®è°ƒ
        "source_model": "deepseek-ai/DeepSeek-V3-0324",  # å¤§æ¨¡å‹ç”¨äºé‡‡æ ·
        "target_model": "deepseek-ai/deepseek-coder-1.3b-instruct",  # å°æ¨¡å‹ç”¨äºå¾®è°ƒ
        
        # BTPå‚æ•°
        "num_beams": 8,  # å¢åŠ beamæ•°é‡è·å¾—æ›´å¤šæ ·çš„å€™é€‰
        "n_iterations": 3,
        "batch_size": 64,
        "max_problems": 100,
        
        # ä¼˜å…ˆé‡‡æ ·å‚æ•°
        "sampling_method": "power",
        "sampling_alpha": 1.5,  # é€‚ä¸­çš„é›†ä¸­åº¦
        "p2value_alpha": 0.7,   # æ›´é‡è§†é€šè¿‡ç‡
        
        # LoRAé…ç½® - é’ˆå¯¹å°æ¨¡å‹ä¼˜åŒ–
        "lora_config": {
            "r": 32,
            "alpha": 64,
            "dropout": 0.05,
            "target_modules": [
                "q_proj", "v_proj", "k_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"  # åŒ…å«MLPå±‚
            ]
        }
    }

def create_training_args(output_dir: str):
    """åˆ›å»ºè®­ç»ƒå‚æ•°"""
    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=2,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,  # æœ‰æ•ˆbatch_size = 2 * 8 = 16
        learning_rate=1e-4,
        warmup_steps=50,
        logging_steps=10,
        save_strategy="epoch",
        evaluation_strategy="no",
        fp16=True,
        remove_unused_columns=False,
        dataloader_num_workers=0,
        save_total_limit=2,
        load_best_model_at_end=False,
        report_to=None  # ä¸ä¸Šä¼ åˆ°wandbç­‰
    )

def run_big_to_small_experiment():
    """è¿è¡Œå¤§æ¨¡å‹é‡‡æ ·->å°æ¨¡å‹å¾®è°ƒå®éªŒ"""
    
    # åˆ›å»ºé…ç½®
    config = create_experiment_config()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"experiments/results_archive/big_to_small_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(f"{output_dir}/experiment_config.json", "w") as f:
        json.dump(config, f, indent=2)
    
    print("=" * 60)
    print("ğŸš€ å¤§æ¨¡å‹é‡‡æ · -> å°æ¨¡å‹å¾®è°ƒ BTPå®éªŒ")
    print("=" * 60)
    print(f"ğŸ“Š é‡‡æ ·æ¨¡å‹: {config['source_model']}")
    print(f"ğŸ¯ ç›®æ ‡æ¨¡å‹: {config['target_model']}")
    print(f"ğŸ”„ å¾®è°ƒè¿­ä»£: {config['n_iterations']}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print("=" * 60)
    
    try:
        # åˆ›å»ºBTPå®éªŒ
        experiment = BTPFineTuneExperiment(
            source_model_path=config['source_model'],
            target_model_path=config['target_model'],
            dataset="mbpp",
            sampling_method=config['sampling_method'],
            sampling_alpha=config['sampling_alpha'],
            p2value_alpha=config['p2value_alpha'],
            use_lora=True,
            lora_config=config['lora_config']
        )
        
        # åˆ›å»ºè®­ç»ƒå‚æ•°
        training_args = create_training_args(f"{output_dir}/checkpoints")
        
        # è¿è¡Œå®éªŒ
        results = experiment.run_experiment(
            max_problems=config['max_problems'],
            num_beams=config['num_beams'],
            n_iterations=config['n_iterations'],
            batch_size=config['batch_size'],
            training_args=training_args,
            output_dir=output_dir
        )
        
        # ä¿å­˜ç»“æœ
        with open(f"{output_dir}/experiment_results.json", "w") as f:
            json.dump(results, f, indent=2)
        
        print("\n" + "=" * 60)
        print("âœ… å®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {output_dir}")
        print("=" * 60)
        
        return results
        
    except Exception as e:
        print(f"\nâŒ å®éªŒå¤±è´¥: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(description="å¤§æ¨¡å‹é‡‡æ ·->å°æ¨¡å‹å¾®è°ƒBTPå®éªŒ")
    parser.add_argument("--max_problems", type=int, default=100, help="æœ€å¤§é—®é¢˜æ•°é‡")
    parser.add_argument("--num_beams", type=int, default=8, help="Beam Searchæ•°é‡")
    parser.add_argument("--n_iterations", type=int, default=3, help="å¾®è°ƒè¿­ä»£æ¬¡æ•°")
    parser.add_argument("--batch_size", type=int, default=64, help="æ‰¹æ¬¡å¤§å°")
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    config = create_experiment_config()
    config.update({
        "max_problems": args.max_problems,
        "num_beams": args.num_beams,
        "n_iterations": args.n_iterations,
        "batch_size": args.batch_size
    })
    
    run_big_to_small_experiment()

if __name__ == "__main__":
    main() 