# æ™ºèƒ½Promptæ¨¡æ¿ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

åŸºäºã€Šä»£ç ç”Ÿæˆæ¨¡å‹Promptæ¨¡æ¿æƒå¨æŒ‡å—ã€‹ï¼Œæˆ‘ä»¬ä¸ºEG-CFGé¡¹ç›®å¼€å‘äº†æ™ºèƒ½Prompté€‚é…ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿèƒ½å¤Ÿï¼š

- **è‡ªåŠ¨æ£€æµ‹æ¨¡å‹å®¶æ—**å¹¶é€‰æ‹©æ­£ç¡®çš„promptæ ¼å¼
- **ä¼˜åŒ–ç”Ÿæˆå‚æ•°**ä»¥è·å¾—æœ€ä½³æ€§èƒ½
- **ç»Ÿä¸€ç®¡ç†**æ‰€æœ‰æ¨¡å‹é…ç½®å’Œæ¨¡æ¿
- **ä¿æŒå‘åå…¼å®¹**å¹¶æ–¹ä¾¿æ‰©å±•

## æ ¸å¿ƒåŸåˆ™

### 1. æ¨¡å‹å†³å®šæ¨¡æ¿
ä¸åŒæ¨¡å‹å®¶æ—æœ‰ä¸åŒçš„æ ¼å¼è¦æ±‚ï¼š

| æ¨¡å‹å®¶æ— | æ ¼å¼ç¤ºä¾‹ | ç‰¹ç‚¹ |
|---------|---------|------|
| **LLaMA/CodeLLaMA** | `[INST]`æ ¼å¼ | 70Bæœ‰ç‰¹æ®ŠSourceæ ¼å¼ |
| **DeepSeek-Coder** | `### Instruction/Response` | å›ºå®šæ ¼å¼ï¼Œå¿…é¡»éµå®ˆ |
| **StarCoder** | ç›´æ¥ç»­å†™ vs Alpaca | baseç‰ˆåœ¨HumanEvalä¸Šæœ‰ç‰¹æ®ŠæŠ€å·§ |
| **Qwen** | `<\|im_start\|>` ChatML | å»ºè®®ç”¨å®˜æ–¹tokenizer |
| **OpenAI/Claude** | `messages`åˆ—è¡¨ | å¿…é¡»ç”¨systemæ¶ˆæ¯çº¦æŸè¾“å‡º |

### 2. æ•°æ®é›†å†³å®šå†…å®¹
- **MBPP**: é—®é¢˜æè¿° + æµ‹è¯•ç”¨ä¾‹æ ¼å¼
- **HumanEval**: å‡½æ•°ç­¾å + æ–‡æ¡£å­—ç¬¦ä¸²
- **APPS**: å¤æ‚é—®é¢˜æè¿° + å¤šæµ‹è¯•ç”¨ä¾‹

### 3. åŸºç¡€ç‰ˆvsæŒ‡ä»¤ç‰ˆ
è‡ªåŠ¨è¯†åˆ«å¹¶ä½¿ç”¨ç›¸åº”çš„æ¨¡æ¿æ ¼å¼ã€‚

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
from experiments.prompt_templates import get_model_prompt, detect_model_info
from experiments.shared.model_configs import get_model_config, get_optimal_generation_params

# 1. æ£€æµ‹æ¨¡å‹ä¿¡æ¯
model_name = "deepseek-ai/deepseek-coder-1.3b-instruct"
model_info = detect_model_info(model_name)
print(f"å®¶æ—: {model_info.family.value}, ç±»å‹: {model_info.type.value}")

# 2. è·å–ä¼˜åŒ–é…ç½®
config = get_model_config(model_name)
params = get_optimal_generation_params(model_name, "mbpp")

# 3. ç”Ÿæˆé€‚é…çš„prompt
problem = {
    'text': 'Write a function to find the maximum element in a list.',
    'test_list': ['assert max_element([1, 2, 3]) == 3']
}

prompt = get_model_prompt(
    model_name=model_name,
    dataset="mbpp",
    problem=problem
)
print(prompt)
```

### é«˜çº§ä½¿ç”¨ï¼ˆFew-shot Examplesï¼‰

```python
# ä¸ºDeepSeekç­‰æ¨¡å‹ä½¿ç”¨few-shot examples
examples = [
    {
        "problem": "Write a function to find similar elements from two tuple lists.",
        "test_cases": ["assert similar_elements((3, 4, 5, 6),(5, 7, 4, 10)) == (4, 5)"],
        "solution": "def similar_elements(test_tup1, test_tup2):\n  res = tuple(set(test_tup1) & set(test_tup2))\n  return res"
    }
]

prompt = get_model_prompt(
    model_name="deepseek-ai/deepseek-coder-6.7b-instruct",
    dataset="mbpp",
    problem=problem,
    use_examples=True,
    examples=examples
)
```

## ä¸åŒæ¨¡å‹å®¶æ—çš„å…·ä½“æ ¼å¼

### 1. DeepSeek-Coderç³»åˆ—

**æ¨¡å‹æ£€æµ‹**: `deepseek`, `deepseek-coder`, `deepseek-v*`

**Instructæ ¼å¼**:
```
You are an AI programming assistant, utilizing the Deepseek Coder model...
### Instruction:
{user_prompt}
### Response:
```

**ä½¿ç”¨å»ºè®®**:
- MBPPæ•°æ®é›†å»ºè®®ä½¿ç”¨few-shot examples
- æ¸©åº¦è®¾ç½®0.6-0.7è·å¾—æœ€ä½³æ•ˆæœ
- æ”¯æŒé«˜è¾¾8Kä¸Šä¸‹æ–‡é•¿åº¦

### 2. LLaMA/CodeLLaMAç³»åˆ—

**æ¨¡å‹æ£€æµ‹**: `llama`, `codellama`, `meta-llama`

**Instructæ ¼å¼**:
```
<s>[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]
```

**70Bç‰¹æ®Šæ ¼å¼**:
```
<s>Source: system

{system_prompt}<step>Source: user

{user_prompt}<step>Source: assistant
Destination: user

```

**ä½¿ç”¨å»ºè®®**:
- å»ºè®®æ·»åŠ ç³»ç»Ÿæç¤ºçº¦æŸè¾“å‡º
- Baseç‰ˆæœ¬åœ¨HumanEvalä¸Šç›´æ¥ä½¿ç”¨promptå­—æ®µ
- 13BåŠä»¥ä¸Šæ¨¡å‹è¡¨ç°æ›´ç¨³å®š

### 3. StarCoderç³»åˆ—

**æ¨¡å‹æ£€æµ‹**: `starcoder`, `wizardcoder`, `bigcode`

**Baseæ ¼å¼ï¼ˆHumanEvalä¼˜åŒ–ï¼‰**:
```
<filename>solutions/solution_1.py
# Here is the correct implementation of the code exercise
{content}
```

**WizardCoderï¼ˆAlpacaæ ¼å¼ï¼‰**:
```
Below is an instruction that describes a task...

### Instruction:
{user_prompt}

### Response:
```

**ä½¿ç”¨å»ºè®®**:
- Baseç‰ˆæœ¬åœ¨HumanEvalä¸Šä½¿ç”¨ç‰¹æ®Šå‰ç¼€
- WizardCoderä½¿ç”¨Alpacaæ ¼å¼è€ŒéåŸå§‹StarCoderæ ¼å¼
- æ¸©åº¦è®¾ç½®0.5-0.7

### 4. Qwenç³»åˆ—

**æ¨¡å‹æ£€æµ‹**: `qwen`, `qwen-coder`, `tongyi`

**ChatMLæ ¼å¼**:
```
<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{user_prompt}
<|im_end|>
<|im_start|>assistant
```

**ä½¿ç”¨å»ºè®®**:
- å¼ºçƒˆå»ºè®®ä½¿ç”¨å®˜æ–¹tokenizerçš„apply_chat_template
- æ”¯æŒé•¿ä¸Šä¸‹æ–‡ï¼ˆ8K+ï¼‰
- ä¸­è‹±æ–‡æ··åˆæ•°æ®é›†è¡¨ç°ä¼˜ç§€

### 5. OpenAI/Claudeç³»åˆ—

**æ ¼å¼ï¼ˆMessagesï¼‰**:
```python
[
    {"role": "system", "content": "You are an expert Python programmer..."},
    {"role": "user", "content": "{user_prompt}"}
]
```

**ä½¿ç”¨å»ºè®®**:
- **å¿…é¡»**ä½¿ç”¨systemæ¶ˆæ¯çº¦æŸè¾“å‡ºæ ¼å¼
- é¿å…æ¨¡å‹ç”Ÿæˆè§£é‡Šæ€§æ–‡å­—å½±å“è¯„æµ‹
- GPT-4å»ºè®®é™ä½æ¸©åº¦åˆ°0.5-0.6

## å®éªŒæ–‡ä»¶é›†æˆ

### æ›´æ–°ç°æœ‰å®éªŒ

```python
# åŸæœ‰æ–¹å¼
def format_prompt(self, problem):
    return f"Solve: {problem['text']}"

# æ–°çš„æ™ºèƒ½æ–¹å¼
from experiments.prompt_templates import get_model_prompt
from experiments.shared.model_configs import get_model_config

def format_prompt(self, problem):
    # è‡ªåŠ¨é€‚é…æ¨¡å‹æ ¼å¼
    return get_model_prompt(
        model_name=self.model_name,
        dataset="mbpp",
        problem=problem,
        use_examples=self.model_config.use_examples
    )
```

### å‚æ•°ä¼˜åŒ–

```python
from experiments.shared.model_configs import get_optimal_generation_params

# è·å–æ•°æ®é›†ç‰¹å®šçš„æœ€ä¼˜å‚æ•°
params = get_optimal_generation_params(model_name, "mbpp")
# è¾“å‡º: {'temperature': 0.7, 'max_tokens': 512, 'use_examples': True, ...}

# åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨
outputs = model.generate(prompt, **params)
```

## æ€§èƒ½å»ºè®®

### æ¨¡å‹æ¨è

**MBPPæ•°æ®é›†**ï¼ˆå‡†ç¡®ç‡ä¼˜å…ˆï¼‰:
1. `deepseek-ai/deepseek-coder-6.7b-instruct` - é«˜å‡†ç¡®ç‡
2. `gpt-4` - æœ€é«˜å‡†ç¡®ç‡ï¼ˆæˆæœ¬é«˜ï¼‰
3. `meta-llama/CodeLlama-7b-Instruct-hf` - å¹³è¡¡æ€§èƒ½

**HumanEvalæ•°æ®é›†**ï¼ˆå‡†ç¡®ç‡ä¼˜å…ˆï¼‰:
1. `gpt-4` - ä¸šç•Œæ ‡æ†
2. `claude-3-sonnet` - å¼ºå¤§æ¨ç†èƒ½åŠ›
3. `bigcode/starcoder-15b` - HumanEvalä¼˜åŒ–

**APPSæ•°æ®é›†**ï¼ˆå¤æ‚é—®é¢˜ï¼‰:
1. `gpt-4` - å¤æ‚é—®é¢˜å¤„ç†èƒ½åŠ›å¼º
2. `claude-3-sonnet` - ä¼˜ç§€æ¨ç†èƒ½åŠ›
3. `meta-llama/CodeLlama-13b-Instruct-hf` - å¤§æ¨¡å‹ä¼˜åŠ¿

### å‚æ•°è°ƒä¼˜å»ºè®®

| æ•°æ®é›† | æ¸©åº¦èŒƒå›´ | Max Tokens | Few-shot | ç‰¹æ®Šè€ƒè™‘ |
|--------|----------|------------|----------|----------|
| **MBPP** | 0.6-0.8 | 256-512 | æ¨èï¼ˆDeepSeekï¼‰ | ç®€å•é—®é¢˜ï¼Œé¿å…è¿‡åº¦ç”Ÿæˆ |
| **HumanEval** | 0.5-0.7 | 256-512 | ä¸æ¨è | ç²¾ç¡®è¡¥å…¨ï¼Œä½æ¸©åº¦ |
| **APPS** | 0.7-0.9 | 1024-2048 | å¯é€‰ | å¤æ‚é—®é¢˜ï¼Œéœ€è¦æ›´å¤štoken |

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **Unknownæ¨¡å‹å®¶æ—**
   ```python
   # æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
   model_info = detect_model_info("your-model-name")
   if model_info.family == ModelFamily.UNKNOWN:
       print("éœ€è¦æ·»åŠ æ¨¡å‹æ£€æµ‹è§„åˆ™")
   ```

2. **APIå¯†é’¥é—®é¢˜**
   ```python
   validation = validate_model_setup("gpt-4")
   if validation["warnings"]:
       print("é…ç½®é—®é¢˜:", validation["warnings"])
   ```

3. **æ€§èƒ½ä¸ä½³**
   ```python
   # æ£€æŸ¥å…¼å®¹æ€§å»ºè®®
   compatibility = validate_model_compatibility("model-name", "mbpp")
   print("ä¼˜åŒ–å»ºè®®:", compatibility["recommendations"])
   ```

### è°ƒè¯•æŠ€å·§

```python
# 1. æ£€æŸ¥ç”Ÿæˆçš„prompt
prompt = get_model_prompt(model_name, dataset, problem)
print("Generated prompt:", prompt[:200])

# 2. éªŒè¯å‚æ•°è®¾ç½®
params = get_optimal_generation_params(model_name, dataset)
print("Optimal params:", params)

# 3. æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
model_info = detect_model_info(model_name)
print(f"Family: {model_info.family}, Type: {model_info.type}")
```

## æ‰©å±•æŒ‡å—

### æ·»åŠ æ–°æ¨¡å‹å®¶æ—

1. åœ¨`experiments/prompt_templates.py`ä¸­æ·»åŠ æ£€æµ‹è§„åˆ™ï¼š
   ```python
   FAMILY_PATTERNS = {
       ModelFamily.NEW_FAMILY: [r"pattern1", r"pattern2"]
   }
   ```

2. åœ¨`experiments/shared/model_configs.py`ä¸­æ·»åŠ é…ç½®ï¼š
   ```python
   self.register_config(ModelConfig(
       name="new-model",
       family=ModelFamily.NEW_FAMILY,
       # ... å…¶ä»–é…ç½®
   ))
   ```

### æ·»åŠ æ–°æ•°æ®é›†

1. åœ¨`_format_dataset_content`æ–¹æ³•ä¸­æ·»åŠ æ ¼å¼åŒ–é€»è¾‘
2. åœ¨æ¨¡å‹é…ç½®ä¸­æ·»åŠ æ•°æ®é›†ç‰¹å®šå‚æ•°
3. æ›´æ–°æ¨èç®—æ³•

## ç‰ˆæœ¬æ›´æ–°

### v1.0 ç‰¹æ€§
- âœ… è‡ªåŠ¨æ¨¡å‹æ£€æµ‹
- âœ… æ™ºèƒ½prompté€‚é…
- âœ… å‚æ•°ä¼˜åŒ–
- âœ… å‘åå…¼å®¹

### è®¡åˆ’ç‰¹æ€§
- ğŸ”„ æ›´å¤šæ¨¡å‹å®¶æ—æ”¯æŒ
- ğŸ”„ åŠ¨æ€å‚æ•°è°ƒä¼˜
- ğŸ”„ A/Bæµ‹è¯•æ¡†æ¶
- ğŸ”„ æ€§èƒ½ç›‘æ§

---

**æ³¨æ„**: è¯¥ç³»ç»Ÿè®¾è®¡ä¸ºæ¸è¿›å¼æ”¹è¿›ã€‚ç°æœ‰ä»£ç å¯ä»¥é€æ­¥è¿ç§»ï¼Œæ— éœ€ä¸€æ¬¡æ€§é‡å†™æ‰€æœ‰å®éªŒæ–‡ä»¶ã€‚ 