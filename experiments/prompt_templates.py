"""
æ™ºèƒ½Promptæ¨¡æ¿é€‚é…ç³»ç»Ÿ
åŸºäºä»£ç ç”Ÿæˆæ¨¡å‹Promptæ¨¡æ¿æƒå¨æŒ‡å—

æ ¸å¿ƒåŸåˆ™ï¼š
1. æ¨¡å‹å†³å®šæ¨¡æ¿ï¼šæ ¹æ®æ¨¡å‹å®¶æ—è‡ªåŠ¨é€‰æ‹©æ­£ç¡®çš„æ ¼å¼å’Œç‰¹æ®Šæ ‡è®°
2. æ•°æ®é›†å†³å®šå†…å®¹ï¼šæ ¹æ®æ•°æ®é›†ç±»å‹æ ¼å¼åŒ–é—®é¢˜å†…å®¹
3. åŸºç¡€ç‰ˆvsæŒ‡ä»¤ç‰ˆï¼šè‡ªåŠ¨åŒºåˆ†å¹¶ä½¿ç”¨ç›¸åº”çš„æ¨¡æ¿

æ”¯æŒçš„æ¨¡å‹å®¶æ—ï¼š
- LLaMA/CodeLLaMAç³»åˆ— ([INST]æ ¼å¼)
- DeepSeek-Coderç³»åˆ— (### Instructionæ ¼å¼)
- StarCoderç³»åˆ— (ç›´æ¥è¡¥å…¨æˆ–Alpacaæ ¼å¼)
- Qwenç³»åˆ— (ChatMLæ ¼å¼)
- OpenAI/Claudeç³»åˆ— (messagesæ ¼å¼)
"""

import re
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum


class ModelFamily(Enum):
    """æ¨¡å‹å®¶æ—æšä¸¾"""
    LLAMA = "llama"
    DEEPSEEK = "deepseek"
    STARCODER = "starcoder"
    QWEN = "qwen"
    OPENAI = "openai"
    CLAUDE = "claude"
    UNKNOWN = "unknown"


class ModelType(Enum):
    """æ¨¡å‹ç±»å‹æšä¸¾"""
    BASE = "base"           # åŸºç¡€/è¡¥å…¨æ¨¡å‹
    INSTRUCT = "instruct"   # æŒ‡ä»¤å¾®è°ƒæ¨¡å‹
    CHAT = "chat"           # å¯¹è¯æ¨¡å‹


@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    family: ModelFamily
    type: ModelType
    name: str
    size: Optional[str] = None
    special_format: Optional[str] = None


class ModelDetector:
    """æ™ºèƒ½æ¨¡å‹æ£€æµ‹å™¨"""
    
    # æ¨¡å‹å®¶æ—æ£€æµ‹è§„åˆ™
    FAMILY_PATTERNS = {
        ModelFamily.LLAMA: [
            r"llama", r"code[-_]?llama", r"codellama",
            r"llama[-_]?2", r"llama2", r"meta[-_]?llama"
        ],
        ModelFamily.DEEPSEEK: [
            r"deepseek", r"deepseek[-_]?coder", r"deepseek[-_]?v[0-9]"
        ],
        ModelFamily.STARCODER: [
            r"starcoder", r"star[-_]?coder", r"wizardcoder",
            r"wizard[-_]?coder", r"bigcode"
        ],
        ModelFamily.QWEN: [
            r"qwen", r"qwen[-_]?coder", r"qwen[-_]?code",
            r"tongyi", r"é€šä¹‰"
        ],
        ModelFamily.OPENAI: [
            r"gpt[-_]?[0-9]", r"gpt[-_]?3\.?5", r"gpt[-_]?4",
            r"text[-_]?davinci", r"code[-_]?davinci"
        ],
        ModelFamily.CLAUDE: [
            r"claude", r"claude[-_]?[0-9]", r"anthropic"
        ]
    }
    
    # æ¨¡å‹ç±»å‹æ£€æµ‹è§„åˆ™
    TYPE_PATTERNS = {
        ModelType.INSTRUCT: [r"instruct", r"instruction", r"chat"],
        ModelType.CHAT: [r"chat", r"conversation"],
        ModelType.BASE: [r"base", r"completion", r"^(?!.*instruct)(?!.*chat).*$"]
    }
    
    # ç‰¹æ®Šæ ¼å¼æ£€æµ‹ï¼ˆå¦‚CodeLLaMA-70Bçš„ç‰¹æ®Šæ ¼å¼ï¼‰
    SPECIAL_FORMAT_PATTERNS = {
        "llama_70b_instruct": [r"llama.*70b.*instruct", r"codellama.*70b.*instruct"],
        "starcoder_base_humaneval": [r"starcoder.*base", r"star[-_]?coder(?!.*wizard)"]
    }
    
    @classmethod
    def detect_model(cls, model_name: str) -> ModelInfo:
        """æ£€æµ‹æ¨¡å‹ä¿¡æ¯"""
        model_name_lower = model_name.lower()
        
        # æ£€æµ‹å®¶æ—
        family = cls._detect_family(model_name_lower)
        
        # æ£€æµ‹ç±»å‹
        model_type = cls._detect_type(model_name_lower)
        
        # æ£€æµ‹ç‰¹æ®Šæ ¼å¼
        special_format = cls._detect_special_format(model_name_lower)
        
        # æ£€æµ‹å°ºå¯¸
        size = cls._extract_size(model_name_lower)
        
        return ModelInfo(
            family=family,
            type=model_type,
            name=model_name,
            size=size,
            special_format=special_format
        )
    
    @classmethod
    def _detect_family(cls, model_name: str) -> ModelFamily:
        """æ£€æµ‹æ¨¡å‹å®¶æ—"""
        for family, patterns in cls.FAMILY_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, model_name):
                    return family
        return ModelFamily.UNKNOWN
    
    @classmethod
    def _detect_type(cls, model_name: str) -> ModelType:
        """æ£€æµ‹æ¨¡å‹ç±»å‹"""
        for model_type, patterns in cls.TYPE_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, model_name):
                    return model_type
        return ModelType.BASE  # é»˜è®¤ä¸ºbaseç±»å‹
    
    @classmethod
    def _detect_special_format(cls, model_name: str) -> Optional[str]:
        """æ£€æµ‹ç‰¹æ®Šæ ¼å¼"""
        for format_name, patterns in cls.SPECIAL_FORMAT_PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, model_name):
                    return format_name
        return None
    
    @classmethod
    def _extract_size(cls, model_name: str) -> Optional[str]:
        """æå–æ¨¡å‹å°ºå¯¸"""
        size_patterns = [
            r"(\d+\.?\d*[bm])",  # å¦‚ 7b, 13b, 70b, 1.3b
            r"(\d+\.?\d*)([-_]?)(billion|million|b|m)"
        ]
        
        for pattern in size_patterns:
            match = re.search(pattern, model_name)
            if match:
                return match.group(1)
        return None


class PromptTemplateEngine:
    """æ™ºèƒ½Promptæ¨¡æ¿å¼•æ“"""
    
    def __init__(self):
        self.detector = ModelDetector()
        self._init_templates()
    
    def _init_templates(self):
        """åˆå§‹åŒ–æ‰€æœ‰æ¨¡æ¿"""
        
        # LLaMA/CodeLLaMAæ¨¡æ¿
        self.llama_templates = {
            ModelType.INSTRUCT: {
                "system_template": """<s>[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]""",
                "user_only_template": """<s>[INST] {user_prompt} [/INST]""",
                "default_system": "You are an expert Python programmer. Please only output the code."
            },
            ModelType.BASE: {
                "template": "{content}",  # ç›´æ¥ç»­å†™
                "default_system": ""
            }
        }
        
        # CodeLLaMA-70B-Instructç‰¹æ®Šæ ¼å¼
        self.llama_70b_template = """<s>Source: system

{system_prompt}<step>Source: user

{user_prompt}<step>Source: assistant
Destination: user

"""
        
        # DeepSeek-Coderæ¨¡æ¿
        self.deepseek_templates = {
            ModelType.INSTRUCT: {
                "template": """You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer
### Instruction:
{user_prompt}
### Response:""",
                "with_examples_template": """You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer
### Instruction:
Please refer the given examples and generate a python function for my problem.
Examples are listed as follows:
{examples}

Here is my problem:
{user_prompt}

### Response:"""
            },
            ModelType.BASE: {
                "template": "{content}",  # ç›´æ¥ç»­å†™
            }
        }
        
        # StarCoderæ¨¡æ¿
        self.starcoder_templates = {
            ModelType.BASE: {
                "template": "{content}",  # ç›´æ¥ç»­å†™
                "humaneval_prefix": """<filename>solutions/solution_1.py
# Here is the correct implementation of the code exercise
{content}"""
            },
            ModelType.INSTRUCT: {  # WizardCoderç­‰å¾®è°ƒå˜ä½“
                "template": """Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{user_prompt}

### Response:"""
            }
        }
        
        # Qwenæ¨¡æ¿ (ChatMLæ ¼å¼)
        self.qwen_templates = {
            ModelType.INSTRUCT: {
                "template": """<|im_start|>system
{system_prompt}
<|im_end|>
<|im_start|>user
{user_prompt}
<|im_end|>
<|im_start|>assistant""",
                "user_only_template": """<|im_start|>user
{user_prompt}
<|im_end|>
<|im_start|>assistant""",
                "default_system": "You are a helpful programming assistant."
            }
        }
        
        # OpenAI/Claudeæ¨¡æ¿ (Messagesæ ¼å¼)
        self.openai_claude_templates = {
            ModelType.CHAT: {
                "messages_format": True,
                "default_system": "You are an expert Python programmer. Please provide only the complete, runnable code solution without any extra explanations."
            }
        }
    
    def format_prompt(self, model_name: str, dataset: str, problem: Dict[str, Any], 
                     system_prompt: Optional[str] = None, 
                     use_examples: bool = False,
                     examples: Optional[List[Dict]] = None) -> Union[str, List[Dict]]:
        """
        æ™ºèƒ½æ ¼å¼åŒ–prompt
        
        Args:
            model_name: æ¨¡å‹åç§°
            dataset: æ•°æ®é›†åç§° ('mbpp', 'humaneval', 'apps')
            problem: é—®é¢˜å­—å…¸
            system_prompt: è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            use_examples: æ˜¯å¦ä½¿ç”¨few-shot examples
            examples: ç¤ºä¾‹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: æ ¼å¼åŒ–çš„promptå­—ç¬¦ä¸²ï¼ˆå¤§å¤šæ•°æ¨¡å‹ï¼‰
            List[Dict]: messagesæ ¼å¼ï¼ˆOpenAI/Claudeï¼‰
        """
        
        # æ£€æµ‹æ¨¡å‹ä¿¡æ¯
        model_info = self.detector.detect_model(model_name)
        
        # è·å–æ•°æ®é›†å†…å®¹
        user_content = self._format_dataset_content(dataset, problem)
        
        # æ ¹æ®æ¨¡å‹å®¶æ—å’Œç±»å‹é€‰æ‹©æ¨¡æ¿
        if model_info.family == ModelFamily.LLAMA:
            return self._format_llama_prompt(model_info, user_content, system_prompt, dataset)
        
        elif model_info.family == ModelFamily.DEEPSEEK:
            return self._format_deepseek_prompt(model_info, user_content, use_examples, examples)
        
        elif model_info.family == ModelFamily.STARCODER:
            return self._format_starcoder_prompt(model_info, user_content, dataset)
        
        elif model_info.family == ModelFamily.QWEN:
            return self._format_qwen_prompt(model_info, user_content, system_prompt)
        
        elif model_info.family in [ModelFamily.OPENAI, ModelFamily.CLAUDE]:
            return self._format_openai_claude_prompt(model_info, user_content, system_prompt)
        
        else:
            # æœªçŸ¥æ¨¡å‹ï¼Œä½¿ç”¨é€šç”¨æ ¼å¼
            return self._format_generic_prompt(user_content)
    
    def _format_dataset_content(self, dataset: str, problem: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ•°æ®é›†å†…å®¹"""
        if dataset.lower() == "mbpp":
            test_cases = "\n".join([f"  {test}" for test in problem.get('test_list', [])])
            return f"""Problem: {problem['text']}

Test cases:
{test_cases}

Provide a complete Python function:"""
        
        elif dataset.lower() == "humaneval":
            return f"""Complete the following Python function:

{problem['prompt']}"""
        
        elif dataset.lower() == "apps":
            test_cases = "\n".join([f"  {test}" for test in problem.get('test_list', [])])
            return f"""Problem: {problem['question']}

Test cases:
{test_cases}

Provide a complete Python function:"""
        
        else:
            return f"Solve this programming problem: {problem}"
    
    def _format_llama_prompt(self, model_info: ModelInfo, user_content: str, 
                           system_prompt: Optional[str], dataset: str) -> str:
        """æ ¼å¼åŒ–LLaMA prompt"""
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯70Bçš„ç‰¹æ®Šæ ¼å¼
        if model_info.special_format == "llama_70b_instruct":
            system = system_prompt or self.llama_templates[ModelType.INSTRUCT]["default_system"]
            return self.llama_70b_template.format(
                system_prompt=system,
                user_prompt=user_content
            )
        
        if model_info.type == ModelType.INSTRUCT:
            template_info = self.llama_templates[ModelType.INSTRUCT]
            
            if system_prompt:
                return template_info["system_template"].format(
                    system_prompt=system_prompt,
                    user_prompt=user_content
                )
            else:
                # ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤º
                return template_info["system_template"].format(
                    system_prompt=template_info["default_system"],
                    user_prompt=user_content
                )
        else:
            # Baseæ¨¡å‹ï¼Œç›´æ¥ç»­å†™
            if dataset.lower() == "humaneval":
                return problem['prompt']  # HumanEvalç›´æ¥ä½¿ç”¨promptå­—æ®µ
            else:
                # å¯¹å…¶ä»–æ•°æ®é›†ï¼Œç®€å•æ ¼å¼åŒ–
                return f"# {user_content}\ndef "
    
    def _format_deepseek_prompt(self, model_info: ModelInfo, user_content: str,
                              use_examples: bool, examples: Optional[List[Dict]]) -> str:
        """æ ¼å¼åŒ–DeepSeek prompt"""
        
        if model_info.type == ModelType.INSTRUCT:
            template_info = self.deepseek_templates[ModelType.INSTRUCT]
            
            if use_examples and examples:
                # æ ¼å¼åŒ–examples
                formatted_examples = self._format_examples_for_deepseek(examples)
                return template_info["with_examples_template"].format(
                    examples=formatted_examples,
                    user_prompt=user_content
                )
            else:
                return template_info["template"].format(user_prompt=user_content)
        else:
            # Baseæ¨¡å‹
            return user_content
    
    def _format_starcoder_prompt(self, model_info: ModelInfo, user_content: str, dataset: str) -> str:
        """æ ¼å¼åŒ–StarCoder prompt"""
        
        if model_info.type == ModelType.INSTRUCT:
            # WizardCoderç­‰å¾®è°ƒå˜ä½“ä½¿ç”¨Alpacaæ ¼å¼
            template_info = self.starcoder_templates[ModelType.INSTRUCT]
            return template_info["template"].format(user_prompt=user_content)
        else:
            # Baseæ¨¡å‹
            template_info = self.starcoder_templates[ModelType.BASE]
            
            if dataset.lower() == "humaneval":
                # HumanEvalä½¿ç”¨ç‰¹æ®Šå‰ç¼€æŠ€å·§
                return template_info["humaneval_prefix"].format(content=user_content)
            else:
                # å…¶ä»–æ•°æ®é›†ç›´æ¥ç»­å†™
                return template_info["template"].format(content=user_content)
    
    def _format_qwen_prompt(self, model_info: ModelInfo, user_content: str, 
                          system_prompt: Optional[str]) -> str:
        """æ ¼å¼åŒ–Qwen prompt"""
        
        if model_info.type == ModelType.INSTRUCT:
            template_info = self.qwen_templates[ModelType.INSTRUCT]
            
            if system_prompt:
                return template_info["template"].format(
                    system_prompt=system_prompt,
                    user_prompt=user_content
                )
            else:
                return template_info["user_only_template"].format(user_prompt=user_content)
        else:
            return user_content
    
    def _format_openai_claude_prompt(self, model_info: ModelInfo, user_content: str, 
                                   system_prompt: Optional[str]) -> List[Dict]:
        """æ ¼å¼åŒ–OpenAI/Claude prompt (è¿”å›messagesæ ¼å¼)"""
        
        template_info = self.openai_claude_templates[ModelType.CHAT]
        system = system_prompt or template_info["default_system"]
        
        return [
            {"role": "system", "content": system},
            {"role": "user", "content": user_content}
        ]
    
    def _format_generic_prompt(self, user_content: str) -> str:
        """é€šç”¨æ ¼å¼ï¼ˆæœªçŸ¥æ¨¡å‹ï¼‰"""
        return f"Solve the following programming problem:\n\n{user_content}"
    
    def _format_examples_for_deepseek(self, examples: List[Dict]) -> str:
        """ä¸ºDeepSeekæ ¼å¼åŒ–few-shot examples"""
        formatted = []
        for i, example in enumerate(examples, 1):
            formatted.append(f"""- Example {i}:
>>> Problem:
{example['problem']}
>>> Test Cases:
{chr(10).join(example['test_cases'])}

>>> Code:
```python
{example['solution']}
```""")
        return "\n\n".join(formatted)


# å…¨å±€å®ä¾‹
prompt_engine = PromptTemplateEngine()


def get_model_prompt(model_name: str, dataset: str, problem: Dict[str, Any], 
                    system_prompt: Optional[str] = None, 
                    use_examples: bool = False,
                    examples: Optional[List[Dict]] = None) -> Union[str, List[Dict]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šè·å–ç‰¹å®šæ¨¡å‹çš„prompt
    
    è¿™æ˜¯ä¸»è¦çš„å¯¹å¤–æ¥å£ï¼Œè‡ªåŠ¨å¤„ç†æ‰€æœ‰æ¨¡å‹é€‚é…é€»è¾‘
    """
    return prompt_engine.format_prompt(
        model_name=model_name,
        dataset=dataset,
        problem=problem,
        system_prompt=system_prompt,
        use_examples=use_examples,
        examples=examples
    )


def detect_model_info(model_name: str) -> ModelInfo:
    """ä¾¿æ·å‡½æ•°ï¼šæ£€æµ‹æ¨¡å‹ä¿¡æ¯"""
    return ModelDetector.detect_model(model_name)


def validate_model_compatibility(model_name: str, dataset: str) -> Dict[str, Any]:
    """éªŒè¯æ¨¡å‹ä¸æ•°æ®é›†çš„å…¼å®¹æ€§"""
    model_info = detect_model_info(model_name)
    
    compatibility = {
        "model_info": model_info,
        "supported": True,
        "recommendations": [],
        "warnings": []
    }
    
    # é’ˆå¯¹ä¸åŒæ¨¡å‹å®¶æ—çš„å»ºè®®
    if model_info.family == ModelFamily.STARCODER and dataset.lower() != "humaneval":
        compatibility["recommendations"].append(
            "StarCoderåœ¨HumanEvalä¸Šè¡¨ç°æœ€ä½³ï¼Œå»ºè®®ä½¿ç”¨ç‰¹æ®Šå‰ç¼€æ ¼å¼"
        )
    
    if model_info.family == ModelFamily.DEEPSEEK and dataset.lower() == "mbpp":
        compatibility["recommendations"].append(
            "DeepSeek-Coderå»ºè®®ä½¿ç”¨few-shot examplesä»¥è·å¾—æ›´å¥½æ€§èƒ½"
        )
    
    if model_info.family == ModelFamily.UNKNOWN:
        compatibility["warnings"].append(
            "æœªè¯†åˆ«çš„æ¨¡å‹å®¶æ—ï¼Œå°†ä½¿ç”¨é€šç”¨æ ¼å¼ï¼Œå¯èƒ½å½±å“æ€§èƒ½"
        )
    
    return compatibility


# å‘åå…¼å®¹æ€§æ”¯æŒ
class PromptTemplates:
    """ä¿æŒå‘åå…¼å®¹çš„æ—§æ¥å£"""
    
    @classmethod
    def get_prompt(cls, dataset: str, problem: dict, model_name: str = "generic") -> str:
        """å‘åå…¼å®¹çš„æ¥å£"""
        result = get_model_prompt(model_name, dataset, problem)
        
        # å¦‚æœè¿”å›çš„æ˜¯messagesæ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(result, list):
            # æå–userå†…å®¹
            for msg in result:
                if msg["role"] == "user":
                    return msg["content"]
            return str(result)
        
        return result
    
    @classmethod
    def validate_consistency(cls) -> bool:
        """å‘åå…¼å®¹çš„éªŒè¯æ–¹æ³•"""
        print("âœ… æ–°çš„æ™ºèƒ½Promptç³»ç»Ÿå·²å¯ç”¨")
        print("âœ… æ”¯æŒè‡ªåŠ¨æ¨¡å‹æ£€æµ‹å’Œæ ¼å¼é€‚é…")
        print("âœ… å‘åå…¼å®¹æ€§å·²ä¿è¯")
        return True


# ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹æ£€æµ‹
    test_models = [
        "deepseek-ai/deepseek-coder-1.3b-instruct",
        "meta-llama/CodeLlama-7b-Instruct-hf",
        "bigcode/starcoder-15b",
        "WizardLM/WizardCoder-15B-V1.0",
        "Qwen/CodeQwen1.5-7B-Chat",
        "gpt-3.5-turbo",
        "claude-3-sonnet"
    ]
    
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹æ£€æµ‹å’Œpromptæ ¼å¼åŒ–ï¼š\n")
    
    sample_problem = {
        'text': 'Write a function to find the maximum element in a list.',
        'test_list': ['assert max_element([1, 2, 3]) == 3', 'assert max_element([10, 5, 8]) == 10']
    }
    
    for model in test_models:
        print(f"ğŸ“‹ æ¨¡å‹: {model}")
        model_info = detect_model_info(model)
        print(f"   å®¶æ—: {model_info.family.value}")
        print(f"   ç±»å‹: {model_info.type.value}")
        if model_info.size:
            print(f"   å°ºå¯¸: {model_info.size}")
        if model_info.special_format:
            print(f"   ç‰¹æ®Šæ ¼å¼: {model_info.special_format}")
        
        # ç”Ÿæˆpromptç¤ºä¾‹
        prompt = get_model_prompt(model, "mbpp", sample_problem)
        if isinstance(prompt, list):
            print(f"   æ ¼å¼: Messages (OpenAI/Claude)")
            print(f"   ç³»ç»Ÿ: {prompt[0]['content'][:50]}...")
            print(f"   ç”¨æˆ·: {prompt[1]['content'][:50]}...")
        else:
            print(f"   Prompt: {prompt[:100]}...")
        print()
    
    print("âœ… æ™ºèƒ½Promptæ¨¡æ¿ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼") 