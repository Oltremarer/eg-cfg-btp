"""
æ™ºèƒ½æ¨¡å‹é…ç½®ç®¡ç†ç³»ç»Ÿ
é…åˆpromptæ¨¡æ¿ç³»ç»Ÿï¼Œä¸ºä¸åŒæ¨¡å‹æä¾›ä¼˜åŒ–çš„å‚æ•°è®¾ç½®
"""

from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
from experiments.prompt_templates import ModelFamily, ModelType, detect_model_info


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ä¿¡æ¯"""
    name: str
    family: ModelFamily
    type: ModelType
    
    # ç”Ÿæˆå‚æ•°
    default_temperature: float = 0.8
    default_max_tokens: int = 512
    default_top_p: float = 0.95
    default_num_beams: int = 5
    
    # æ•°æ®é›†ç‰¹å®šè®¾ç½®
    dataset_preferences: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    
    # æ€§èƒ½ä¼˜åŒ–å»ºè®®
    batch_size_recommendation: int = 1
    use_examples: bool = False
    preferred_examples_count: int = 3
    
    # API/æ¨ç†é…ç½®
    api_endpoint: Optional[str] = None
    requires_api_key: bool = False
    supports_streaming: bool = False
    timeout_seconds: int = 30
    
    # ç‰¹æ®Šå¤„ç†æ ‡å¿—
    needs_special_encoding: bool = False
    supports_system_prompt: bool = True
    max_context_length: int = 4096
    
    # è¯„ä¼°å»ºè®®
    recommended_metrics: List[str] = field(default_factory=lambda: ["pass@1", "pass@5"])


class ModelConfigManager:
    """æ¨¡å‹é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self):
        self.configs = {}
        self._init_predefined_configs()
    
    def _init_predefined_configs(self):
        """åˆå§‹åŒ–é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®"""
        
        # DeepSeekç³»åˆ—é…ç½®
        self.register_config(ModelConfig(
            name="deepseek-coder",
            family=ModelFamily.DEEPSEEK,
            type=ModelType.INSTRUCT,
            default_temperature=0.7,
            default_max_tokens=512,
            dataset_preferences={
                "mbpp": {
                    "use_examples": True,
                    "examples_count": 3,
                    "temperature": 0.6
                },
                "humaneval": {
                    "use_examples": False,
                    "temperature": 0.8
                }
            },
            use_examples=True,
            preferred_examples_count=3,
            api_endpoint="https://api.deepseek.com/v1/chat/completions",
            requires_api_key=True,
            supports_streaming=True,
            max_context_length=8192,
            recommended_metrics=["pass@1", "pass@5", "pass@10"]
        ))
        
        # LLaMA/CodeLLaMAç³»åˆ—é…ç½®
        self.register_config(ModelConfig(
            name="codellama-instruct",
            family=ModelFamily.LLAMA,
            type=ModelType.INSTRUCT,
            default_temperature=0.8,
            default_max_tokens=1024,
            dataset_preferences={
                "mbpp": {"temperature": 0.7},
                "humaneval": {"temperature": 0.8, "max_tokens": 512},
                "apps": {"temperature": 0.9, "max_tokens": 2048}
            },
            batch_size_recommendation=2,
            max_context_length=4096,
            supports_system_prompt=True
        ))
        
        self.register_config(ModelConfig(
            name="codellama-base", 
            family=ModelFamily.LLAMA,
            type=ModelType.BASE,
            default_temperature=0.7,
            default_max_tokens=512,
            dataset_preferences={
                "humaneval": {"temperature": 0.6, "max_tokens": 256}
            },
            supports_system_prompt=False,
            batch_size_recommendation=4
        ))
        
        # StarCoderç³»åˆ—é…ç½®
        self.register_config(ModelConfig(
            name="starcoder-base",
            family=ModelFamily.STARCODER,
            type=ModelType.BASE,
            default_temperature=0.6,
            default_max_tokens=512,
            dataset_preferences={
                "humaneval": {
                    "temperature": 0.5,
                    "use_special_prefix": True,
                    "max_tokens": 256
                },
                "mbpp": {"temperature": 0.7}
            },
            supports_system_prompt=False,
            batch_size_recommendation=4
        ))
        
        self.register_config(ModelConfig(
            name="wizardcoder",
            family=ModelFamily.STARCODER,
            type=ModelType.INSTRUCT,
            default_temperature=0.7,
            default_max_tokens=512,
            dataset_preferences={
                "mbpp": {"temperature": 0.6},
                "apps": {"temperature": 0.8, "max_tokens": 1024}
            },
            supports_system_prompt=True,
            batch_size_recommendation=2
        ))
        
        # Qwenç³»åˆ—é…ç½®
        self.register_config(ModelConfig(
            name="qwen-coder",
            family=ModelFamily.QWEN,
            type=ModelType.INSTRUCT,
            default_temperature=0.8,
            default_max_tokens=1024,
            dataset_preferences={
                "mbpp": {"temperature": 0.7},
                "humaneval": {"temperature": 0.8},
                "apps": {"temperature": 0.9, "max_tokens": 2048}
            },
            api_endpoint="https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
            requires_api_key=True,
            max_context_length=8192
        ))
        
        # OpenAIç³»åˆ—é…ç½®
        self.register_config(ModelConfig(
            name="gpt-3.5-turbo",
            family=ModelFamily.OPENAI,
            type=ModelType.CHAT,
            default_temperature=0.7,
            default_max_tokens=512,
            dataset_preferences={
                "mbpp": {"temperature": 0.6, "max_tokens": 256},
                "humaneval": {"temperature": 0.7, "max_tokens": 512},
                "apps": {"temperature": 0.8, "max_tokens": 1024}
            },
            api_endpoint="https://api.openai.com/v1/chat/completions",
            requires_api_key=True,
            supports_streaming=True,
            max_context_length=4096,
            recommended_metrics=["pass@1", "pass@3", "pass@5"]
        ))
        
        self.register_config(ModelConfig(
            name="gpt-4",
            family=ModelFamily.OPENAI,
            type=ModelType.CHAT,
            default_temperature=0.6,
            default_max_tokens=1024,
            dataset_preferences={
                "mbpp": {"temperature": 0.5, "max_tokens": 512},
                "humaneval": {"temperature": 0.6, "max_tokens": 512},
                "apps": {"temperature": 0.7, "max_tokens": 2048}
            },
            api_endpoint="https://api.openai.com/v1/chat/completions",
            requires_api_key=True,
            supports_streaming=True,
            max_context_length=8192,
            batch_size_recommendation=1,  # GPT-4æˆæœ¬è¾ƒé«˜ï¼Œå»ºè®®å°æ‰¹é‡
            recommended_metrics=["pass@1", "pass@3"]
        ))
        
        # Claudeç³»åˆ—é…ç½®
        self.register_config(ModelConfig(
            name="claude-3-sonnet",
            family=ModelFamily.CLAUDE,
            type=ModelType.CHAT,
            default_temperature=0.7,
            default_max_tokens=1024,
            dataset_preferences={
                "mbpp": {"temperature": 0.6},
                "humaneval": {"temperature": 0.7},
                "apps": {"temperature": 0.8, "max_tokens": 2048}
            },
            api_endpoint="https://api.anthropic.com/v1/messages",
            requires_api_key=True,
            max_context_length=8192,
            timeout_seconds=60  # Claudeå¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´
        ))
    
    def register_config(self, config: ModelConfig):
        """æ³¨å†Œæ¨¡å‹é…ç½®"""
        key = f"{config.family.value}_{config.type.value}"
        if key not in self.configs:
            self.configs[key] = []
        self.configs[key].append(config)
    
    def get_config_for_model(self, model_name: str) -> ModelConfig:
        """ä¸ºç‰¹å®šæ¨¡å‹è·å–é…ç½®"""
        
        # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
        for config_list in self.configs.values():
            for config in config_list:
                if config.name.lower() in model_name.lower():
                    return config
        
        # ç„¶ååŸºäºæ¨¡å‹æ£€æµ‹æ¥åŒ¹é…
        model_info = detect_model_info(model_name)
        key = f"{model_info.family.value}_{model_info.type.value}"
        
        if key in self.configs and self.configs[key]:
            # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„é…ç½®
            base_config = self.configs[key][0]
            
            # åˆ›å»ºä¸€ä¸ªé’ˆå¯¹è¯¥æ¨¡å‹çš„å®šåˆ¶é…ç½®
            custom_config = ModelConfig(
                name=model_name,
                family=model_info.family,
                type=model_info.type,
                default_temperature=base_config.default_temperature,
                default_max_tokens=base_config.default_max_tokens,
                default_top_p=base_config.default_top_p,
                default_num_beams=base_config.default_num_beams,
                dataset_preferences=base_config.dataset_preferences.copy(),
                batch_size_recommendation=base_config.batch_size_recommendation,
                use_examples=base_config.use_examples,
                preferred_examples_count=base_config.preferred_examples_count,
                api_endpoint=base_config.api_endpoint,
                requires_api_key=base_config.requires_api_key,
                supports_streaming=base_config.supports_streaming,
                timeout_seconds=base_config.timeout_seconds,
                needs_special_encoding=base_config.needs_special_encoding,
                supports_system_prompt=base_config.supports_system_prompt,
                max_context_length=base_config.max_context_length,
                recommended_metrics=base_config.recommended_metrics.copy()
            )
            return custom_config
        
        # è¿”å›é»˜è®¤é…ç½®
        return self._create_default_config(model_name, model_info)
    
    def _create_default_config(self, model_name: str, model_info) -> ModelConfig:
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        return ModelConfig(
            name=model_name,
            family=model_info.family,
            type=model_info.type,
            default_temperature=0.8,
            default_max_tokens=512,
            dataset_preferences={},
            batch_size_recommendation=1,
            use_examples=False,
            preferred_examples_count=3,
            requires_api_key=False,
            supports_streaming=False,
            timeout_seconds=30,
            supports_system_prompt=True,
            max_context_length=2048,
            recommended_metrics=["pass@1", "pass@5"]
        )
    
    def get_optimal_params(self, model_name: str, dataset: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šçš„æœ€ä¼˜å‚æ•°"""
        config = self.get_config_for_model(model_name)
        
        # åŸºç¡€å‚æ•°
        params = {
            "temperature": config.default_temperature,
            "max_tokens": config.default_max_tokens,
            "top_p": config.default_top_p,
            "num_beams": config.default_num_beams,
            "batch_size": config.batch_size_recommendation,
            "timeout": config.timeout_seconds
        }
        
        # æ•°æ®é›†ç‰¹å®šä¼˜åŒ–
        if dataset in config.dataset_preferences:
            dataset_params = config.dataset_preferences[dataset]
            params.update(dataset_params)
        
        # Few-shot examplesè®¾ç½®
        if config.use_examples:
            params["use_examples"] = True
            params["examples_count"] = config.preferred_examples_count
        
        return params
    
    def get_model_recommendations(self, dataset: str, 
                                performance_priority: str = "accuracy") -> List[str]:
        """ä¸ºç‰¹å®šæ•°æ®é›†æ¨èæ¨¡å‹"""
        recommendations = []
        
        if dataset.lower() == "mbpp":
            if performance_priority == "accuracy":
                recommendations = [
                    "deepseek-ai/deepseek-coder-6.7b-instruct",  # é«˜å‡†ç¡®ç‡
                    "gpt-4",  # æœ€é«˜å‡†ç¡®ç‡ï¼ˆä½†æˆæœ¬é«˜ï¼‰
                    "meta-llama/CodeLlama-7b-Instruct-hf"  # å¹³è¡¡æ€§èƒ½
                ]
            elif performance_priority == "speed":
                recommendations = [
                    "deepseek-ai/deepseek-coder-1.3b-instruct",  # å¿«é€Ÿæ¨ç†
                    "bigcode/starcoder-1b",  # è½»é‡çº§
                    "meta-llama/CodeLlama-7b-hf"  # baseæ¨¡å‹æ›´å¿«
                ]
            elif performance_priority == "cost":
                recommendations = [
                    "deepseek-ai/deepseek-coder-1.3b-instruct",  # å¼€æºå…è´¹
                    "gpt-3.5-turbo",  # APIæˆæœ¬è¾ƒä½
                    "bigcode/starcoder-7b"  # å¼€æº
                ]
        
        elif dataset.lower() == "humaneval":
            if performance_priority == "accuracy":
                recommendations = [
                    "gpt-4",  # ä¸šç•Œæ ‡æ†
                    "claude-3-sonnet",  # å¼ºå¤§æ¨ç†èƒ½åŠ›
                    "bigcode/starcoder-15b"  # HumanEvalä¼˜åŒ–
                ]
            elif performance_priority == "speed":
                recommendations = [
                    "bigcode/starcoder-7b",  # HumanEvalç‰¹æ®Šä¼˜åŒ–
                    "meta-llama/CodeLlama-7b-hf",  # å¿«é€Ÿbaseæ¨¡å‹
                    "deepseek-ai/deepseek-coder-1.3b-instruct"
                ]
        
        elif dataset.lower() == "apps":
            if performance_priority == "accuracy":
                recommendations = [
                    "gpt-4",  # å¤æ‚é—®é¢˜å¤„ç†èƒ½åŠ›å¼º
                    "claude-3-sonnet",  # ä¼˜ç§€çš„æ¨ç†èƒ½åŠ›
                    "meta-llama/CodeLlama-13b-Instruct-hf"  # å¤§æ¨¡å‹å¤„ç†å¤æ‚é—®é¢˜
                ]
            elif performance_priority == "speed":
                recommendations = [
                    "deepseek-ai/deepseek-coder-6.7b-instruct",
                    "meta-llama/CodeLlama-7b-Instruct-hf",
                    "WizardLM/WizardCoder-15B-V1.0"
                ]
        
        return recommendations
    
    def validate_setup(self, model_name: str) -> Dict[str, Any]:
        """éªŒè¯æ¨¡å‹è®¾ç½®"""
        config = self.get_config_for_model(model_name)
        
        validation = {
            "model_name": model_name,
            "config": config,
            "status": "valid",
            "warnings": [],
            "requirements": [],
            "suggestions": []
        }
        
        # æ£€æŸ¥API requirements
        if config.requires_api_key:
            validation["requirements"].append("éœ€è¦é…ç½®APIå¯†é’¥")
            validation["suggestions"].append(f"è®¾ç½®ç¯å¢ƒå˜é‡æˆ–é…ç½®API key for {config.family.value}")
        
        # æ£€æŸ¥ä¸Šä¸‹æ–‡é•¿åº¦
        if config.max_context_length < 2048:
            validation["warnings"].append("æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦è¾ƒçŸ­ï¼Œå¯èƒ½å½±å“å¤æ‚é—®é¢˜çš„å¤„ç†")
        
        # æ£€æŸ¥ç‰¹æ®Šå¤„ç†éœ€æ±‚
        if config.needs_special_encoding:
            validation["requirements"].append("éœ€è¦ç‰¹æ®Šçš„ç¼–ç å¤„ç†")
        
        return validation


# å…¨å±€é…ç½®ç®¡ç†å™¨å®ä¾‹
model_config_manager = ModelConfigManager()


def get_model_config(model_name: str) -> ModelConfig:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–æ¨¡å‹é…ç½®"""
    return model_config_manager.get_config_for_model(model_name)


def get_optimal_generation_params(model_name: str, dataset: str) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–æœ€ä¼˜ç”Ÿæˆå‚æ•°"""
    return model_config_manager.get_optimal_params(model_name, dataset)


def recommend_models(dataset: str, priority: str = "accuracy") -> List[str]:
    """ä¾¿æ·å‡½æ•°ï¼šæ¨èæ¨¡å‹"""
    return model_config_manager.get_model_recommendations(dataset, priority)


def validate_model_setup(model_name: str) -> Dict[str, Any]:
    """ä¾¿æ·å‡½æ•°ï¼šéªŒè¯æ¨¡å‹è®¾ç½®"""
    return model_config_manager.validate_setup(model_name)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    print("ğŸ”§ æ™ºèƒ½æ¨¡å‹é…ç½®ç³»ç»Ÿæ¼”ç¤ºï¼š\n")
    
    test_models = [
        "deepseek-ai/deepseek-coder-1.3b-instruct",
        "meta-llama/CodeLlama-7b-Instruct-hf", 
        "bigcode/starcoder-15b",
        "gpt-3.5-turbo"
    ]
    
    for model in test_models:
        print(f"ğŸ“‹ æ¨¡å‹: {model}")
        config = get_model_config(model)
        print(f"   å®¶æ—: {config.family.value}")
        print(f"   ç±»å‹: {config.type.value}")
        
        # MBPPä¼˜åŒ–å‚æ•°
        mbpp_params = get_optimal_generation_params(model, "mbpp")
        print(f"   MBPPæœ€ä¼˜å‚æ•°: {mbpp_params}")
        
        # éªŒè¯è®¾ç½®
        validation = validate_model_setup(model)
        if validation["warnings"]:
            print(f"   âš ï¸  è­¦å‘Š: {validation['warnings']}")
        
        print()
    
    # æ¨èæ¼”ç¤º
    print("ğŸ“Š æ•°æ®é›†æ¨¡å‹æ¨è:")
    for dataset in ["mbpp", "humaneval", "apps"]:
        recommendations = recommend_models(dataset, "accuracy")
        print(f"   {dataset.upper()}: {recommendations[:2]}")  # æ˜¾ç¤ºå‰2ä¸ªæ¨è
    
    print("\nâœ… æ¨¡å‹é…ç½®ç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼") 