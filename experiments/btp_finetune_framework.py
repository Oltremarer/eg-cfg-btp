#!/usr/bin/env python3
"""
BTP (Beam Search + Testing + Prioritized Experience Replay) å¾®è°ƒæ¡†æ¶
å®ç°è®ºæ–‡Algorithm 1ä¸­çš„å®Œæ•´BTPç®—æ³•ï¼Œæ”¯æŒä¸°å¯Œçš„å‘½ä»¤è¡Œå‚æ•°é…ç½®

æ”¯æŒçš„é‡‡æ ·æ–¹å¼:
1. Power Sampling: P(i) = pi^Î± / Î£ pk^Î±
2. Rank Sampling: pi = 1/rank(i)
"""

import os
import sys
import json
import argparse
import random
import math
from datetime import datetime
from typing import List, Dict, Any, Optional
from collections import deque


class P2ValueCalculator:
    """P2Value = Î± Ã— possibility + (1-Î±) Ã— pass_rate"""
    
    def __init__(self, alpha: float = 0.5):
        self.alpha = alpha
    
    def calculate(self, possibility: float, pass_rate: float) -> float:
        return self.alpha * possibility + (1 - self.alpha) * pass_rate


class PrioritizedSampler:
    """æ”¯æŒä¸¤ç§é‡‡æ ·æ–¹å¼çš„ä¼˜å…ˆé‡‡æ ·å™¨"""
    
    def __init__(self, method: str = "power", alpha: float = 1.0):
        self.method = method
        self.alpha = alpha
    
    def sample(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """æ ¹æ®P2Valueè¿›è¡Œä¼˜å…ˆé‡‡æ ·"""
        if not experiences or batch_size <= 0:
            return []
        
        if self.method == "power":
            return self._power_sampling(experiences, batch_size)
        elif self.method == "rank":
            return self._rank_sampling(experiences, batch_size)
        else:
            raise ValueError(f"Unsupported sampling method: {self.method}")
    
    def _power_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """å¹‚æ¬¡é‡‡æ ·: P(i) = pi^Î± / Î£ pk^Î±"""
        import numpy as np
        
        p2values = [exp['p2value'] for exp in experiences]
        p2values = np.maximum(p2values, 1e-8)  # é¿å…é™¤é›¶
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        powered = np.power(p2values, self.alpha)
        probabilities = powered / np.sum(powered)
        
        # é‡‡æ ·
        indices = np.random.choice(
            len(experiences), 
            size=min(batch_size, len(experiences)),
            p=probabilities, 
            replace=False
        )
        
        return [experiences[i] for i in indices]
    
    def _rank_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """æ’åé‡‡æ ·: pi = 1/rank(i)"""
        import numpy as np
        
        # æŒ‰P2Valueæ’åº
        sorted_exp = sorted(experiences, key=lambda x: x['p2value'], reverse=True)
        
        # è®¡ç®—æ’åæ¦‚ç‡
        ranks = np.arange(1, len(sorted_exp) + 1)
        probabilities = 1.0 / ranks
        probabilities = probabilities / np.sum(probabilities)
        
        # é‡‡æ ·
        indices = np.random.choice(
            len(sorted_exp),
            size=min(batch_size, len(sorted_exp)),
            p=probabilities,
            replace=False
        )
        
        return [sorted_exp[i] for i in indices]


class ExperienceBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, max_size: int = 10000):
        self.buffer = deque(maxlen=max_size)
        self.p2calc = P2ValueCalculator()
    
    def add(self, experience: Dict):
        """æ·»åŠ ç»éªŒ"""
        if 'p2value' not in experience:
            experience['p2value'] = self.p2calc.calculate(
                experience['possibility'], 
                experience['pass_rate']
            )
        self.buffer.append(experience)
    
    def get_all(self) -> List[Dict]:
        return list(self.buffer)
    
    def get_stats(self) -> Dict:
        if not self.buffer:
            return {'total': 0}
        
        experiences = list(self.buffer)
        p2values = [exp['p2value'] for exp in experiences]
        pass_rates = [exp['pass_rate'] for exp in experiences]
        
        return {
            'total': len(experiences),
            'avg_p2value': sum(p2values) / len(p2values),
            'avg_pass_rate': sum(pass_rates) / len(pass_rates),
            'perfect_solutions': sum(1 for exp in experiences if exp['pass_rate'] >= 1.0)
        }


class BTPFramework:
    """BTPå¾®è°ƒæ¡†æ¶ä¸»ç±»"""
    
    def __init__(self, 
                 source_model: str,
                 target_model: Optional[str] = None,
                 dataset: str = "mbpp",
                 sampling_method: str = "power",
                 sampling_alpha: float = 1.0,
                 p2value_alpha: float = 0.5,
                 use_lora: bool = True,
                 lora_config: Optional[Dict] = None):
        
        self.source_model = source_model
        self.target_model = target_model or source_model
        self.dataset = dataset
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.p2calc = P2ValueCalculator(p2value_alpha)
        self.sampler = PrioritizedSampler(sampling_method, sampling_alpha)
        self.buffer = ExperienceBuffer()
        
        # LoRAé…ç½®
        self.use_lora = use_lora
        self.lora_config = lora_config or {
            'r': 16,
            'alpha': 32,
            'dropout': 0.1,
            'target_modules': ['q_proj', 'v_proj', 'k_proj', 'o_proj']
        }
        
        print(f"BTPæ¡†æ¶åˆå§‹åŒ–å®Œæˆ:")
        print(f"  æºæ¨¡å‹: {self.source_model}")
        print(f"  ç›®æ ‡æ¨¡å‹: {self.target_model}")
        print(f"  é‡‡æ ·æ–¹æ³•: {sampling_method}")
        print(f"  é‡‡æ ·Î±: {sampling_alpha}")
        print(f"  P2Value Î±: {p2value_alpha}")
        print(f"  ä½¿ç”¨LoRA: {use_lora}")
    
    def phase1_beam_search_sampling(self, 
                                   problems: List[Dict], 
                                   num_beams: int = 5) -> None:
        """é˜¶æ®µ1: Beam Searché‡‡æ ·"""
        print(f"é˜¶æ®µ1: Beam Searché‡‡æ · (beams={num_beams})")
        
        for i, problem in enumerate(problems):
            print(f"å¤„ç†é—®é¢˜ {i+1}/{len(problems)}")
            
            # æ¨¡æ‹Ÿbeam searchç”Ÿæˆ
            candidates = self._simulate_beam_generation(problem, num_beams)
            
            # æµ‹è¯•æ¯ä¸ªå€™é€‰
            for j, candidate in enumerate(candidates):
                # æ¨¡æ‹Ÿæµ‹è¯•ç»“æœ
                pass_rate = self._simulate_testing(candidate['code'], problem)
                
                experience = {
                    'problem_id': problem.get('task_id', i),
                    'problem_text': problem.get('text', ''),
                    'code': candidate['code'],
                    'possibility': candidate['possibility'],
                    'pass_rate': pass_rate,
                    'beam_rank': j,
                    'source_model': self.source_model
                }
                
                self.buffer.add(experience)
        
        print(f"é˜¶æ®µ1å®Œæˆï¼Œæ”¶é›†äº† {len(self.buffer.get_all())} ä¸ªç»éªŒ")
    
    def phase2_pper_training(self, 
                           n_iterations: int = 3,
                           batch_size: int = 100,
                           training_config: Optional[Dict] = None) -> None:
        """é˜¶æ®µ2: PPERè®­ç»ƒ"""
        print(f"é˜¶æ®µ2: PPERè®­ç»ƒ ({n_iterations}è½®è¿­ä»£)")
        
        training_config = training_config or {
            'learning_rate': 1e-4,
            'num_epochs': 1,
            'batch_size': 2,
            'gradient_accumulation_steps': 8
        }
        
        for iteration in range(n_iterations):
            print(f"\nè¿­ä»£ {iteration + 1}/{n_iterations}")
            
            # ä¼˜å…ˆé‡‡æ ·
            all_experiences = self.buffer.get_all()
            sampled = self.sampler.sample(all_experiences, batch_size)
            
            print(f"ä» {len(all_experiences)} ä¸ªç»éªŒä¸­é‡‡æ ·äº† {len(sampled)} ä¸ª")
            
            # æ¨¡æ‹Ÿå¾®è°ƒè¿‡ç¨‹
            self._simulate_finetuning(sampled, training_config, iteration)
            
            # æ˜¾ç¤ºé‡‡æ ·ç»Ÿè®¡
            self._show_sampling_stats(sampled, all_experiences)
    
    def _simulate_beam_generation(self, problem: Dict, num_beams: int) -> List[Dict]:
        """æ¨¡æ‹Ÿbeam searchç”Ÿæˆ"""
        candidates = []
        
        for i in range(num_beams):
            # æ¨¡æ‹Ÿç”Ÿæˆçš„ä»£ç å’Œæ¦‚ç‡
            code = f"# Generated solution {i+1} for problem\ndef solution():\n    return 'mock'"
            possibility = 1.0 / (i + 1)  # ç®€å•çš„æ’åæ¦‚ç‡
            
            candidates.append({
                'code': code,
                'possibility': possibility
            })
        
        return candidates
    
    def _simulate_testing(self, code: str, problem: Dict) -> float:
        """æ¨¡æ‹Ÿä»£ç æµ‹è¯•"""
        # ç®€å•æ¨¡æ‹Ÿï¼šéšæœºç”Ÿæˆé€šè¿‡ç‡
        return random.uniform(0.0, 1.0)
    
    def _simulate_finetuning(self, 
                           experiences: List[Dict], 
                           config: Dict, 
                           iteration: int) -> None:
        """æ¨¡æ‹Ÿå¾®è°ƒè¿‡ç¨‹"""
        print(f"  æ¨¡æ‹Ÿå¾®è°ƒé…ç½®:")
        print(f"    å­¦ä¹ ç‡: {config['learning_rate']}")
        print(f"    è®­ç»ƒè½®æ•°: {config['num_epochs']}")
        print(f"    æ‰¹å¤§å°: {config['batch_size']}")
        print(f"    LoRA: {self.use_lora}")
        
        if self.use_lora:
            print(f"    LoRA r: {self.lora_config['r']}")
            print(f"    LoRA Î±: {self.lora_config['alpha']}")
        
        # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
        import time
        time.sleep(0.5)  # æ¨¡æ‹Ÿè®­ç»ƒå»¶è¿Ÿ
        
        print(f"  âœ“ è¿­ä»£ {iteration + 1} å¾®è°ƒå®Œæˆ")
    
    def _show_sampling_stats(self, sampled: List[Dict], all_exp: List[Dict]) -> None:
        """æ˜¾ç¤ºé‡‡æ ·ç»Ÿè®¡"""
        if not sampled:
            return
        
        sampled_p2values = [exp['p2value'] for exp in sampled]
        all_p2values = [exp['p2value'] for exp in all_exp]
        
        print(f"  é‡‡æ ·ç»Ÿè®¡:")
        print(f"    é‡‡æ ·å¹³å‡P2Value: {sum(sampled_p2values)/len(sampled_p2values):.4f}")
        print(f"    å…¨ä½“å¹³å‡P2Value: {sum(all_p2values)/len(all_p2values):.4f}")
        print(f"    é‡‡æ ·ä¸­å®Œç¾è§£å†³æ–¹æ¡ˆ: {sum(1 for exp in sampled if exp['pass_rate'] >= 1.0)}")
    
    def run_experiment(self, 
                      max_problems: int = 50,
                      num_beams: int = 5,
                      n_iterations: int = 2,
                      batch_size: int = 50,
                      output_dir: str = "./btp_results") -> Dict:
        """è¿è¡Œå®Œæ•´BTPå®éªŒ"""
        
        print("=" * 60)
        print("BTPå¾®è°ƒå®éªŒ")
        print("=" * 60)
        
        # æ¨¡æ‹ŸåŠ è½½æ•°æ®é›†
        problems = self._load_mock_dataset(max_problems)
        print(f"åŠ è½½äº† {len(problems)} ä¸ªé—®é¢˜")
        
        # é˜¶æ®µ1: Beam Search + Testing
        self.phase1_beam_search_sampling(problems, num_beams)
        
        initial_stats = self.buffer.get_stats()
        print(f"\nåˆå§‹ç»Ÿè®¡:")
        for k, v in initial_stats.items():
            print(f"  {k}: {v}")
        
        # é˜¶æ®µ2: PPERå¾®è°ƒ
        self.phase2_pper_training(n_iterations, batch_size)
        
        # ä¿å­˜ç»“æœ
        results = {
            'experiment_type': 'BTP_FineTune',
            'source_model': self.source_model,
            'target_model': self.target_model,
            'sampling_method': self.sampler.method,
            'sampling_alpha': self.sampler.alpha,
            'p2value_alpha': self.p2calc.alpha,
            'max_problems': max_problems,
            'num_beams': num_beams,
            'n_iterations': n_iterations,
            'batch_size': batch_size,
            'use_lora': self.use_lora,
            'lora_config': self.lora_config,
            'initial_stats': initial_stats,
            'final_stats': self.buffer.get_stats(),
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        os.makedirs(output_dir, exist_ok=True)
        result_file = os.path.join(output_dir, 
                                 f"btp_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nå®éªŒç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        return results
    
    def _load_mock_dataset(self, max_problems: int) -> List[Dict]:
        """åŠ è½½æ¨¡æ‹Ÿæ•°æ®é›†"""
        problems = []
        for i in range(max_problems):
            problems.append({
                'task_id': i,
                'text': f'æ¨¡æ‹Ÿç¼–ç¨‹é—®é¢˜ {i+1}',
                'test_list': [f'assert solution() == expected_{i}']
            })
        return problems


def main():
    parser = argparse.ArgumentParser(
        description='BTPå¾®è°ƒå®éªŒæ¡†æ¶ - æ”¯æŒè®ºæ–‡ä¸­çš„ä¸¤ç§é‡‡æ ·æ–¹å¼',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
é‡‡æ ·æ–¹å¼è¯¦è§£:

1. å¹‚æ¬¡é‡‡æ · (Power Sampling):
   P(i) = pi^Î± / Î£ pk^Î±
   - Î± > 1: æ›´å€¾å‘äºé«˜P2Valueçš„ç»éªŒ
   - Î± = 1: æŒ‰P2Valueæ¯”ä¾‹é‡‡æ ·  
   - Î± < 1: æ›´å¹³æ»‘çš„é‡‡æ ·åˆ†å¸ƒ

2. æ’åé‡‡æ · (Rank Sampling):
   pi = 1/rank(i)
   - åŸºäºP2Valueæ’åçš„å€’æ•°
   - å¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥

ä½¿ç”¨ç¤ºä¾‹:

1. å¹‚æ¬¡é‡‡æ ·å®éªŒ:
   python btp_finetune_framework.py \\
     --source-model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --sampling-method power \\
     --sampling-alpha 1.2

2. æ’åé‡‡æ ·å®éªŒ:
   python btp_finetune_framework.py \\
     --source-model deepseek-ai/deepseek-coder-6.7b-instruct \\
     --sampling-method rank

3. è°ƒæ•´P2Valueæƒé‡ (æ›´é‡è§†é€šè¿‡ç‡):
   python btp_finetune_framework.py \\
     --source-model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --p2value-alpha 0.2 \\
     --sampling-alpha 1.5

4. ä¸åŒæºæ¨¡å‹å’Œç›®æ ‡æ¨¡å‹:
   python btp_finetune_framework.py \\
     --source-model deepseek-ai/deepseek-coder-6.7b-instruct \\
     --target-model codellama/CodeLlama-7b-Instruct-hf \\
     --sampling-method power \\
     --sampling-alpha 0.8

5. è°ƒæ•´LoRAå‚æ•°:
   python btp_finetune_framework.py \\
     --source-model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --lora-r 32 \\
     --lora-alpha 64 \\
     --learning-rate 2e-4
""")
    
    # æ¨¡å‹å‚æ•°
    model_group = parser.add_argument_group('æ¨¡å‹å‚æ•°')
    model_group.add_argument('--source-model', required=True,
                           help='ç”¨äºåˆå§‹beam searchç”Ÿæˆçš„æºæ¨¡å‹è·¯å¾„')
    model_group.add_argument('--target-model', 
                           help='ç”¨äºå¾®è°ƒçš„ç›®æ ‡æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤ä¸æºæ¨¡å‹ç›¸åŒï¼‰')
    
    # æ•°æ®é›†å‚æ•°
    data_group = parser.add_argument_group('æ•°æ®é›†å‚æ•°')
    data_group.add_argument('--dataset', default='mbpp',
                          help='æ•°æ®é›†åç§° (é»˜è®¤: mbpp)')
    data_group.add_argument('--max-problems', type=int, default=50,
                          help='å¤„ç†çš„æœ€å¤§é—®é¢˜æ•°é‡')
    
    # BTPç®—æ³•å‚æ•°
    algo_group = parser.add_argument_group('BTPç®—æ³•å‚æ•°')
    algo_group.add_argument('--num-beams', type=int, default=5,
                          help='Beam Searchçš„beamæ•°é‡')
    algo_group.add_argument('--n-iterations', type=int, default=3,
                          help='PPERè®­ç»ƒè¿­ä»£æ¬¡æ•°')
    algo_group.add_argument('--batch-size', type=int, default=100,
                          help='æ¯æ¬¡è¿­ä»£é‡‡æ ·çš„ç»éªŒæ•°é‡')
    
    # é‡‡æ ·å‚æ•° (æ ¸å¿ƒé…ç½®)
    sampling_group = parser.add_argument_group('é‡‡æ ·å‚æ•° (æ ¸å¿ƒé…ç½®)')
    sampling_group.add_argument('--sampling-method', default='power',
                              choices=['power', 'rank'],
                              help='é‡‡æ ·æ–¹æ³•: power(å¹‚æ¬¡) æˆ– rank(æ’å)')
    sampling_group.add_argument('--sampling-alpha', type=float, default=1.0,
                              help='é‡‡æ ·å‚æ•°Î±ï¼Œæ§åˆ¶é‡‡æ ·å€¾å‘æ€§')
    sampling_group.add_argument('--p2value-alpha', type=float, default=0.5,
                              help='P2Valueæƒé‡Î± (0=ä»…é€šè¿‡ç‡, 1=ä»…å¯èƒ½æ€§)')
    
    # LoRAå¾®è°ƒå‚æ•°
    lora_group = parser.add_argument_group('LoRAå¾®è°ƒå‚æ•°')
    lora_group.add_argument('--use-lora', action='store_true', default=True,
                          help='ä½¿ç”¨LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒ')
    lora_group.add_argument('--lora-r', type=int, default=16,
                          help='LoRA rank (ç§©)')
    lora_group.add_argument('--lora-alpha', type=int, default=32,
                          help='LoRAç¼©æ”¾å‚æ•°')
    lora_group.add_argument('--lora-dropout', type=float, default=0.1,
                          help='LoRA dropoutç‡')
    lora_group.add_argument('--lora-target-modules', nargs='+', 
                          default=['q_proj', 'v_proj', 'k_proj', 'o_proj'],
                          help='LoRAç›®æ ‡æ¨¡å—')
    
    # è®­ç»ƒå‚æ•°
    train_group = parser.add_argument_group('è®­ç»ƒå‚æ•°')
    train_group.add_argument('--learning-rate', type=float, default=1e-4,
                           help='å¾®è°ƒå­¦ä¹ ç‡')
    train_group.add_argument('--num-epochs', type=int, default=1,
                           help='æ¯æ¬¡è¿­ä»£çš„è®­ç»ƒè½®æ•°')
    train_group.add_argument('--per-device-batch-size', type=int, default=2,
                           help='æ¯è®¾å¤‡æ‰¹å¤§å°')
    train_group.add_argument('--gradient-accumulation-steps', type=int, default=8,
                           help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    train_group.add_argument('--warmup-steps', type=int, default=100,
                           help='é¢„çƒ­æ­¥æ•°')
    train_group.add_argument('--weight-decay', type=float, default=0.01,
                           help='æƒé‡è¡°å‡')
    
    # è¾“å‡ºå’Œå®éªŒå‚æ•°
    output_group = parser.add_argument_group('è¾“å‡ºå’Œå®éªŒå‚æ•°')
    output_group.add_argument('--output-dir', default='./btp_results',
                            help='ç»“æœè¾“å‡ºç›®å½•')
    output_group.add_argument('--checkpoint-dir', default='./btp_checkpoints',
                            help='æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•')
    output_group.add_argument('--experiment-name', 
                            help='å®éªŒåç§°ï¼ˆç”¨äºç»“æœæ–‡ä»¶å‘½åï¼‰')
    output_group.add_argument('--seed', type=int, default=42,
                            help='éšæœºç§å­')
    output_group.add_argument('--debug', action='store_true',
                            help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    print("ğŸš€ BTPå¾®è°ƒå®éªŒæ¡†æ¶")
    print("=" * 60)
    print("ğŸ“Š å®éªŒé…ç½®:")
    print(f"  ğŸ¤– æºæ¨¡å‹: {args.source_model}")
    print(f"  ğŸ¯ ç›®æ ‡æ¨¡å‹: {args.target_model or 'ä¸æºæ¨¡å‹ç›¸åŒ'}")
    print(f"  ğŸ“š æ•°æ®é›†: {args.dataset} ({args.max_problems} é—®é¢˜)")
    print()
    print("ğŸ² é‡‡æ ·é…ç½®:")
    print(f"  ğŸ“ˆ é‡‡æ ·æ–¹æ³•: {args.sampling_method}")
    if args.sampling_method == 'power':
        print(f"       å…¬å¼: P(i) = pi^{args.sampling_alpha} / Î£ pk^{args.sampling_alpha}")
    else:
        print(f"       å…¬å¼: pi = 1/rank(i)")
    print(f"  âš–ï¸  P2Value Î±: {args.p2value_alpha} (å¯èƒ½æ€§ vs é€šè¿‡ç‡æƒé‡)")
    print()
    print("ğŸ”§ è®­ç»ƒé…ç½®:")
    print(f"  ğŸŒŸ Beamæ•°é‡: {args.num_beams}")
    print(f"  ğŸ”„ è¿­ä»£æ¬¡æ•°: {args.n_iterations}")
    print(f"  ğŸ“¦ æ‰¹å¤§å°: {args.batch_size}")
    print(f"  ğŸ“š å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  ğŸª ä½¿ç”¨LoRA: {args.use_lora}")
    if args.use_lora:
        print(f"       r={args.lora_r}, Î±={args.lora_alpha}, dropout={args.lora_dropout}")
    print()
    
    # å±•ç¤ºå‚æ•°ç»„åˆçš„å½±å“
    print("ğŸ’¡ å‚æ•°è¯´æ˜:")
    
    if args.sampling_method == 'power':
        if args.sampling_alpha > 1.0:
            print(f"  ğŸ“ˆ é‡‡æ ·Î±={args.sampling_alpha} > 1.0: å¼ºçƒˆåå‘é«˜P2Valueç»éªŒ")
        elif args.sampling_alpha == 1.0:
            print(f"  ğŸ“Š é‡‡æ ·Î±={args.sampling_alpha} = 1.0: æŒ‰P2Valueæ¯”ä¾‹é‡‡æ ·")
        else:
            print(f"  ğŸ“‰ é‡‡æ ·Î±={args.sampling_alpha} < 1.0: æ›´å¹³æ»‘çš„é‡‡æ ·åˆ†å¸ƒ")
    else:
        print(f"  ğŸ† æ’åé‡‡æ ·: åŸºäºP2Valueæ’åï¼Œå¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥")
    
    if args.p2value_alpha > 0.5:
        print(f"  ğŸ¯ P2Value Î±={args.p2value_alpha} > 0.5: æ›´é‡è§†ç”Ÿæˆå¯èƒ½æ€§")
    elif args.p2value_alpha == 0.5:
        print(f"  âš–ï¸  P2Value Î±={args.p2value_alpha} = 0.5: å¹³è¡¡å¯èƒ½æ€§å’Œé€šè¿‡ç‡")
    else:
        print(f"  âœ… P2Value Î±={args.p2value_alpha} < 0.5: æ›´é‡è§†æµ‹è¯•é€šè¿‡ç‡")
    
    print()
    print("ğŸ¯ æ¨èé…ç½®ç»„åˆ:")
    print("  ğŸ’ª æ¿€è¿›ç­–ç•¥: --sampling-method power --sampling-alpha 1.5 --p2value-alpha 0.3")
    print("  ğŸ¯ å¹³è¡¡ç­–ç•¥: --sampling-method power --sampling-alpha 1.0 --p2value-alpha 0.5") 
    print("  ğŸ›¡ï¸  ä¿å®ˆç­–ç•¥: --sampling-method rank --p2value-alpha 0.7")
    
    print()
    print("âš ï¸  æ³¨æ„: è¿™æ˜¯å®éªŒæ¡†æ¶æ¼”ç¤ºç‰ˆæœ¬")
    print("   çœŸå®å®ç°éœ€è¦é›†æˆtransformersã€peftç­‰åº“è¿›è¡Œå®é™…å¾®è°ƒ")
    print("   å½“å‰ç‰ˆæœ¬ä¸»è¦å±•ç¤ºBTPç®—æ³•çš„å‚æ•°é…ç½®é€‰é¡¹")
    
    # ç”Ÿæˆå®éªŒé…ç½®æ–‡ä»¶
    config = {
        'experiment_name': args.experiment_name or f"btp_{args.sampling_method}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        'source_model': args.source_model,
        'target_model': args.target_model,
        'sampling_config': {
            'method': args.sampling_method,
            'alpha': args.sampling_alpha
        },
        'p2value_config': {
            'alpha': args.p2value_alpha
        },
        'btp_config': {
            'num_beams': args.num_beams,
            'n_iterations': args.n_iterations,
            'batch_size': args.batch_size
        },
        'lora_config': {
            'use_lora': args.use_lora,
            'r': args.lora_r,
            'alpha': args.lora_alpha,
            'dropout': args.lora_dropout,
            'target_modules': args.lora_target_modules
        } if args.use_lora else None,
        'training_config': {
            'learning_rate': args.learning_rate,
            'num_epochs': args.num_epochs,
            'per_device_batch_size': args.per_device_batch_size,
            'gradient_accumulation_steps': args.gradient_accumulation_steps,
            'warmup_steps': args.warmup_steps,
            'weight_decay': args.weight_decay
        },
        'dataset': args.dataset,
        'max_problems': args.max_problems,
        'seed': args.seed,
        'timestamp': datetime.now().isoformat()
    }
    
    # ä¿å­˜é…ç½®
    os.makedirs(args.output_dir, exist_ok=True)
    config_file = os.path.join(args.output_dir, f"{config['experiment_name']}_config.json")
    
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“ å®éªŒé…ç½®å·²ä¿å­˜åˆ°: {config_file}")
    print("   å¯ç”¨æ­¤é…ç½®æ–‡ä»¶é©±åŠ¨çœŸå®çš„BTPå¾®è°ƒå®éªŒ")


if __name__ == "__main__":
    main() 