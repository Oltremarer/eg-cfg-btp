#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„BTPå®éªŒè„šæœ¬ - æ”¯æŒæœ¬åœ°æ¨¡å‹å¾®è°ƒå’ŒAPIæ¨¡å‹
BTP = Beam Search + Testing + Prioritized Experience Replay

æ”¯æŒçš„åŠŸèƒ½ï¼š
1. æœ¬åœ°æ¨¡å‹çš„BTPå®éªŒï¼ˆä¸å«å¾®è°ƒï¼‰
2. æœ¬åœ°æ¨¡å‹çš„BTPå¾®è°ƒå®éªŒ
3. OpenAI APIçš„BTPå®éªŒ
4. DeepSeek APIçš„BTPå®éªŒ
5. æ··åˆæ¨¡å¼ï¼ˆAPIé‡‡æ ·+æœ¬åœ°å¾®è°ƒï¼‰

ä½¿ç”¨ç¤ºä¾‹ï¼š
1. æœ¬åœ°æ¨¡å‹BTPå®éªŒï¼ˆæ— å¾®è°ƒï¼‰ï¼š
   python experiments/step2_btp_finetune_experiment.py --source-model deepseek-ai/deepseek-coder-1.3b-instruct --mode btp_only

2. æœ¬åœ°æ¨¡å‹å¾®è°ƒï¼š
   python experiments/step2_btp_finetune_experiment.py --source-model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --target-model deepseek-ai/deepseek-coder-1.3b-instruct --mode finetune

3. OpenAI BTPå®éªŒï¼š
   python experiments/step2_btp_finetune_experiment.py --source-model gpt-4 --mode openai --api-key YOUR_KEY

4. æ··åˆæ¨¡å¼ï¼ˆAPIé‡‡æ ·+æœ¬åœ°å¾®è°ƒï¼‰ï¼š
   python experiments/step2_btp_finetune_experiment.py --source-model gpt-4 --target-model deepseek-ai/deepseek-coder-1.3b-instruct --mode hybrid --api-key YOUR_KEY
"""

import os
import sys
import json
import argparse
import numpy as np
import random
import torch
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict, deque
from datetime import datetime
import math
import logging
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# æ¡ä»¶å¯¼å…¥
try:
    # Transformersç›¸å…³å¯¼å…¥
    from transformers import (
        AutoModelForCausalLM, 
        AutoTokenizer, 
        Trainer, 
        TrainingArguments,
        DataCollatorForLanguageModeling,
        EarlyStoppingCallback
    )
    from peft import LoraConfig, get_peft_model, TaskType
    from datasets import Dataset
    HF_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  ç¼ºå°‘HuggingFaceä¾èµ–: {e}")
    print("æœ¬åœ°æ¨¡å‹åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚å¦‚éœ€ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œè¯·å®‰è£…: pip install transformers peft datasets")
    HF_AVAILABLE = False

try:
    from eg_cfg.openai_utils import OpenAIClient, OpenAIInferenceError
    OPENAI_AVAILABLE = True
except ImportError:
    print("âš ï¸  OpenAIå·¥å…·ä¸å¯ç”¨ï¼ŒOpenAIåŠŸèƒ½å°†è¢«ç¦ç”¨")
    OPENAI_AVAILABLE = False

try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# é¡¹ç›®ç›¸å…³å¯¼å…¥
from eg_cfg.mbpp_utils import load_mbpp_problems, run_tests
if HF_AVAILABLE:
    from eg_cfg.model_utils import setup_device, load_model, load_tokenizer


class ModelAdapter:
    """ç»Ÿä¸€æ¨¡å‹é€‚é…å™¨ - æ”¯æŒæœ¬åœ°å’ŒAPIæ¨¡å‹"""
    
    def __init__(self, model_name: str, model_type: str = "local", 
                 api_key: str = None, api_base: str = None, **kwargs):
        self.model_name = model_name
        self.model_type = model_type
        self.api_key = api_key
        self.api_base = api_base
        self.kwargs = kwargs
        
        self.model = None
        self.tokenizer = None
        self.device = None
        self._setup_model()
    
    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        if self.model_type == "local":
            self._setup_local_model()
        elif self.model_type == "openai":
            self._setup_openai_model()
        elif self.model_type in ["deepseek", "api"]:
            self._setup_api_model()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _setup_local_model(self):
        """è®¾ç½®æœ¬åœ°æ¨¡å‹"""
        if not HF_AVAILABLE:
            raise ImportError("æœ¬åœ°æ¨¡å‹éœ€è¦å®‰è£…transformersåº“")
        
        print(f"ğŸ”§ åŠ è½½æœ¬åœ°æ¨¡å‹: {self.model_name}")
        
        self.device = setup_device()
        self.model, self.tokenizer = load_model(self.model_name, self.device)
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def _setup_openai_model(self):
        """è®¾ç½®OpenAIæ¨¡å‹"""
        if not OPENAI_AVAILABLE:
            raise ImportError("OpenAIæ¨¡å‹éœ€è¦å®‰è£…openaiç›¸å…³ä¾èµ–")
        
        print(f"ğŸ”§ é…ç½®OpenAIæ¨¡å‹: {self.model_name}")
        self.client = OpenAIClient(api_key=self.api_key, model=self.model_name)
    
    def _setup_api_model(self):
        """è®¾ç½®APIæ¨¡å‹"""
        if not REQUESTS_AVAILABLE:
            raise ImportError("APIæ¨¡å‹éœ€è¦å®‰è£…requestsåº“")
        
        print(f"ğŸ”§ é…ç½®APIæ¨¡å‹: {self.model_name}")
        self.api_headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def generate(self, prompt: str, **generation_kwargs) -> List[Dict]:
        """ç»Ÿä¸€ç”Ÿæˆæ¥å£"""
        if self.model_type == "local":
            return self._generate_local(prompt, **generation_kwargs)
        elif self.model_type == "openai":
            return self._generate_openai(prompt, **generation_kwargs)
        elif self.model_type in ["deepseek", "api"]:
            return self._generate_api(prompt, **generation_kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _generate_local(self, prompt: str, num_beams: int = 5, 
                       temperature: float = 0.8, max_tokens: int = 512,
                       **kwargs) -> List[Dict]:
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                num_beams=num_beams,
                num_return_sequences=num_beams,
                max_new_tokens=max_tokens,
                do_sample=temperature > 0,
                temperature=temperature if temperature > 0 else 1.0,
                return_dict_in_generate=True,
                output_scores=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                **kwargs
            )
        
        results = []
        sequences = outputs.sequences
        scores = outputs.sequences_scores if hasattr(outputs, 'sequences_scores') else None
        
        for i, sequence in enumerate(sequences):
            generated_text = self.tokenizer.decode(sequence, skip_special_tokens=True)
            code = generated_text[len(prompt):].strip()
            
            if scores is not None:
                log_prob = scores[i].item()
                possibility = min(math.exp(log_prob / len(sequence)), 1.0)
            else:
                log_prob = -10.0
                possibility = 0.5
            
            results.append({
                'code': code,
                'possibility': possibility,
                'log_prob': log_prob,
                'beam_rank': i,
                'sequence_length': len(sequence) - inputs['input_ids'].shape[1]
            })
        
        return results
    
    def _generate_openai(self, prompt: str, num_beams: int = 5, 
                        temperature: float = 0.8, **kwargs) -> List[Dict]:
        """OpenAIæ¨¡å‹ç”Ÿæˆ"""
        results = []
        
        try:
            solutions = self.client.generate_code(
                prompt=prompt,
                max_tokens=512,
                temperature=temperature,
                n=num_beams
            )
            
            for i, code in enumerate(solutions):
                # ä¸ºOpenAIç”Ÿæˆçš„ä»£ç è®¡ç®—å¯èƒ½æ€§åˆ†æ•°
                possibility = max(0.1, 1.0 - (temperature * 0.5) - (i * 0.1))
                
                results.append({
                    'code': code,
                    'possibility': possibility,
                    'beam_rank': i,
                    'temperature': temperature
                })
                
        except Exception as e:
            print(f"âš ï¸  OpenAIç”Ÿæˆå¤±è´¥: {e}")
            for i in range(num_beams):
                results.append({
                    'code': f"# APIè°ƒç”¨å¤±è´¥: {e}",
                    'possibility': 0.0,
                    'beam_rank': i
                })
        
        return results
    
    def _generate_api(self, prompt: str, num_beams: int = 5, 
                     temperature: float = 0.8, **kwargs) -> List[Dict]:
        """é€šç”¨APIæ¨¡å‹ç”Ÿæˆ"""
        results = []
        api_url = self.api_base or "https://api.deepseek.com/v1/chat/completions"
        
        for i in range(num_beams):
            try:
                payload = {
                    "model": self.model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": temperature,
                    "max_tokens": 512
                }
                
                response = requests.post(api_url, headers=self.api_headers, json=payload)
                response.raise_for_status()
                
                data = response.json()
                code = data['choices'][0]['message']['content'].strip()
                possibility = random.uniform(0.3, 0.9)  # æ¨¡æ‹Ÿæ¦‚ç‡
                
                results.append({
                    'code': code,
                    'possibility': possibility,
                    'beam_rank': i
                })
                
            except Exception as e:
                print(f"âš ï¸  APIè°ƒç”¨å¤±è´¥: {e}")
                results.append({
                    'code': f"# APIè°ƒç”¨å¤±è´¥: {e}",
                    'possibility': 0.0,
                    'beam_rank': i
                })
        
        return results


class P2ValueCalculator:
    """P2Valueè®¡ç®—å™¨"""
    
    def __init__(self, alpha: float = 0.5):
        self.alpha = alpha
    
    def calculate_p2value(self, possibility: float, pass_rate: float) -> float:
        """è®¡ç®—P2Value = Î± Ã— possibility + (1-Î±) Ã— pass_rate"""
        return self.alpha * possibility + (1 - self.alpha) * pass_rate
    
    def calculate_p2value_extended(self, log_prob=None, sequence_length=None, 
                                 possibility=None, passed_tests=0, total_tests=1):
        """æ‰©å±•çš„P2Valueè®¡ç®—ï¼Œå…¼å®¹ä¸åŒè¾“å…¥æ ¼å¼"""
        if possibility is None:
            if log_prob is not None and sequence_length is not None:
                possibility = min(math.exp(log_prob / max(sequence_length, 1)), 1.0)
            else:
                possibility = 0.5  # é»˜è®¤å€¼
        
        pass_rate = passed_tests / max(total_tests, 1)
        p2value = self.alpha * possibility + (1 - self.alpha) * pass_rate
        
        return p2value, possibility, pass_rate


class PrioritizedSampler:
    """ä¼˜å…ˆç»éªŒé‡‡æ ·å™¨ - æ”¯æŒä¸¤ç§é‡‡æ ·æ–¹å¼"""
    
    def __init__(self, sampling_method: str = "power", alpha: float = 1.0):
        """
        Args:
            sampling_method: "power" æˆ– "rank"
            alpha: é‡‡æ ·å‚æ•°
        """
        self.sampling_method = sampling_method
        self.alpha = alpha
    
    def sample(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """æ ¹æ®P2Valueè¿›è¡Œä¼˜å…ˆé‡‡æ ·"""
        if not experiences:
            return []
        
        if self.sampling_method == "power":
            return self._power_sampling(experiences, batch_size)
        elif self.sampling_method == "rank":
            return self._rank_sampling(experiences, batch_size)
        else:
            raise ValueError(f"Unknown sampling method: {self.sampling_method}")
    
    def _power_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """å¹‚æ¬¡é‡‡æ ·: P(i) = pi^Î± / Î£ pk^Î±"""
        p2values = np.array([exp['p2value'] for exp in experiences])
        
        # é¿å…é™¤é›¶é”™è¯¯
        p2values = np.maximum(p2values, 1e-8)
        
        # è®¡ç®—é‡‡æ ·æ¦‚ç‡
        powered_values = np.power(p2values, self.alpha)
        probabilities = powered_values / np.sum(powered_values)
        
        # é‡‡æ ·
        indices = np.random.choice(
            len(experiences), 
            size=min(batch_size, len(experiences)), 
            p=probabilities, 
            replace=False
        )
        
        return [experiences[i] for i in indices]
    
    def _rank_sampling(self, experiences: List[Dict], batch_size: int) -> List[Dict]:
        """æ’åé‡‡æ ·: pi = 1/rank(i)"""
        # æŒ‰P2Valueæ’åº
        sorted_experiences = sorted(experiences, key=lambda x: x['p2value'], reverse=True)
        
        # è®¡ç®—æ’åæ¦‚ç‡
        ranks = np.arange(1, len(sorted_experiences) + 1)
        probabilities = 1.0 / ranks
        probabilities = probabilities / np.sum(probabilities)
        
        # é‡‡æ ·
        indices = np.random.choice(
            len(sorted_experiences), 
            size=min(batch_size, len(sorted_experiences)), 
            p=probabilities, 
            replace=False
        )
        
        return [sorted_experiences[i] for i in indices]


class ExperienceReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    
    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self.buffer = deque(maxlen=max_size)
        self.p2calculator = P2ValueCalculator()
    
    def add_experience(self, experience: Dict):
        """æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº"""
        if 'p2value' not in experience:
            experience['p2value'] = self.p2calculator.calculate_p2value(
                experience['possibility'], 
                experience['pass_rate']
            )
        self.buffer.append(experience)
    
    def get_all_experiences(self) -> List[Dict]:
        """è·å–æ‰€æœ‰ç»éªŒ"""
        return list(self.buffer)
    
    def get_stats(self, include_samples: bool = False, max_samples: int = 10) -> Dict:
        """è·å–ç¼“å†²åŒºç»Ÿè®¡ä¿¡æ¯"""
        if not self.buffer:
            return {}
        
        experiences = list(self.buffer)
        p2values = [exp['p2value'] for exp in experiences]
        pass_rates = [exp['pass_rate'] for exp in experiences]
        
        stats = {
            'total_experiences': len(experiences),
            'avg_p2value': np.mean(p2values),
            'std_p2value': np.std(p2values),
            'avg_pass_rate': np.mean(pass_rates),
            'fully_passed_count': sum(1 for exp in experiences if exp['pass_rate'] >= 1.0),
            'zero_passed_count': sum(1 for exp in experiences if exp['pass_rate'] == 0.0),
            'max_p2value': np.max(p2values),
            'min_p2value': np.min(p2values)
        }
        
        # å¦‚æœéœ€è¦åŒ…å«æ ·æœ¬æ•°æ®ï¼Œæ·»åŠ ä¸€äº›ä»£è¡¨æ€§æ ·æœ¬
        if include_samples:
            # è·å–ä¸åŒé€šè¿‡ç‡çš„æ ·æœ¬
            samples = []
            
            # å°è¯•æ‰¾åˆ°é€šè¿‡ç‡æœ€é«˜çš„æ ·æœ¬
            best_experiences = sorted(experiences, key=lambda x: x['pass_rate'], reverse=True)[:max_samples//2]
            samples.extend(best_experiences)
            
            # æ·»åŠ ä¸€äº›éšæœºæ ·æœ¬
            import random
            remaining_samples = max_samples - len(samples)
            if remaining_samples > 0 and len(experiences) > len(samples):
                random_experiences = random.sample(
                    [exp for exp in experiences if exp not in samples], 
                    min(remaining_samples, len(experiences) - len(samples))
                )
                samples.extend(random_experiences)
            
            stats['sample_experiences'] = samples
        
        return stats


class BTPModelManager:
    """BTPæ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self, 
                 source_model_path: str,
                 target_model_path: Optional[str] = None,
                 use_lora: bool = True,
                 lora_config: Optional[Dict] = None):
        """
        Args:
            source_model_path: ç”¨äºç”Ÿæˆåˆå§‹ç»éªŒçš„æ¨¡å‹è·¯å¾„
            target_model_path: è¢«å¾®è°ƒçš„ç›®æ ‡æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸åŒäºæºæ¨¡å‹ï¼‰
            use_lora: æ˜¯å¦ä½¿ç”¨LoRAå¾®è°ƒ
            lora_config: LoRAé…ç½®å‚æ•°
        """
        self.source_model_path = source_model_path
        self.target_model_path = target_model_path or source_model_path
        self.use_lora = use_lora
        
        # åŠ è½½æºæ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆï¼‰
        print(f"Loading source model: {self.source_model_path}")
        self.source_model = AutoModelForCausalLM.from_pretrained(
            self.source_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(self.source_model_path, trust_remote_code=True)
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½ç›®æ ‡æ¨¡å‹ï¼ˆç”¨äºå¾®è°ƒï¼‰
        if self.target_model_path != self.source_model_path:
            print(f"Loading target model: {self.target_model_path}")
            self.target_model = AutoModelForCausalLM.from_pretrained(
                self.target_model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
        else:
            self.target_model = self.source_model
        
        # é…ç½®LoRA
        if self.use_lora:
            default_lora_config = {
                "r": 64,
                "lora_alpha": 128,
                "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
                "lora_dropout": 0.1,
                "bias": "none",
                "task_type": TaskType.CAUSAL_LM
            }
            
            if lora_config:
                default_lora_config.update(lora_config)
            
            lora_config_obj = LoraConfig(**default_lora_config)
            self.target_model = get_peft_model(self.target_model, lora_config_obj)
    
    def generate_beam_candidates(self, 
                                prompt: str, 
                                num_beams: int = 5, 
                                max_new_tokens: int = 512) -> List[Dict]:
        """ä½¿ç”¨beam searchç”Ÿæˆå€™é€‰è§£å†³æ–¹æ¡ˆ"""
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=1024)
        inputs = {k: v.to(self.source_model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.source_model.generate(
                **inputs,
                num_beams=num_beams,
                num_return_sequences=num_beams,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                return_dict_in_generate=True,
                output_scores=True,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        candidates = []
        for i, sequence in enumerate(outputs.sequences):
            generated_text = self.tokenizer.decode(sequence, skip_special_tokens=True)
            code = generated_text[len(prompt):].strip()
            
            # è®¡ç®—ç”Ÿæˆæ¦‚ç‡
            if hasattr(outputs, 'sequences_scores') and outputs.sequences_scores is not None:
                log_prob = outputs.sequences_scores[i].item()
                possibility = min(math.exp(log_prob / len(sequence)), 1.0)
            else:
                possibility = 1.0 / (i + 1)  # ç®€å•çš„æ’åæ¦‚ç‡
            
            candidates.append({
                'code': code,
                'possibility': possibility,
                'beam_rank': i
            })
        
        return candidates


class BTPFineTuneExperiment:
    """BTPå¾®è°ƒå®éªŒä¸»ç±»"""
    
    def __init__(self, 
                 source_model_path: str,
                 target_model_path: Optional[str] = None,
                 dataset: str = "mbpp",
                 sampling_method: str = "power",
                 sampling_alpha: float = 1.0,
                 p2value_alpha: float = 0.5,
                 use_lora: bool = True,
                 lora_config: Optional[Dict] = None):
        
        self.dataset = dataset
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model_manager = BTPModelManager(
            source_model_path, target_model_path, use_lora, lora_config
        )
        self.experience_buffer = ExperienceReplayBuffer()
        self.sampler = PrioritizedSampler(sampling_method, sampling_alpha)
        self.p2calculator = P2ValueCalculator(p2value_alpha)
        
        # åŠ è½½æ•°æ®é›†
        print(f"Loading dataset: {dataset}")
        if dataset == "mbpp":
            self.problems = load_mbpp_problems()
        else:
            raise ValueError(f"Unsupported dataset: {dataset}")
        
        print(f"Loaded {len(self.problems)} problems")
    
    def format_problem_prompt(self, problem: Dict) -> str:
        """æ ¼å¼åŒ–é—®é¢˜ä¸ºæç¤ºè¯"""
        test_examples = "\n".join([f"  {test}" for test in problem.get('test_list', [])])
        
        return f"""Solve the following programming problem:

Problem: {problem['text']}

Test cases:
{test_examples}

Provide a complete Python function:

```python
"""
    
    def phase1_beam_search_sampling(self, problems: List[Dict], num_beams: int = 5) -> None:
        """é˜¶æ®µ1: Beam Searché‡‡æ ·é˜¶æ®µ"""
        print("Phase 1: Beam Search Sampling")
        
        for problem_id, problem in tqdm(problems, desc="Beam Search Sampling"):
            prompt = self.format_problem_prompt(problem)
            
            # ç”Ÿæˆbeam candidates
            candidates = self.model_manager.generate_beam_candidates(
                prompt, num_beams=num_beams
            )
            
            # æµ‹è¯•æ¯ä¸ªå€™é€‰å¹¶å­˜å‚¨ç»éªŒ
            for candidate in candidates:
                try:
                    test_results = run_tests(candidate['code'], problem['test_list'])
                    passed_tests = sum(1 for r in test_results.values() if r.get('result', False))
                    total_tests = len(test_results)
                    pass_rate = passed_tests / total_tests if total_tests > 0 else 0.0
                    
                    experience = {
                        'problem_id': problem_id,
                        'problem_text': problem['text'],
                        'code': candidate['code'],
                        'possibility': candidate['possibility'],
                        'pass_rate': pass_rate,
                        'test_results': test_results,
                        'beam_rank': candidate['beam_rank']
                    }
                    
                    self.experience_buffer.add_experience(experience)
                    
                except Exception as e:
                    # è®°å½•å¤±è´¥ä½†ç»§ç»­
                    experience = {
                        'problem_id': problem_id,
                        'problem_text': problem['text'],
                        'code': candidate['code'],
                        'possibility': candidate['possibility'],
                        'pass_rate': 0.0,
                        'error': str(e),
                        'beam_rank': candidate['beam_rank']
                    }
                    self.experience_buffer.add_experience(experience)
    
    def phase2_pper_training(self, 
                           n_iterations: int = 3,
                           batch_size: int = 100,
                           training_args: Optional[TrainingArguments] = None) -> None:
        """é˜¶æ®µ2: PPERè®­ç»ƒé˜¶æ®µ"""
        print(f"Phase 2: PPER Training ({n_iterations} iterations)")
        
        for iteration in range(n_iterations):
            print(f"\nIteration {iteration + 1}/{n_iterations}")
            
            # ä¼˜å…ˆé‡‡æ ·é«˜è´¨é‡ç»éªŒ
            all_experiences = self.experience_buffer.get_all_experiences()
            sampled_experiences = self.sampler.sample(all_experiences, batch_size)
            
            print(f"Sampled {len(sampled_experiences)} experiences for training")
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            train_dataset = self._prepare_training_dataset(sampled_experiences)
            
            # æ‰§è¡Œå¾®è°ƒ
            self._finetune_model(train_dataset, training_args, iteration)
            
            # å¯é€‰ï¼šç”¨å¾®è°ƒåçš„æ¨¡å‹é‡æ–°è¯„ä¼°éƒ¨åˆ†é—®é¢˜
            # self._re_evaluate_with_finetuned_model(sample_problems)
    
    def _prepare_training_dataset(self, experiences: List[Dict]) -> Dataset:
        """å‡†å¤‡å¾®è°ƒè®­ç»ƒæ•°æ®é›†"""
        texts = []
        
        for exp in experiences:
            # æ„é€ æŒ‡ä»¤å¾®è°ƒæ ¼å¼
            instruction = f"Solve this programming problem:\n{exp['problem_text']}"
            response = exp['code']
            
            # æ ¼å¼åŒ–ä¸ºè®­ç»ƒæ–‡æœ¬
            text = f"### Instruction:\n{instruction}\n\n### Response:\n{response}{self.model_manager.tokenizer.eos_token}"
            texts.append(text)
        
        # åˆ†è¯ - ä¿®å¤è¿™é‡Œçš„æ‰¹å¤„ç†é—®é¢˜
        def tokenize_function(examples):
            # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            if isinstance(examples['text'], str):
                examples['text'] = [examples['text']]
            
            tokenized = self.model_manager.tokenizer(
                examples['text'],
                truncation=True,
                padding=True,
                max_length=1024,
                return_tensors="pt"
            )
            
            # ä¸ºè¯­è¨€æ¨¡å‹å‡†å¤‡labels
            tokenized["labels"] = tokenized["input_ids"].clone()
            
            return tokenized
        
        dataset = Dataset.from_dict({'text': texts})
        tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['text'])
        
        return tokenized_dataset
    
    def _finetune_model(self, 
                       train_dataset: Dataset, 
                       training_args: Optional[TrainingArguments],
                       iteration: int) -> None:
        """æ‰§è¡Œæ¨¡å‹å¾®è°ƒ"""
        
        if training_args is None:
            training_args = TrainingArguments(
                output_dir=f"./btp_checkpoints/iteration_{iteration}",
                num_train_epochs=1,
                per_device_train_batch_size=4,
                gradient_accumulation_steps=4,
                warmup_steps=50,
                learning_rate=2e-5,
                fp16=True,
                logging_steps=10,
                save_steps=100,
                save_total_limit=2,
                remove_unused_columns=False,
            )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.model_manager.tokenizer,
            mlm=False,
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model_manager.target_model,
            args=training_args,
            train_dataset=train_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print(f"Starting fine-tuning iteration {iteration}...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        print(f"Model saved to {training_args.output_dir}")
    
    def run_experiment(self, 
                      max_problems: int = 100,
                      num_beams: int = 5,
                      n_iterations: int = 3,
                      batch_size: int = 100,
                      output_dir: str = "./btp_results") -> Dict:
        """è¿è¡Œå®Œæ•´çš„BTPå®éªŒ"""
        
        print("=" * 60)
        print("BTP Fine-tuning Experiment")
        print("=" * 60)
        
        # é€‰æ‹©é—®é¢˜å­é›†
        problems_list = list(self.problems.items())[:max_problems]
        
        # é˜¶æ®µ1: Beam Search + Testing
        self.phase1_beam_search_sampling(problems_list, num_beams)
        
        # è¾“å‡ºåˆå§‹ç»Ÿè®¡
        initial_stats = self.experience_buffer.get_stats(include_samples=True, max_samples=20)
        print(f"\nInitial experience buffer stats:")
        for key, value in initial_stats.items():
            if key != 'sample_experiences':  # ä¸æ‰“å°æ ·æœ¬æ•°æ®ï¼Œå¤ªé•¿äº†
            print(f"  {key}: {value}")
        
        # é˜¶æ®µ2: PPERå¾®è°ƒ
        self.phase2_pper_training(n_iterations, batch_size)
        
        # è·å–æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯ï¼ˆåŒ…å«æ ·æœ¬ï¼‰
        final_stats = self.experience_buffer.get_stats(include_samples=True, max_samples=20)
        
        # è·å–æ‰€æœ‰ç»éªŒæ•°æ®
        all_experiences = self.experience_buffer.get_all_experiences()
        
        # ä¿å­˜å®éªŒç»“æœ
        results = {
            'experiment_type': 'BTP_FineTune',
            'source_model': self.model_manager.source_model_path,
            'target_model': self.model_manager.target_model_path,
            'dataset': self.dataset,
            'sampling_method': self.sampler.sampling_method,
            'sampling_alpha': self.sampler.alpha,
            'p2value_alpha': self.p2calculator.alpha,
            'max_problems': max_problems,
            'num_beams': num_beams,
            'n_iterations': n_iterations,
            'batch_size': batch_size,
            'initial_stats': initial_stats,
            'final_stats': final_stats,
            'all_experiences': all_experiences,  # ä¿å­˜æ‰€æœ‰ç”Ÿæˆçš„ä»£ç å’Œç»“æœ
            'timestamp': datetime.now().isoformat()
        }
        
        # ä¿å­˜ç»“æœ
        os.makedirs(output_dir, exist_ok=True)
        result_file = os.path.join(output_dir, f"btp_finetune_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
        # ä¿å­˜å®Œæ•´ç»“æœï¼ˆåŒ…å«æ‰€æœ‰ç”Ÿæˆçš„ä»£ç ï¼‰
        print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜å®Œæ•´å®éªŒç»“æœ...")
        print(f"   - åŒ…å« {len(all_experiences)} ä¸ªå®Œæ•´çš„ä»£ç ç”Ÿæˆæ ·æœ¬")
        print(f"   - æ¯ä¸ªæ ·æœ¬åŒ…å«ï¼šç”Ÿæˆä»£ç ã€æµ‹è¯•ç»“æœã€é€šè¿‡ç‡ã€æ¦‚ç‡ç­‰")
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        import os
        file_size = os.path.getsize(result_file) / (1024 * 1024)  # MB
        print(f"\nâœ… å®éªŒç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
        print(f"ğŸ“Š åŒ…å«å†…å®¹:")
        print(f"   - å®éªŒé…ç½®å’Œç»Ÿè®¡ä¿¡æ¯")
        print(f"   - {len(all_experiences)} ä¸ªå®Œæ•´çš„ä»£ç ç”Ÿæˆè®°å½•")
        print(f"   - æ¯ä¸ªé—®é¢˜çš„æµ‹è¯•ç»“æœè¯¦æƒ…")
        
        # æ‰“å°ä¸€äº›ç”Ÿæˆçš„ä»£ç æ ·æœ¬ç”¨äºè°ƒè¯•
        print("\n" + "="*80)
        print("ğŸ” ç”Ÿæˆä»£ç æ ·æœ¬åˆ†æ (ç”¨äºè°ƒè¯•0%é€šè¿‡ç‡é—®é¢˜)")
        print("="*80)
        
        if 'sample_experiences' in final_stats:
            samples = final_stats['sample_experiences'][:5]  # åªçœ‹å‰5ä¸ª
            for i, exp in enumerate(samples):
                print(f"\nğŸ“ æ ·æœ¬ {i+1}:")
                print(f"   é—®é¢˜ID: {exp.get('problem_id', 'N/A')}")
                print(f"   é€šè¿‡ç‡: {exp.get('pass_rate', 0):.2f}")
                print(f"   ç”Ÿæˆæ¦‚ç‡: {exp.get('possibility', 0):.4f}")
                print(f"   ç”Ÿæˆä»£ç :")
                print("   " + "-"*60)
                code_lines = str(exp.get('code', '')).split('\n')
                for line in code_lines[:10]:  # åªæ˜¾ç¤ºå‰10è¡Œ
                    print(f"   {line}")
                if len(code_lines) > 10:
                    print(f"   ... (è¿˜æœ‰ {len(code_lines)-10} è¡Œ)")
                print("   " + "-"*60)
                
                # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
                if 'test_results' in exp and exp['test_results']:
                    test_results = exp['test_results']
                    passed = sum(1 for r in test_results.values() if r.get('result', False))
                    total = len(test_results)
                    print(f"   æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
                    
                    # æ˜¾ç¤ºå¤±è´¥çš„æµ‹è¯•ï¼ˆå¦‚æœæœ‰ï¼‰
                    failed_tests = [k for k, v in test_results.items() if not v.get('result', False)]
                    if failed_tests:
                        print(f"   å¤±è´¥æµ‹è¯•: {failed_tests[:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥æµ‹è¯•
                
        print("="*80)
        
        return results


def main():
    parser = argparse.ArgumentParser(
        description='ç»Ÿä¸€çš„BTPå®éªŒè„šæœ¬ - æ”¯æŒæœ¬åœ°æ¨¡å‹å¾®è°ƒå’ŒAPIæ¨¡å‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:

1. æœ¬åœ°æ¨¡å‹BTPå®éªŒï¼ˆæ— å¾®è°ƒï¼‰:
   python experiments/step2_btp_finetune_experiment.py \\
     --source-model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode btp_only --max-problems 50

2. æœ¬åœ°æ¨¡å‹å¾®è°ƒ:
   python experiments/step2_btp_finetune_experiment.py \\
     --source-model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \\
     --target-model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --mode finetune --max-problems 100

3. OpenAI BTPå®éªŒ:
   python experiments/step2_btp_finetune_experiment.py \\
     --source-model gpt-4 --mode openai \\
     --api-key YOUR_OPENAI_KEY --max-problems 30

4. æ··åˆæ¨¡å¼ï¼ˆAPIé‡‡æ ·+æœ¬åœ°å¾®è°ƒï¼‰:
   python experiments/step2_btp_finetune_experiment.py \\
     --source-model gpt-4 --mode hybrid \\
     --target-model deepseek-ai/deepseek-coder-1.3b-instruct \\
     --api-key YOUR_OPENAI_KEY --max-problems 50

5. DeepSeek APIå®éªŒ:
   python experiments/step2_btp_finetune_experiment.py \\
     --source-model deepseek-chat --mode deepseek \\
     --api-key YOUR_DEEPSEEK_KEY \\
     --api-base https://api.deepseek.com --max-problems 30
        """)
    
    # å®éªŒæ¨¡å¼
    parser.add_argument('--mode', type=str, default='finetune',
                       choices=['btp_only', 'finetune', 'openai', 'deepseek', 'hybrid'],
                       help='å®éªŒæ¨¡å¼')
    
    # æ¨¡å‹ç›¸å…³å‚æ•°
    model_group = parser.add_argument_group('æ¨¡å‹å‚æ•°')
    model_group.add_argument('--source-model', type=str, required=True,
                           help='æºæ¨¡å‹è·¯å¾„æˆ–åç§°')
    model_group.add_argument('--target-model', type=str, default=None,
                           help='ç›®æ ‡æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºå¾®è°ƒï¼‰')
    
    # APIå‚æ•°
    api_group = parser.add_argument_group('APIå‚æ•°')
    api_group.add_argument('--api-key', type=str,
                         help='APIå¯†é’¥ï¼ˆOpenAI/DeepSeekç­‰ï¼‰')
    api_group.add_argument('--api-base', type=str,
                         help='APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰')
    
    # æ•°æ®é›†å‚æ•°
    data_group = parser.add_argument_group('æ•°æ®é›†å‚æ•°')
    data_group.add_argument('--dataset', type=str, default='mbpp',
                          choices=['mbpp', 'humaneval'],
                          help='æ•°æ®é›†åç§°')
    data_group.add_argument('--max-problems', type=int, default=50,
                          help='æœ€å¤§é—®é¢˜æ•°é‡')
    
    # BTPç®—æ³•å‚æ•°
    btp_group = parser.add_argument_group('BTPç®—æ³•å‚æ•°')
    btp_group.add_argument('--num-beams', type=int, default=5,
                         help='Beam Searchæ•°é‡')
    btp_group.add_argument('--n-iterations', type=int, default=2,
                         help='PPERè®­ç»ƒè¿­ä»£æ¬¡æ•°')
    btp_group.add_argument('--batch-size', type=int, default=50,
                         help='è®­ç»ƒæ‰¹å¤§å°')
    
    # é‡‡æ ·å‚æ•°
    sampling_group = parser.add_argument_group('é‡‡æ ·å‚æ•°')
    sampling_group.add_argument('--sampling-method', type=str, default='power', 
                              choices=['power', 'rank'],
                              help='é‡‡æ ·æ–¹æ³•')
    sampling_group.add_argument('--sampling-alpha', type=float, default=1.0,
                              help='é‡‡æ ·Î±å‚æ•°')
    sampling_group.add_argument('--p2value-alpha', type=float, default=0.5,
                              help='P2Valueæƒé‡Î±')
    
    # LoRAå‚æ•°
    lora_group = parser.add_argument_group('LoRAå‚æ•°')
    lora_group.add_argument('--use-lora', action='store_true', default=True,
                          help='ä½¿ç”¨LoRAå¾®è°ƒ')
    lora_group.add_argument('--lora-r', type=int, default=16,
                          help='LoRA rank')
    lora_group.add_argument('--lora-alpha', type=int, default=32,
                          help='LoRA alpha')
    lora_group.add_argument('--lora-dropout', type=float, default=0.1,
                          help='LoRA dropout')
    
    # è®­ç»ƒå‚æ•°
    train_group = parser.add_argument_group('è®­ç»ƒå‚æ•°')
    train_group.add_argument('--learning-rate', type=float, default=1e-4,
                           help='å­¦ä¹ ç‡')
    train_group.add_argument('--num-epochs', type=int, default=1,
                           help='æ¯è½®è¿­ä»£çš„è®­ç»ƒè½®æ•°')
    train_group.add_argument('--per-device-batch-size', type=int, default=2,
                           help='æ¯è®¾å¤‡æ‰¹å¤§å°')
    train_group.add_argument('--gradient-accumulation-steps', type=int, default=4,
                           help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    
    # è¾“å‡ºå‚æ•°
    output_group = parser.add_argument_group('è¾“å‡ºå‚æ•°')
    output_group.add_argument('--output-dir', type=str, default='./btp_results',
                            help='ç»“æœè¾“å‡ºç›®å½•')
    output_group.add_argument('--checkpoint-dir', type=str, default='./btp_checkpoints',
                            help='æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ—¥å¿—')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    
    # å‡†å¤‡LoRAé…ç½®
    lora_config = {
        'r': args.lora_r,
        'lora_alpha': args.lora_alpha,
        'lora_dropout': args.lora_dropout
    } if args.use_lora else None
    
    # æ‰“å°é…ç½®
    print("BTP Fine-tuning Experiment Configuration:")
    print(f"  Source Model: {args.source_model}")
    print(f"  Target Model: {args.target_model or 'same as source'}")
    print(f"  Dataset: {args.dataset}")
    print(f"  Max Problems: {args.max_problems}")
    print(f"  Sampling Method: {args.sampling_method}")
    print(f"  Sampling Alpha: {args.sampling_alpha}")
    print(f"  P2Value Alpha: {args.p2value_alpha}")
    print(f"  Use LoRA: {args.use_lora}")
    if args.use_lora:
        print(f"  LoRA Config: r={args.lora_r}, alpha={args.lora_alpha}, dropout={args.lora_dropout}")
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = BTPFineTuneExperiment(
        source_model_path=args.source_model,
        target_model_path=args.target_model,
        dataset=args.dataset,
        sampling_method=args.sampling_method,
        sampling_alpha=args.sampling_alpha,
        p2value_alpha=args.p2value_alpha,
        use_lora=args.use_lora,
        lora_config=lora_config
    )
    
    # è¿è¡Œå®éªŒ - æ·»åŠ å¼‚å¸¸å¤„ç†
    try:
        results = experiment.run_experiment(
            max_problems=args.max_problems,
            num_beams=args.num_beams,
            n_iterations=args.n_iterations,
            batch_size=args.batch_size,
            output_dir=args.output_dir
        )
        
        print("\nExperiment completed successfully!")
        print(f"Final stats:")
        for key, value in results['final_stats'].items():
            print(f"  {key}: {value}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ Experiment failed: {e}")
        import traceback
        traceback.print_exc()
        
        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        try:
            os.makedirs(args.output_dir, exist_ok=True)
            error_info = {
                'error': str(e),
                'traceback': traceback.format_exc(),
                'timestamp': datetime.now().isoformat(),
                'experiment_type': 'BTP_FineTune',
                'source_model': args.source_model,
                'target_model': args.target_model,
                'dataset': args.dataset,
                'max_problems': args.max_problems,
                'num_beams': args.num_beams,
                'n_iterations': args.n_iterations,
                'batch_size': args.batch_size,
                'status': 'failed'
            }
            
            error_file = os.path.join(args.output_dir, f"error_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(error_file, 'w', encoding='utf-8') as f:
                json.dump(error_info, f, indent=2, ensure_ascii=False)
            
            print(f"Error log saved to: {error_file}")
            
        except Exception as save_error:
            print(f"Failed to save error log: {save_error}")
        
        return 1


if __name__ == "__main__":
    import sys
    sys.exit(main()) 