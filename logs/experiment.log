nohup: ignoring input
BTP Fine-tuning Experiment Configuration:
  Source Model: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
  Target Model: deepseek-ai/deepseek-coder-1.3b-instruct
  Dataset: mbpp
  Max Problems: 30
  Sampling Method: power
  Sampling Alpha: 1.0
  P2Value Alpha: 0.5
  Use LoRA: True
  LoRA Config: r=16, alpha=32, dropout=0.1
Loading source model: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  50%|█████     | 2/4 [24:32<24:32, 736.18s/it]Downloading shards:  75%|███████▌  | 3/4 [49:43<17:39, 1059.17s/it]Downloading shards: 100%|██████████| 4/4 [1:08:11<00:00, 1077.24s/it]Downloading shards: 100%|██████████| 4/4 [1:08:11<00:00, 1022.83s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:25<01:15, 25.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:58<00:59, 29.91s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [01:29<00:30, 30.27s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 22.91s/it]Loading checkpoint shards: 100%|██████████| 4/4 [01:40<00:00, 25.19s/it]
Loading target model: deepseek-ai/deepseek-coder-1.3b-instruct
Loading dataset: mbpp
Loaded 500 problems
============================================================
BTP Fine-tuning Experiment
============================================================
Phase 1: Beam Search Sampling
Beam Search Sampling:   0%|          | 0/30 [00:00<?, ?it/s]/home/ryan/miniforge3/envs/eg/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/ryan/miniforge3/envs/eg/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
Beam Search Sampling:   0%|          | 0/30 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/ryan/eg-cfg-btp/experiments/step2_btp_finetune_experiment.py", line 942, in <module>
    main() 
  File "/home/ryan/eg-cfg-btp/experiments/step2_btp_finetune_experiment.py", line 927, in main
    results = experiment.run_experiment(
  File "/home/ryan/eg-cfg-btp/experiments/step2_btp_finetune_experiment.py", line 722, in run_experiment
    self.phase1_beam_search_sampling(problems_list, num_beams)
  File "/home/ryan/eg-cfg-btp/experiments/step2_btp_finetune_experiment.py", line 564, in phase1_beam_search_sampling
    candidates = self.model_manager.generate_beam_candidates(
  File "/home/ryan/eg-cfg-btp/experiments/step2_btp_finetune_experiment.py", line 476, in generate_beam_candidates
    outputs = self.source_model.generate(
  File "/home/ryan/miniforge3/envs/eg/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/ryan/miniforge3/envs/eg/lib/python3.9/site-packages/transformers/generation/utils.py", line 2254, in generate
    result = self._beam_search(
  File "/home/ryan/miniforge3/envs/eg/lib/python3.9/site-packages/transformers/generation/utils.py", line 3424, in _beam_search
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
  File "/home/ryan/.cache/huggingface/modules/transformers_modules/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct/e434a23f91ba5b4923cf6c9d9a238eb4a08e3a11/modeling_deepseek.py", line 1728, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
  File "/home/ryan/miniforge3/envs/eg/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1940, in __getattr__
    raise AttributeError(
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
