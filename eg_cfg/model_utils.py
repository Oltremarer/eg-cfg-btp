import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from .consts import *
import math
import os
import json


def setup_device():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Running on: {device}")
    return device


def load_model(model_name: str, device):
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
    if os.path.exists(model_name) and os.path.isdir(model_name):
        # æœ¬åœ°æ£€æŸ¥ç‚¹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        print(f"ğŸ”§ åŠ è½½æœ¬åœ°æ£€æŸ¥ç‚¹: {model_name}")
        
        # ä¼˜å…ˆæŸ¥æ‰¾adapter_config.jsonï¼ˆLoRA adapterï¼‰ï¼Œå¦åˆ™æŸ¥æ‰¾config.json
        config_path = os.path.join(model_name, "adapter_config.json")
        if not os.path.exists(config_path):
            config_path = os.path.join(model_name, "config.json")
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯LoRAæ£€æŸ¥ç‚¹
                if "peft_type" in config or "base_model_name_or_path" in config:
                    print("ğŸ”§ æ£€æµ‹åˆ°LoRAå¾®è°ƒæ£€æŸ¥ç‚¹ï¼Œä½¿ç”¨PEFTåŠ è½½")
                    try:
                        from peft import PeftModel
                        
                        # åŠ è½½tokenizer
                        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
                        
                        # è·å–åŸºç¡€æ¨¡å‹è·¯å¾„
                        base_model_path = config.get("base_model_name_or_path", "deepseek-ai/deepseek-coder-1.3b-instruct")
                        print(f"ğŸ”§ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
                        
                        # åŠ è½½åŸºç¡€æ¨¡å‹
                        base_model = AutoModelForCausalLM.from_pretrained(
                            base_model_path,
                            trust_remote_code=True,
                            torch_dtype="auto"
                        ).to(device)
                        
                        # åŠ è½½LoRAé€‚é…å™¨
                        model = PeftModel.from_pretrained(base_model, model_name)
                        print("âœ… LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
                        
                        return model, tokenizer
                        
                    except ImportError:
                        print("âš ï¸  PEFTåº“æœªå®‰è£…ï¼Œå°è¯•ç›´æ¥åŠ è½½")
                    except Exception as e:
                        print(f"âš ï¸  LoRAåŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ç›´æ¥åŠ è½½")
            except Exception as e:
                print(f"âš ï¸  è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°è¯•ç›´æ¥åŠ è½½")
        
        # å¦‚æœä¸æ˜¯LoRAæ£€æŸ¥ç‚¹æˆ–åŠ è½½å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŠ è½½
        print("ğŸ”§ å°è¯•ç›´æ¥åŠ è½½æ£€æŸ¥ç‚¹")
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            trust_remote_code=True,
            torch_dtype="auto"
        ).to(device)
    else:
        # è¿œç¨‹æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if model_name == DEEPSEEK_CODER_V2_LITE_INSTRUCT_MODEL_NAME:
            model = AutoModelForCausalLM.from_pretrained(
                model_name, trust_remote_code=True
            ).to(device)
        else:
            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto").to(
                device
            )
    #model = torch.compile(model)
    return model, tokenizer


def load_tokenizer(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    return tokenizer


def extract_new_tokens(tokenizer, input_ids: torch.Tensor, prompt_input_ids_len):
    if input_ids.dim() != 2 or input_ids.size(0) != 1:
        raise ValueError("Expected input_ids to have shape (1, sequence_length)")

    new_token_ids = input_ids[:, prompt_input_ids_len:]
    new_text = tokenizer.batch_decode(new_token_ids, skip_special_tokens=True)
    return new_text, new_token_ids


def calculate_tokens_length(tokenizer, prompt):
    prompt_token_ids = tokenizer(prompt, return_tensors="pt")
    prompt_input_ids = prompt_token_ids["input_ids"]  # shape: (1pt_len)
    prompt_input_ids_len = prompt_input_ids.shape[1]
    return prompt_input_ids_len


def convert_logprobs_dist_dict_to_tokenizer_prob_dist(tokenizer, logprob_dist_dict):
    prob_dist_dict = {
        token_idx: math.exp(logprob) for token_idx, logprob in logprob_dist_dict.items()
    }
    vocab_size = tokenizer.vocab_size
    prob_dist = torch.zeros(vocab_size)

    for token_idx, prob in prob_dist_dict.items():
        prob_dist[token_idx] = prob

    return prob_dist
