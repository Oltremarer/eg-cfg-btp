import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from .consts import *
import math
import os
import json


def setup_device():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Running on: {device}")
    return device


def load_model(model_name: str, device):
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
    if os.path.exists(model_name) and os.path.isdir(model_name):
        print(f"ğŸ”§ åŠ è½½æœ¬åœ°æ£€æŸ¥ç‚¹: {model_name}")
        # ä¼˜å…ˆæŸ¥æ‰¾adapter_config.jsonï¼ˆLoRA adapterï¼‰ï¼Œå¦åˆ™æŸ¥æ‰¾config.json
        config_path = os.path.join(model_name, "adapter_config.json")
        if not os.path.exists(config_path):
            config_path = os.path.join(model_name, "config.json")
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    config = json.load(f)
                print(f"ã€DEBUGã€‘è¯»å–åˆ°config: {config}")
                if "peft_type" in config or "base_model_name_or_path" in config:
                    print("ã€DEBUGã€‘è¿›å…¥LoRAåˆ†æ”¯ï¼Œå‡†å¤‡åŠ è½½PEFTæ¨¡å‹")
                    try:
                        from peft import PeftModel
                        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
                        base_model_path = config.get("base_model_name_or_path", "deepseek-ai/deepseek-coder-1.3b-instruct")
                        print(f"ã€DEBUGã€‘åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
                        base_model = AutoModelForCausalLM.from_pretrained(
                            base_model_path,
                            trust_remote_code=True,
                            torch_dtype="auto"
                        ).to(device)
                        model = PeftModel.from_pretrained(base_model, model_name)
                        print("ã€DEBUGã€‘LoRAé€‚é…å™¨åŠ è½½æˆåŠŸ")
                        return model, tokenizer
                    except ImportError:
                        print("ã€DEBUGã€‘PEFTåº“æœªå®‰è£…ï¼Œæ— æ³•åŠ è½½LoRA")
                        raise
                    except Exception as e:
                        print(f"ã€DEBUGã€‘LoRAåŠ è½½å¤±è´¥: {e}")
                        raise
            except Exception as e:
                print(f"ã€DEBUGã€‘è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                raise
        # è¿™é‡Œç›´æ¥æŠ¥é”™ï¼Œä¸é™çº§åˆ°transformersåŸç”Ÿåˆ†æ”¯
        raise ValueError("æœ¬åœ°LoRAæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥adapter_config.jsonå’Œpeftåº“")
    else:
        # è¿œç¨‹æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype="auto").to(device)
        return model, tokenizer


def load_tokenizer(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    return tokenizer


def extract_new_tokens(tokenizer, input_ids: torch.Tensor, prompt_input_ids_len):
    if input_ids.dim() != 2 or input_ids.size(0) != 1:
        raise ValueError("Expected input_ids to have shape (1, sequence_length)")

    new_token_ids = input_ids[:, prompt_input_ids_len:]
    new_text = tokenizer.batch_decode(new_token_ids, skip_special_tokens=True)
    return new_text, new_token_ids


def calculate_tokens_length(tokenizer, prompt):
    prompt_token_ids = tokenizer(prompt, return_tensors="pt")
    prompt_input_ids = prompt_token_ids["input_ids"]  # shape: (1pt_len)
    prompt_input_ids_len = prompt_input_ids.shape[1]
    return prompt_input_ids_len


def convert_logprobs_dist_dict_to_tokenizer_prob_dist(tokenizer, logprob_dist_dict):
    prob_dist_dict = {
        token_idx: math.exp(logprob) for token_idx, logprob in logprob_dist_dict.items()
    }
    vocab_size = tokenizer.vocab_size
    prob_dist = torch.zeros(vocab_size)

    for token_idx, prob in prob_dist_dict.items():
        prob_dist[token_idx] = prob

    return prob_dist
